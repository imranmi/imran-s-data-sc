[
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html",
    "href": "R-ex/R-Ex5/R_Ex5.html",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the modules of our proposed Shiny application (Group Project) and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for our Shiny application are supported in R CRAN,\nTo prepare and test that the specific R codes can run and returns the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications,\nTo select the appropriate Shiny UI components for exposing the parameters determined above, and\nTo include a section called UI design for the different components of the UIs for the proposed design."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#loading-r-packages",
    "href": "R-ex/R-Ex5/R_Ex5.html#loading-r-packages",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.2.1 Loading R packages",
    "text": "4.2.1 Loading R packages\nThe below R packages will be used in this exercise and for the Shiny application\n\npacman::p_load(sf, tidyverse, tmap, dplyr,\n               spatstat, spdep,\n               lubridate, leaflet,\n               plotly, DT, viridis,\n               ggplot2, sfdep)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#importing-and-loading-the-acled-data",
    "href": "R-ex/R-Ex5/R_Ex5.html#importing-and-loading-the-acled-data",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.2.2 Importing and loading the ACLED data",
    "text": "4.2.2 Importing and loading the ACLED data\nCountry specific data from the Armed Conflict Location & Event Data Project (ACLED) can be downloaded at https://acleddata.com/data-export-tool/\nLoading the ACLED data set for Myanmar.\n\nACLED_MMR &lt;- read_csv(\"data/MMR.csv\")"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#downloading-and-loading-the-shape-files-for-country",
    "href": "R-ex/R-Ex5/R_Ex5.html#downloading-and-loading-the-shape-files-for-country",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.2.3 Downloading and loading the shape files for country",
    "text": "4.2.3 Downloading and loading the shape files for country\nShape files were downloaded from the Myanmmar Information Management Unit (MIMU) website.\nI chose this source over GADM and GeoBoundaries due to its updated administrative region information and map levels.\n\n\n\n\n\n\nNote - Data Quality Issues\n\n\n\nACLED captures event data from national, sub-national and other media sources, and populates event locations based on the last known information.\n\nHowever, some names of administrative areas were found to have changed; either disaggregated into new administrative areas or previously active but now defunct. Some administrative areas were also aggregated into higher administrative areas.\nAs part of our data cleaning and preparation process, I had to identify discrepancies in both admin1 & 2 (administrative levels) and re-name some administrative areas to sync with the downloaded shape files from MIMU."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#loading-admin-1-2-administrative-regionarea-shape-files",
    "href": "R-ex/R-Ex5/R_Ex5.html#loading-admin-1-2-administrative-regionarea-shape-files",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.3.1 Loading Admin 1 & 2 (administrative region/area) shape files",
    "text": "4.3.1 Loading Admin 1 & 2 (administrative region/area) shape files\n\nmmr_shp_mimu_1 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda2_adm1_250k_mimu_1\")\n\nReading layer `mmr_polbnda2_adm1_250k_mimu_1' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex5\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 18 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex5\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nclass(mmr_shp_mimu_2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nThe Shape file for admin2 level map, is an SF object, with geometry type: Multipolygon.\n\nst_geometry(mmr_shp_mimu_2)\n\nGeometry set for 80 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\n\nst_crs(mmr_shp_mimu_2)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]]]\n\n\nNext, I will check the unique district names in this shape file (admin2)\n\nunique_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\nunique_regions_mimu2\n\n [1] \"Hinthada\"                        \"Labutta\"                        \n [3] \"Maubin\"                          \"Myaungmya\"                      \n [5] \"Pathein\"                         \"Pyapon\"                         \n [7] \"Bago\"                            \"Taungoo\"                        \n [9] \"Pyay\"                            \"Thayarwady\"                     \n[11] \"Falam\"                           \"Hakha\"                          \n[13] \"Matupi\"                          \"Mindat\"                         \n[15] \"Bhamo\"                           \"Mohnyin\"                        \n[17] \"Myitkyina\"                       \"Puta-O\"                         \n[19] \"Bawlake\"                         \"Loikaw\"                         \n[21] \"Hpa-An\"                          \"Hpapun\"                         \n[23] \"Kawkareik\"                       \"Myawaddy\"                       \n[25] \"Gangaw\"                          \"Magway\"                         \n[27] \"Minbu\"                           \"Pakokku\"                        \n[29] \"Thayet\"                          \"Kyaukse\"                        \n[31] \"Maungdaw\"                        \"Mrauk-U\"                        \n[33] \"Sittwe\"                          \"Thandwe\"                        \n[35] \"Hkamti\"                          \"Kale\"                           \n[37] \"Kanbalu\"                         \"Katha\"                          \n[39] \"Kawlin\"                          \"Mawlaik\"                        \n[41] \"Monywa\"                          \"Naga Self-Administered Zone\"    \n[43] \"Sagaing\"                         \"Shwebo\"                         \n[45] \"Tamu\"                            \"Yinmarbin\"                      \n[47] \"Kengtung\"                        \"Monghsat\"                       \n[49] \"Tachileik\"                       \"Hopang\"                         \n[51] \"Kokang Self-Administered Zone\"   \"Kyaukme\"                        \n[53] \"Lashio\"                          \"Matman\"                         \n[55] \"Mongmit\"                         \"Muse\"                           \n[57] \"Pa Laung Self-Administered Zone\" \"Danu Self-Administered Zone\"    \n[59] \"Langkho\"                         \"Loilen\"                         \n[61] \"Pa-O Self-Administered Zone\"     \"Taunggyi\"                       \n[63] \"Dawei\"                           \"Kawthoung\"                      \n[65] \"Mandalay\"                        \"Meiktila\"                       \n[67] \"Myingyan\"                        \"Nyaung-U\"                       \n[69] \"Pyinoolwin\"                      \"Yamethin\"                       \n[71] \"Mawlamyine\"                      \"Thaton\"                         \n[73] \"Det Khi Na\"                      \"Oke Ta Ra\"                      \n[75] \"Kyaukpyu\"                        \"Myeik\"                          \n[77] \"Yangon (East)\"                   \"Yangon (North)\"                 \n[79] \"Yangon (South)\"                  \"Yangon (West)\"                  \n\n\nThere are 80 admin2 levels or districts in mmr_shp_mimu_2\nLets compare with our admin2 levels in our main dataset ACLED_MMR\n\nunique_acled_regions2 &lt;- unique(ACLED_MMR$admin2)\n\nunique_acled_regions2\n\n [1] \"Maungdaw\"                        \"Bago\"                           \n [3] \"Shwebo\"                          \"Kyaukme\"                        \n [5] \"Pyinoolwin\"                      \"Muse\"                           \n [7] \"Sittwe\"                          \"Yinmarbin\"                      \n [9] \"Thaton\"                          \"Yangon-North\"                   \n[11] \"Pa-O Self-Administered Zone\"     \"Hpapun\"                         \n[13] \"Kyaukpyu\"                        \"Yangon-West\"                    \n[15] \"Mongmit\"                         \"Bhamo\"                          \n[17] \"Mrauk-U\"                         \"Yangon-East\"                    \n[19] \"Yangon-South\"                    \"Monywa\"                         \n[21] \"Gangaw\"                          \"Pathein\"                        \n[23] \"Katha\"                           \"Taungoo\"                        \n[25] \"Kanbalu\"                         \"Lashio\"                         \n[27] \"Mawlamyine\"                      \"Myitkyina\"                      \n[29] \"Kawkareik\"                       \"Loilen\"                         \n[31] \"Mandalay\"                        \"Kawlin\"                         \n[33] \"Kyaukse\"                         \"Magway\"                         \n[35] \"Meiktila\"                        \"Pakokku\"                        \n[37] \"Taunggyi\"                        \"Tamu\"                           \n[39] \"Nay Pyi Taw\"                     \"Mohnyin\"                        \n[41] \"Kale\"                            \"Det Khi Na\"                     \n[43] \"Myingyan\"                        \"Loikaw\"                         \n[45] \"Matupi\"                          \"Pyay\"                           \n[47] \"Sagaing\"                         \"Myeik\"                          \n[49] \"Dawei\"                           \"Thayarwady\"                     \n[51] \"Thandwe\"                         \"Mawlaik\"                        \n[53] \"Bawlake\"                         \"Pyapon\"                         \n[55] \"Hinthada\"                        \"Thayet\"                         \n[57] \"Pa Laung Self-Administered Zone\" \"Mindat\"                         \n[59] \"Hkamti\"                          \"Kokang Self-Administered Zone\"  \n[61] \"Hpa-An\"                          \"Danu Self-Administered Zone\"    \n[63] \"Myawaddy\"                        \"Maubin\"                         \n[65] \"Hakha\"                           \"Falam\"                          \n[67] \"Minbu\"                           \"Monghsat\"                       \n[69] \"Puta-O\"                          \"Hopang\"                         \n[71] \"Nyaung-U\"                        \"Kawthoung\"                      \n[73] \"Yamethin\"                        \"Yangon\"                         \n[75] \"Myaungmya\"                       \"Mong Pawk (Wa SAD)\"             \n[77] \"Oke Ta Ra\"                       \"Matman\"                         \n[79] \"Kengtung\"                        \"Naga Self-Administered Zone\"    \n[81] \"Labutta\"                         \"Langkho\"                        \n[83] \"Tachileik\"                      \n\n\nI will write a simple function below to identify the discrepancies between the shape file and our state/district names in our main dataset.\n\n# Find the unique region names that are in 'unique_acled_regions2' but not in 'unique_regions_mimu2'\n\nmismatched_admin2 &lt;- setdiff(unique_acled_regions2, unique_regions_mimu2)\n\nif (length(mismatched_admin2) &gt; 0) {\n  print(\"The following region names from 'acled_mmr' do not match any in 'mimu2':\")\n  print(mismatched_admin2)\n} else {\n  print(\"All unique region names in 'acled_mmr' match the unique region names in 'mimu2.'\")\n}\n\n[1] \"The following region names from 'acled_mmr' do not match any in 'mimu2':\"\n[1] \"Yangon-North\"       \"Yangon-West\"        \"Yangon-East\"       \n[4] \"Yangon-South\"       \"Nay Pyi Taw\"        \"Yangon\"            \n[7] \"Mong Pawk (Wa SAD)\"\n\n\nLets harmonize the names in both data files. I will re-save it to a new data set called ACLED_MMR_1\nFixing our admin 1 names.\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\nFixing our admin 2 names.\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\nChecking if our changes are successful.\n\n# Get unique admin 2 district names from 'ACLED_MMR_1'\nunique_acled_regions2 &lt;- unique(ACLED_MMR_1$admin2)\n\n# Get unique district names from 'mmr_shp_mimu_2'\nunique_map_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\n# Find the unique district names that are in 'unique_acled_regions2' but not in 'unique_map_regions_mimu2'\n\nmismatched_regions2 &lt;- setdiff(unique_acled_regions2, unique_map_regions_mimu2)\n\nif (length(mismatched_regions2) &gt; 0) {\n  print(\"The following district names from 'acled_mmr_1' do not match any in 'mmr_shp_mimu_2':\")\n  print(mismatched_regions2)\n} else {\n  print(\"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\")\n}\n\n[1] \"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\"\n\n\nLets do a sample plot to see how our country map looks like at the admin2 (districts) level.\n\nplot(mmr_shp_mimu_2)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#data-wrangling",
    "href": "R-ex/R-Ex5/R_Ex5.html#data-wrangling",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.3.2 Data Wrangling",
    "text": "4.3.2 Data Wrangling\nFor the purposes of plotting choropleth maps, I will first create attributes subsets to summarise the number of incidents and fatalities, grouped by year, admin region, event type and sub event type.\n\nData2 &lt;- ACLED_MMR_1 %&gt;%\n    group_by(year, admin2, event_type, sub_event_type) %&gt;%\n    summarise(Incidents = n(),\n              Fatalities = sum(fatalities, na.rm = TRUE)) %&gt;%\n              \n    ungroup()\n\nChecking the total sum of incidents and fatalities\n\ntotal_incidents2 &lt;- sum(Data2$Incidents)\ntotal_fatalities2 &lt;- sum(Data2$Fatalities)\n\ntotal_incidents2\n\n[1] 57198\n\ntotal_fatalities2\n\n[1] 57593\n\n\nNext, I will do a spatial join between my shape files and attribute files\n\nACLED_MMR_admin2 &lt;- left_join(mmr_shp_mimu_2, Data2,\n                            by = c(\"DT\" = \"admin2\"))\n\nRemoving the variables I don’t require.\n\nACLED_MMR_admin2 &lt;- ACLED_MMR_admin2 %&gt;%\n                      select(-OBJECTID, -ST, -ST_PCODE)\n\n\nclass(ACLED_MMR_admin2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNext, I will just double check that total sum of incidents and fatalities in our SF files are correct as per our original datasets.\n\ntotal_incidents_check &lt;- sum(ACLED_MMR_admin2$Incidents)\ntotal_fatalities_check &lt;- sum(ACLED_MMR_admin2$Fatalities)\n\ntotal_incidents_check \n\n[1] 57198\n\ntotal_fatalities_check\n\n[1] 57593"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "href": "R-ex/R-Ex5/R_Ex5.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.4.1 Choropleth map of Incidents & Fatalities by Admin2 level (by District)",
    "text": "4.4.1 Choropleth map of Incidents & Fatalities by Admin2 level (by District)\nThe below codes will be used to create the choropleth maps.\n\nFatalities in Battles in 2023, by Districts (Quantile)Incidents of Violence against civilians in 2021, by Districts (Equal)\n\n\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2021, event_type == \"Violence against civilians\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Incidents\",\n          n = 5,\n          style = \"equal\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nAdding Interactivity by using tmap_leaftlet()\n\ndata_filtered &lt;- ACLED_MMR_admin2 %&gt;%\n  filter(year == 2022, event_type == \"Battles\")\n\ntm_map &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(data_filtered) +\n  tm_fill(col = \"Incidents\", n = 5, style = \"equal\", palette = \"Reds\", title = \"Incidents\") +\n  tm_borders(alpha = 0.5)\n\ntmap_leaflet(tm_map)\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nFrom the codes above, below are the variables we can expose as user inputs:-\n\nYear\nEvent type (Battles, Violence against civilians, protests, riots, explosions/remote violence)\nCount type: number of Incidents or Fatalities\nData classification type: eg quantile, equal, jenks, kmeans, pretty etc"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#visualising-the-location-of-conflict-events",
    "href": "R-ex/R-Ex5/R_Ex5.html#visualising-the-location-of-conflict-events",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.5.1 Visualising the location of conflict events",
    "text": "4.5.1 Visualising the location of conflict events\nUsing the leaflet package, I will use the Geometry points from our SF data sets to plot the points of event types in the maps.\nIn this case, I will use admin 1 level regions, to achieve a better aesthetics for users.\nThis is because, visually dividing the country map into more smaller districts (admin2) would likely make the map look “too busy”.\n\nBattles from 2010 to presentViolence against civilians from 2010 to presentProtests from 2010 to present\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Battles) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Battles&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Violence_CV) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Violence on Civillians&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Protests) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Protests&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nThe plots above are sufficiently interactive as users can hover to any “circle” to get more information on the event and location.\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#filtering-the-event-and-year-event-type-battles-in-2023",
    "href": "R-ex/R-Ex5/R_Ex5.html#filtering-the-event-and-year-event-type-battles-in-2023",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.7.1 Filtering the Event and Year (Event type = Battles, in 2023)",
    "text": "4.7.1 Filtering the Event and Year (Event type = Battles, in 2023)\nThe below subset will serve as our reference data subset for our subsequent codes.\n\nBattles_2023 &lt;- Events_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\")"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-contiguity-spatial-weights",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-contiguity-spatial-weights",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.7.2 Computing Contiguity Spatial Weights",
    "text": "4.7.2 Computing Contiguity Spatial Weights\nBefore we can compute any spatial statistics, we need to construct spatial weights of the study area.\nThe spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. admin2) in the study area (Myanmar).\nIn the code below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\nBy default this function will return a list of first order neighbours using the Queen criteria.\nHowever, we can also pass a “queen” argument that takes TRUE or FALSE as options.\n\nwm_q &lt;- poly2nb(Battles_2023, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 356 \nPercentage nonzero weights: 6.501096 \nAverage number of links: 4.810811 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8 10 \n 3  8 11 11 15  9  9  6  2 \n3 least connected regions:\n2 16 58 with 1 link\n2 most connected regions:\n19 56 with 10 links\n\n\nThe summary report above shows that there are 74 area units for this subset (Battles occurring in 2023).\n\nThere are 2 most connected area units with 10 neighbours, and there are 3 area units with only 1 neighbour."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#row-standardised-weights-matrix",
    "href": "R-ex/R-Ex5/R_Ex5.html#row-standardised-weights-matrix",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.7.3 Row-standardised weights matrix",
    "text": "4.7.3 Row-standardised weights matrix\nNext, we assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring admin2 (district) and then summing the weighted income values.\nThis has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons and thus potentially over or under estimating the true nature of the spatial autocorrelation in the data.\nHowever, for this example, I will stick with the style=“W” option for simplicity’s sake. Other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 356 \nPercentage nonzero weights: 6.501096 \nAverage number of links: 4.810811 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 74 5476 74 36.81702 310.0455"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-local-morans-i",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-local-morans-i",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.1 Computing Local Moran’s I",
    "text": "4.8.1 Computing Local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a list object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips &lt;- order(Battles_2023$DT)\nlocalMI &lt;- localmoran(Battles_2023$Incidents, rswm_q)\nhead(localMI)\n\n           Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.46274887 -9.006432e-03 0.1247560902  1.3356292      0.1816705\n2  0.71317847 -1.016877e-02 0.7448368248  0.8381394      0.4019524\n3  0.69218038 -9.386041e-03 0.3392457987  1.2045132      0.2283913\n4  0.68518101 -9.386041e-03 0.3392457987  1.1924960      0.2330668\n5  0.00627746 -1.483579e-05 0.0001702656  0.4822206      0.6296493\n6 -0.23004674 -4.814215e-03 0.0464274459 -1.0453066      0.2958813\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code below lists the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=Battles_2023$DT[fips]),\n  check.names=FALSE)\n\n                                         Ii        E.Ii      Var.Ii        Z.Ii\nBago                             6.2775e-03 -1.4836e-05  1.7027e-04  4.8222e-01\nBawlake                         -1.7909e-02 -4.6941e-04  1.1252e-02 -1.6441e-01\nBhamo                            2.4621e-01 -1.7367e-03  1.9897e-02  1.7577e+00\nDanu Self-Administered Zone      3.3657e-01 -8.2707e-03  1.9670e-01  7.7752e-01\nDawei                            9.4559e-01 -1.4130e-02  5.0825e-01  1.3462e+00\nDet Khi Na                       2.3024e-01 -6.5686e-03  6.3234e-02  9.4170e-01\nFalam                           -7.5203e-03 -2.4381e-03  5.8326e-02 -2.1044e-02\nGangaw                           6.2609e-01 -6.9289e-03  6.6679e-02  2.4514e+00\nHakha                           -1.1347e-02 -6.1003e-05  1.0815e-03 -3.4318e-01\nHinthada                         4.6275e-01 -9.0064e-03  1.2476e-01  1.3356e+00\nHkamti                          -5.3716e-02 -3.0598e-03  3.5009e-02 -2.7074e-01\nHopang                          -2.3647e-01 -9.7735e-03  3.5311e-01 -3.8150e-01\nHpa-An                          -1.3231e-01 -3.0598e-03  1.9751e-02 -9.1968e-01\nHpapun                          -4.1969e-02 -4.2526e-03  5.9189e-02 -1.5503e-01\nKale                             1.2532e-01 -7.7384e-04  6.4571e-03  1.5692e+00\nKanbalu                         -3.2905e-02 -3.4002e-05  3.9022e-04 -1.6640e+00\nKatha                           -8.8073e-03 -1.8682e-02  1.5309e-01  2.5238e-02\nKawkareik                        2.9062e-02 -1.3663e-02  3.2318e-01  7.5156e-02\nKawlin                          -7.2541e-02 -1.4063e-03  3.3678e-02 -3.8762e-01\nKawthoung                       -6.7102e-01 -3.2826e-03  2.4212e-01 -1.3570e+00\nKokang Self-Administered Zone    9.6449e-04 -1.1447e-08  2.7452e-07  1.8408e+00\nKyaukme                         -6.9939e-02 -9.0471e-03  8.6877e-02 -2.0659e-01\nKyaukpyu                         4.7922e-01 -6.8933e-03  1.2137e-01  1.3953e+00\nKyaukse                         -1.4472e-01 -9.0064e-03  7.4533e-02 -4.9713e-01\nLabutta                          7.1318e-01 -1.0169e-02  7.4484e-01  8.3814e-01\nLangkho                         -9.6769e-03 -8.6347e-03  2.0528e-01 -2.3004e-03\nLashio                           2.0219e-01 -4.2805e-03  4.8917e-02  9.3352e-01\nLoikaw                          -4.7652e-01 -2.3869e-02  3.2568e-01 -7.9318e-01\nLoilen                          -2.8804e-02 -5.6413e-03  7.8408e-02 -8.2718e-02\nMagway                           9.4077e-02 -5.9426e-03  4.9330e-02  4.5033e-01\nMandalay                        -2.2955e-01 -3.0598e-03  7.3153e-02 -8.3742e-01\nMatupi                          -1.7986e-02 -3.2117e-04  4.4878e-03 -2.6370e-01\nMaungdaw                         1.2575e-01 -2.6375e-03  6.3084e-02  5.1117e-01\nMawlaik                         -3.2557e-02 -2.6375e-03  3.0190e-02 -1.7220e-01\nMawlamyine                       2.4766e-01 -3.3072e-03  5.8440e-02  1.0381e+00\nMeiktila                         3.8976e-01 -8.2707e-03  7.9484e-02  1.4118e+00\nMinbu                           -1.9321e-01 -6.2516e-03  6.0203e-02 -7.6195e-01\nMindat                           2.2211e-02 -8.7518e-04  1.5503e-02  1.8541e-01\nMohnyin                          1.1002e-01 -8.8788e-04  1.5727e-02  8.8440e-01\nMongmit                         -4.9751e-01 -8.6347e-03  1.1965e-01 -1.4133e+00\nMonywa                           3.7540e+00 -3.3925e-02  5.8106e-01  4.9692e+00\nMrauk-U                          2.1574e-01 -3.9983e-03  4.5705e-02  1.0278e+00\nMuse                             2.6578e-01 -1.6313e-01  2.4204e+00  2.7569e-01\nMyawaddy                         1.8035e-02 -6.4391e-05  2.3492e-03  3.7341e-01\nMyeik                            3.6057e-01 -2.5739e-02  9.1496e-01  4.0386e-01\nMyingyan                         4.3732e-02 -1.0008e-04  1.3987e-03  1.1720e+00\nMyitkyina                       -1.9413e-01 -6.2855e-03  8.7306e-02 -6.3572e-01\nNaga Self-Administered Zone     -8.8212e-02 -1.0169e-02  3.6725e-01 -1.2878e-01\nNyaung-U                        -5.4115e-01 -8.6347e-03  1.5176e-01 -1.3669e+00\nOke Ta Ra                        5.7519e-01 -9.0064e-03  1.5824e-01  1.4686e+00\nPa-O Self-Administered Zone      1.9941e-01 -5.6413e-03  5.4359e-02  8.7950e-01\nPa Laung Self-Administered Zone -5.3309e-01 -5.0623e-03  7.0402e-02 -1.9901e+00\nPakokku                          1.9341e+00 -2.2766e-01  1.4683e+00  1.7840e+00\nPathein                          6.9218e-01 -9.3860e-03  3.3925e-01  1.2045e+00\nPuta-O                          -4.8052e-01 -6.8933e-03  5.0659e-01 -6.6543e-01\nPyapon                           6.8518e-01 -9.3860e-03  3.3925e-01  1.1925e+00\nPyay                             1.0545e-01 -1.2618e-03  1.7615e-02  8.0407e-01\nPyinoolwin                       4.6151e-01 -2.7025e-02  2.1958e-01  1.0426e+00\nSagaing                          9.5824e-01 -1.0212e-02  9.7948e-02  3.0944e+00\nShwebo                           2.1412e+00 -5.0075e-02  5.4593e-01  2.9657e+00\nSittwe                           2.3135e-01 -3.0598e-03  1.1130e-01  7.0266e-01\nTamu                             3.2174e-02 -1.8902e-04  3.3506e-03  5.5911e-01\nTaunggyi                        -1.0168e-01 -1.1395e-03  7.3697e-03 -1.1711e+00\nTaungoo                         -2.3005e-01 -4.8142e-03  4.6427e-02 -1.0453e+00\nThandwe                          5.6456e-01 -1.0169e-02  1.4069e-01  1.5322e+00\nThaton                          -6.7767e-02 -3.0835e-03  5.4499e-02 -2.7708e-01\nThayarwady                       9.0973e-03 -1.4836e-05  2.0737e-04  6.3277e-01\nThayet                           2.9530e-01 -5.3479e-03  5.1546e-02  1.3242e+00\nYamethin                         4.3920e-01 -9.7735e-03  1.3528e-01  1.2207e+00\nYangon (East)                    6.1100e-01 -7.5664e-03  1.8008e-01  1.4576e+00\nYangon (North)                   4.4954e-01 -9.3860e-03  1.0671e-01  1.4049e+00\nYangon (South)                   5.2023e-01 -8.6347e-03  1.1965e-01  1.5289e+00\nYangon (West)                    6.6585e-01 -9.7735e-03  2.3209e-01  1.4024e+00\nYinmarbin                        4.5788e+00 -9.9115e-02  1.2481e+00  4.1872e+00\n                                Pr.z....E.Ii..\nBago                                    0.6296\nBawlake                                 0.8694\nBhamo                                   0.0788\nDanu Self-Administered Zone             0.4369\nDawei                                   0.1782\nDet Khi Na                              0.3463\nFalam                                   0.9832\nGangaw                                  0.0142\nHakha                                   0.7315\nHinthada                                0.1817\nHkamti                                  0.7866\nHopang                                  0.7028\nHpa-An                                  0.3577\nHpapun                                  0.8768\nKale                                    0.1166\nKanbalu                                 0.0961\nKatha                                   0.9799\nKawkareik                               0.9401\nKawlin                                  0.6983\nKawthoung                               0.1748\nKokang Self-Administered Zone           0.0656\nKyaukme                                 0.8363\nKyaukpyu                                0.1629\nKyaukse                                 0.6191\nLabutta                                 0.4020\nLangkho                                 0.9982\nLashio                                  0.3506\nLoikaw                                  0.4277\nLoilen                                  0.9341\nMagway                                  0.6525\nMandalay                                0.4024\nMatupi                                  0.7920\nMaungdaw                                0.6092\nMawlaik                                 0.8633\nMawlamyine                              0.2992\nMeiktila                                0.1580\nMinbu                                   0.4461\nMindat                                  0.8529\nMohnyin                                 0.3765\nMongmit                                 0.1576\nMonywa                                  0.0000\nMrauk-U                                 0.3040\nMuse                                    0.7828\nMyawaddy                                0.7088\nMyeik                                   0.6863\nMyingyan                                0.2412\nMyitkyina                               0.5250\nNaga Self-Administered Zone             0.8975\nNyaung-U                                0.1716\nOke Ta Ra                               0.1419\nPa-O Self-Administered Zone             0.3791\nPa Laung Self-Administered Zone         0.0466\nPakokku                                 0.0744\nPathein                                 0.2284\nPuta-O                                  0.5058\nPyapon                                  0.2331\nPyay                                    0.4214\nPyinoolwin                              0.2972\nSagaing                                 0.0020\nShwebo                                  0.0030\nSittwe                                  0.4823\nTamu                                    0.5761\nTaunggyi                                0.2415\nTaungoo                                 0.2959\nThandwe                                 0.1255\nThaton                                  0.7817\nThayarwady                              0.5269\nThayet                                  0.1854\nYamethin                                0.2222\nYangon (East)                           0.1449\nYangon (North)                          0.1601\nYangon (South)                          0.1263\nYangon (West)                           0.1608\nYinmarbin                               0.0000"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#mapping-the-local-morans-i",
    "href": "R-ex/R-Ex5/R_Ex5.html#mapping-the-local-morans-i",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.2 Mapping the Local Moran’s I",
    "text": "4.8.2 Mapping the Local Moran’s I\nBefore mapping the local Moran’s I map, I will need to append the local Moran’s I dataframe (i.e. localMI) onto the Battles_2023’s SF DataFrame.\n\nBattles_2023.localMI &lt;- cbind(Battles_2023,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nBattles_2023.localMI\n\nSimple feature collection with 74 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DT   DT_PCODE    DT_MMR PCode_V year event_type Incidents Fatalities\n1    Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023    Battles         4          3\n2     Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023    Battles         1          1\n3     Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023    Battles         3          1\n4      Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023    Battles         3          2\n5        Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023    Battles        50        270\n6     Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023    Battles        87        493\n7        Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023    Battles        34         59\n8  Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023    Battles        50         93\n9       Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023    Battles        27         89\n10      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023    Battles        48        210\n             Ii          E.Ii       Var.Ii        Z.Ii     Pr.Ii\n1   0.462748867 -9.006432e-03 0.1247560902  1.33562921 0.1816705\n2   0.713178468 -1.016877e-02 0.7448368248  0.83813940 0.4019524\n3   0.692180375 -9.386041e-03 0.3392457987  1.20451317 0.2283913\n4   0.685181011 -9.386041e-03 0.3392457987  1.19249602 0.2330668\n5   0.006277460 -1.483579e-05 0.0001702656  0.48222056 0.6296493\n6  -0.230046740 -4.814215e-03 0.0464274459 -1.04530664 0.2958813\n7   0.105454317 -1.261774e-03 0.0176145487  0.80407054 0.4213562\n8   0.009097304 -1.483579e-05 0.0002073682  0.63277490 0.5268807\n9  -0.007520292 -2.438086e-03 0.0583263538 -0.02104359 0.9832109\n10 -0.011346560 -6.100301e-05 0.0010814666 -0.34317564 0.7314663\n                         geometry\n1  MULTIPOLYGON (((95.12637 18...\n2  MULTIPOLYGON (((95.04462 15...\n3  MULTIPOLYGON (((94.27572 15...\n4  MULTIPOLYGON (((95.20798 15...\n5  MULTIPOLYGON (((95.90674 18...\n6  MULTIPOLYGON (((96.17964 19...\n7  MULTIPOLYGON (((95.70458 19...\n8  MULTIPOLYGON (((95.85173 18...\n9  MULTIPOLYGON (((93.36931 24...\n10 MULTIPOLYGON (((93.35213 23..."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-values",
    "href": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-values",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.3 Mapping Local Moran’s I values",
    "text": "4.8.3 Mapping Local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code below.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-p-values",
    "href": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-p-values",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.4 Mapping Local Moran’s I p-values",
    "text": "4.8.4 Mapping Local Moran’s I p-values\nThe choropleth above shows there is evidence for both positive and negative Ii values. However, we will also need to consider the p-values for each of these values.\nThe code below produces a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#data-table-for-the-morans-i-values",
    "href": "R-ex/R-Ex5/R_Ex5.html#data-table-for-the-morans-i-values",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.6 Data table for the Moran’s I values",
    "text": "4.8.6 Data table for the Moran’s I values\nFor the sake of readability, it may also be a good idea to add a data table of the values, for users to make sense of both maps.\nThe below code will be used to generate the data table.\n\ndatatable(Battles_2023.localMI)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#plotting-the-moran-scatter-plot",
    "href": "R-ex/R-Ex5/R_Ex5.html#plotting-the-moran-scatter-plot",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.1 Plotting the Moran Scatter plot",
    "text": "4.10.1 Plotting the Moran Scatter plot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code below plots the Moran scatterplot of Battles in 2023 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(Battles_2023$Incidents, rswm_q,\n                  labels=as.character(Battles_2023$DT), \n                  xlab=\"Battles_2023\", \n                  ylab=\"Spatially Lagged Events,Year\")\n\n\n\n\n\n\n\n\nThe plot is split in 4 quadrants. The top right corner belongs to areas that have high incidents of events and are surrounded by other areas that have higher than the average level/number of battles This is the high-high locations.\n\n\n\n\n\n\nNote\n\n\n\nThe Moran scatterplot is divided into four areas, with each quadrant corresponding with one of four categories: (1) High-High (HH) in the top-right quadrant; (2) High-Low (HL) in the bottom right quadrant; (3) Low-High (LH) in the top-left quadrant; (4) Low- Low (LL) in the bottom left quadrant."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "R-ex/R-Ex5/R_Ex5.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.2 Plotting Moran scatterplot with standardised variable",
    "text": "4.10.2 Plotting Moran scatterplot with standardised variable\nFirst, I will use scale() to centre and scale the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centred) variable by their standard deviations.\n\nBattles_2023$Z.Incidents &lt;- scale(Battles_2023$Incidents) %&gt;% \n  as.vector \n\nThe as.vector() is added to the end is to make sure that the data type is a vector, that maps neatly into our dataframe.\nNext, we plot the Moran scatterplot again by using the code below.\n\nnci2 &lt;- moran.plot(Battles_2023$Z.Incidents, rswm_q,\n                   labels=as.character(Battles_2023$DT),\n                   xlab=\"z-Battles in 2023\", \n                   ylab=\"Spatially Lag z-Battles in 2023\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1) High-High (HH): indicates high spatial correlation where incidents of Battles are clustered closely together.\n2) High-Low (HL): where areas of high frequency of incidents of Battles occurred are located next to areas where there is low frequency of incidents of Battles occurred.\n3) Low-High (LH): these are areas of low frequency of incidents where Battles occurred that are located next to areas where high frequency of Battles.\n4) Low-Low (LL): these are clusters of low frequency of incidents of Battles occurred."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#preparing-lisa-map-classes",
    "href": "R-ex/R-Ex5/R_Ex5.html#preparing-lisa-map-classes",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.3 Preparing LISA map classes",
    "text": "4.10.3 Preparing LISA map classes\nAccording to Anselin (1995), LISA can be used to locate “hot spots” or local spatial clusters where the occurrence of Event types is statistically significant.\nIn addition to the four categories described in the Moran Scatterplot, the LISA analysis includes an additional category: (5) Insignificant: where there are no spatial autocorrelation or clusters where event types have occurred.\nThe code below shows the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. Incidents) and centre the spatially lagged variable around its mean.\n\nBattles_2023$lag_Incidents &lt;- lag.listw(rswm_q, Battles_2023$Incidents)\nDV &lt;- Battles_2023$lag_Incidents - mean(Battles_2023$lag_Incidents)     \n\nThis is followed by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThe code below define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, we place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#plotting-lisa-map",
    "href": "R-ex/R-Ex5/R_Ex5.html#plotting-lisa-map",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.4 Plotting LISA Map",
    "text": "4.10.4 Plotting LISA Map\nThe below code is used to create the LISA map.\n\nBattles_2023.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"lightyellow\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#deriving-distance-based-weight-matrix",
    "href": "R-ex/R-Ex5/R_Ex5.html#deriving-distance-based-weight-matrix",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.1 Deriving distance-based weight matrix",
    "text": "4.12.1 Deriving distance-based weight matrix\nWhist the spatial autocorrelation in the previous section considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n4.12.1.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph.\nWe need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length.\nOur input vector will be the geometry column of Battles_2023 dataset. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid() function over the geometry column of our Battles_2023 dataset and access the longitude value through double bracket notation [[]] and 1.\nThis allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[1]])\n\n\nclass(longitude)\n\n[1] \"numeric\"\n\n\n\nlongitude\n\n [1] 95.19035 94.99369 94.74008 95.53705 96.58767 96.34709 95.30186 95.75119\n [9] 93.71488 93.56355 93.22544 93.81953 97.16218 96.49805 97.41891 97.78205\n[17] 97.42880 97.33311 97.45091 97.36866 98.22657 98.51991 94.18202 95.31595\n[25] 94.45732 94.73063 95.05170 96.22693 92.48276 93.35447 92.90601 94.50024\n[33] 95.47237 94.40944 95.49546 96.05576 95.53962 94.74604 95.26376 95.66196\n[41] 95.64449 95.40740 94.38739 94.71565 98.96838 98.65407 97.13646 98.19561\n[49] 96.66042 98.01229 97.21860 96.52970 98.02872 97.92760 97.03704 96.92822\n[57] 98.47076 98.77515 96.16459 95.97341 95.54920 95.15564 96.25009 96.06338\n[65] 97.84218 97.26005 96.16788 96.13921 93.92534 98.93770 96.24776 96.03014\n[73] 96.28511 96.13431\n\n\nWe do the same for latitude by accessing the second value per centroid with [[2]].\n\nlatitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\nclass(coords)\n\n[1] \"matrix\" \"array\" \n\n\n\ncoords\n\n      longitude latitude\n [1,]  95.19035 17.87515\n [2,]  94.99369 16.16236\n [3,]  94.74008 16.86214\n [4,]  95.53705 16.17363\n [5,]  96.58767 17.74287\n [6,]  96.34709 18.81260\n [7,]  95.30186 18.76076\n [8,]  95.75119 18.10024\n [9,]  93.71488 23.42199\n[10,]  93.56355 22.51244\n[11,]  93.22544 21.57539\n[12,]  93.81953 21.26333\n[13,]  97.16218 24.25814\n[14,]  96.49805 25.30215\n[15,]  97.41891 26.01108\n[16,]  97.78205 27.27547\n[17,]  97.42880 18.91417\n[18,]  97.33311 19.48546\n[19,]  97.45091 17.75720\n[20,]  97.36866 18.07659\n[21,]  98.22657 16.00305\n[22,]  98.51991 16.54942\n[23,]  94.18202 21.80335\n[24,]  95.31595 20.26548\n[25,]  94.45732 20.38303\n[26,]  94.73063 21.43135\n[27,]  95.05170 19.43717\n[28,]  96.22693 21.61640\n[29,]  92.48276 21.02760\n[30,]  93.35447 20.47696\n[31,]  92.90601 20.40055\n[32,]  94.50024 18.53159\n[33,]  95.47237 25.34589\n[34,]  94.40944 23.07978\n[35,]  95.49546 23.32423\n[36,]  96.05576 24.25502\n[37,]  95.53962 23.93661\n[38,]  94.74604 23.96919\n[39,]  95.26376 22.25730\n[40,]  95.66196 26.41610\n[41,]  95.64449 21.98775\n[42,]  95.40740 22.74644\n[43,]  94.38739 24.17969\n[44,]  94.71565 22.24865\n[45,]  98.96838 23.02589\n[46,]  98.65407 23.83364\n[47,]  97.13646 22.50271\n[48,]  98.19561 22.76825\n[49,]  96.66042 23.45728\n[50,]  98.01229 23.68574\n[51,]  97.21860 23.21080\n[52,]  96.52970 21.22388\n[53,]  98.02872 20.26507\n[54,]  97.92760 21.39398\n[55,]  97.03704 20.46234\n[56,]  96.92822 20.85826\n[57,]  98.47076 14.08664\n[58,]  98.77515 10.98880\n[59,]  96.16459 21.96431\n[60,]  95.97341 20.95960\n[61,]  95.54920 21.47201\n[62,]  95.15564 20.94652\n[63,]  96.25009 22.62753\n[64,]  96.06338 20.49193\n[65,]  97.84218 15.79499\n[66,]  97.26005 17.13186\n[67,]  96.16788 19.63929\n[68,]  96.13921 20.04539\n[69,]  93.92534 19.67628\n[70,]  98.93770 12.36708\n[71,]  96.24776 16.89759\n[72,]  96.03014 17.26904\n[73,]  96.28511 16.66221\n[74,]  96.13431 16.82926\n\n\n\n\n4.12.1.2 Determining the cut-off distance\nFor the fixed distance weights, first, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\n\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.26   49.33   66.03   71.79   82.19  196.85 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 196.85 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\nwm_d197 &lt;- dnearneigh(coords, 0, 197, longlat = TRUE)\nwm_d197\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm197_lw &lt;- nb2listw(wm_d197, style = 'B')\nsummary(wm197_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 3  1  2  3  3  5  3  3  7  3  4  4  6  5  5  4  1  5  4  3 \n3 least connected regions:\n16 57 58 with 1 link\n3 most connected regions:\n39 42 62 with 20 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 828 1656 45160"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-adaptive-distance-weight-matrix",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-adaptive-distance-weight-matrix",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.2 Computing Adaptive distance weight matrix",
    "text": "4.12.2 Computing Adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours.\nHaving many neighbours smoothes the neighbour relationship across more neighbours.\nHowever, it is also possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n74 \n74 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n74 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 592 1036 19636"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---fixed-distance",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---fixed-distance",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.3 Computing GI statistics - Fixed distance",
    "text": "4.12.3 Computing GI statistics - Fixed distance\n\ngi.fixed &lt;- localG(Battles_2023$Incidents, wm197_lw)\ngi.fixed\n\n [1] -2.16251076 -2.26945742 -2.36608309 -2.28060756 -1.40482297 -1.73401341\n [7] -1.78091543 -2.09517182  1.58404920  3.74739192  1.77357701  1.43327980\n[13]  1.76425408  0.23868264 -0.46980525  0.66543351 -0.74168975 -1.33781214\n[19] -0.51148627 -0.52707475  0.62973486  0.75874852  2.45091315 -0.79794591\n[25] -0.46714759  0.78360503 -2.26619716  1.90728203 -0.19741884  0.53880930\n[31] -1.05349256 -1.42802753 -0.14931467  3.57984196  2.13682831  0.42422030\n[37]  1.15690841  2.06391206  2.02017424  0.24781966  2.12585948  2.66825096\n[43]  0.59177302  2.14376526  1.90967745  1.33161281  1.56642737  0.66429027\n[49]  3.18467135 -0.08013071  2.14013808  0.26220228 -0.29653380 -0.70655951\n[55] -1.52151868 -1.55549239  1.38510282  1.35703308  1.80376535  0.80349295\n[61]  1.58567210  0.37028741  1.42615140 -0.19363169  0.39364611 -0.86461442\n[67] -1.28913769 -1.59126895 -1.85739540  0.40386411 -2.04421053 -1.82676914\n[73] -1.82291155 -2.01994548\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)       Z(Gi) Pr(z != E(Gi))\n [1,] 0.068229167 0.17808219 0.0025805215 -2.16251076   0.0305788284\n [2,] 0.007285974 0.09589041 0.0015242874 -2.26945742   0.0232405238\n [3,] 0.020046863 0.12328767 0.0019038942 -2.36608309   0.0179774092\n [4,] 0.006769071 0.09589041 0.0015270818 -2.28060756   0.0225716798\n [5,] 0.094095941 0.16438356 0.0025033091 -1.40482297   0.1600739255\n [6,] 0.110194304 0.20547945 0.0030195729 -1.73401341   0.0829157041\n [7,] 0.065091864 0.15068493 0.0023098862 -1.78091543   0.0749262673\n [8,] 0.091196626 0.20547945 0.0029752444 -2.09517182   0.0361557216\n [9,] 0.193083573 0.12328767 0.0019414335  1.58404920   0.1131825243\n[10,] 0.289515279 0.12328767 0.0019676510  3.74739192   0.0001786828\n[11,] 0.202220460 0.12328767 0.0019806822  1.77357701   0.0761331435\n[12,] 0.267664828 0.19178082 0.0028030997  1.43327980   0.1517778927\n[13,] 0.219305224 0.13698630 0.0021770936  1.76425408   0.0776892110\n[14,] 0.091077575 0.08219178 0.0013859604  0.23868264   0.8113516776\n[15,] 0.040245203 0.05479452 0.0009590683 -0.46980525   0.6384941605\n[16,] 0.023995827 0.01369863 0.0002394576  0.66543351   0.5057732553\n[17,] 0.090454904 0.12328767 0.0019596135 -0.74168975   0.4582753304\n[18,] 0.074313409 0.13698630 0.0021946699 -1.33781214   0.1809576830\n[19,] 0.139005236 0.16438356 0.0024618296 -0.51148627   0.6090105985\n[20,] 0.125490196 0.15068493 0.0022849420 -0.52707475   0.5981416802\n[21,] 0.058130190 0.04109589 0.0007317001  0.62973486   0.5288680718\n[22,] 0.078141499 0.05479452 0.0009468162  0.75874852   0.4480030085\n[23,] 0.340266667 0.20547945 0.0030244162  2.45091315   0.0142494332\n[24,] 0.200730880 0.24657534 0.0033008582 -0.79794591   0.4249018788\n[25,] 0.167275574 0.19178082 0.0027517563 -0.46714759   0.6403942853\n[26,] 0.303858068 0.26027397 0.0030935821  0.78360503   0.4332719050\n[27,] 0.062418386 0.17808219 0.0026049511 -2.26619716   0.0234393145\n[28,] 0.355729167 0.24657534 0.0032752773  1.90728203   0.0564840763\n[29,] 0.061812467 0.06849315 0.0011451559 -0.19741884   0.8434997910\n[30,] 0.146966527 0.12328767 0.0019313068  0.53880930   0.5900184491\n[31,] 0.043455497 0.08219178 0.0013519884 -1.05349256   0.2921153017\n[32,] 0.030184751 0.08219178 0.0013263280 -1.42802753   0.1532839350\n[33,] 0.076701571 0.08219178 0.0013519884 -0.14931467   0.8813053367\n[34,] 0.363684489 0.17808219 0.0026880602  3.57984196   0.0003438021\n[35,] 0.322002635 0.20547945 0.0029736197  2.13682831   0.0326119589\n[36,] 0.171367177 0.15068493 0.0023769086  0.42422030   0.6714051589\n[37,] 0.252951981 0.19178082 0.0027957315  1.15690841   0.2473097820\n[38,] 0.232058669 0.13698630 0.0021219065  2.06391206   0.0390260550\n[39,] 0.396593674 0.27397260 0.0036842794  2.02017424   0.0433653170\n[40,] 0.047619048 0.04109589 0.0006928579  0.24781966   0.8042739429\n[41,] 0.387329591 0.26027397 0.0035720591  2.12585948   0.0335149615\n[42,] 0.435444414 0.27397260 0.0036621834  2.66825096   0.0076247282\n[43,] 0.134509081 0.10958904 0.0017733202  0.59177302   0.5540025915\n[44,] 0.370217451 0.24657534 0.0033264297  2.14376526   0.0320517005\n[45,] 0.132483082 0.06849315 0.0011228022  1.90967745   0.0561747560\n[46,] 0.113924051 0.06849315 0.0011639833  1.33161281   0.1829874538\n[47,] 0.307425214 0.21917808 0.0031738082  1.56642737   0.1172486006\n[48,] 0.137802607 0.10958904 0.0018038490  0.66429027   0.5065045498\n[49,] 0.339932274 0.17808219 0.0025828347  3.18467135   0.0014491849\n[50,] 0.092809365 0.09589041 0.0014784223 -0.08013071   0.9361332989\n[51,] 0.287356322 0.17808219 0.0026070606  2.14013808   0.0323436091\n[52,] 0.261594581 0.24657534 0.0032811258  0.26220228   0.7931654986\n[53,] 0.071372753 0.08219178 0.0013311533 -0.29653380   0.7668224621\n[54,] 0.080156658 0.10958904 0.0017352153 -0.70655951   0.4798402603\n[55,] 0.123498695 0.20547945 0.0029031487 -1.52151868   0.1281297272\n[56,] 0.131920530 0.21917808 0.0031468082 -1.55549239   0.1198288467\n[57,] 0.035637728 0.01369863 0.0002508843  1.38510282   0.1660210309\n[58,] 0.034807642 0.01369863 0.0002419663  1.35703308   0.1747706992\n[59,] 0.366230366 0.26027397 0.0034505972  1.80376535   0.0712681006\n[60,] 0.292600313 0.24657534 0.0032811258  0.80349295   0.4216898674\n[61,] 0.354370214 0.26027397 0.0035214197  1.58567210   0.1128137120\n[62,] 0.295910393 0.27397260 0.0035100061  0.37028741   0.7111683500\n[63,] 0.299541655 0.21917808 0.0031753179  1.42615140   0.1538246447\n[64,] 0.222019781 0.23287671 0.0031438461 -0.19363169   0.8464642818\n[65,] 0.066967845 0.05479452 0.0009563271  0.39364611   0.6938423348\n[66,] 0.108660999 0.15068493 0.0023623728 -0.86461442   0.3872504579\n[67,] 0.124184712 0.19178082 0.0027494435 -1.28913769   0.1973502243\n[68,] 0.131770833 0.21917808 0.0030172252 -1.59126895   0.1115490605\n[69,] 0.041992697 0.12328767 0.0019156610 -1.85739540   0.0632549198\n[70,] 0.036378335 0.02739726 0.0004945225  0.40386411   0.6863126506\n[71,] 0.063607925 0.16438356 0.0024302999 -2.04421053   0.0409327537\n[72,] 0.096329081 0.19178082 0.0027302372 -1.82676914   0.0677344872\n[73,] 0.085438916 0.17808219 0.0025828347 -1.82291155   0.0683167878\n[74,] 0.065070276 0.16438356 0.0024173270 -2.01994548   0.0433890436\nattr(,\"cluster\")\n [1] Low  Low  Low  Low  Low  High Low  Low  Low  Low  High Low  High High High\n[16] Low  Low  High Low  Low  High High High Low  Low  High Low  Low  Low  Low \n[31] Low  Low  Low  High Low  High Low  Low  High Low  High High Low  High Low \n[46] High High High Low  High Low  Low  Low  Low  Low  High High Low  Low  Low \n[61] High Low  High Low  High High Low  Low  Low  High Low  Low  Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = Battles_2023$Incidents, listw = wm197_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nNext, we will join the Gi values to their corresponding sf data frame by using the code below.\n\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nBattles_2023.gi\n\nSimple feature collection with 74 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DT   DT_PCODE    DT_MMR PCode_V year event_type Incidents Fatalities\n1    Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023    Battles         4          3\n2     Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023    Battles         1          1\n3     Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023    Battles         3          1\n4      Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023    Battles         3          2\n5        Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023    Battles        50        270\n6     Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023    Battles        87        493\n7        Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023    Battles        34         59\n8  Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023    Battles        50         93\n9       Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023    Battles        27         89\n10      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023    Battles        48        210\n   Z.Incidents lag_Incidents gstat_fixed                       geometry\n1  -0.80534765      18.20000   -2.162511 MULTIPOLYGON (((95.12637 18...\n2  -0.85573862       3.00000   -2.269457 MULTIPOLYGON (((95.04462 15...\n3  -0.82214464       2.50000   -2.366083 MULTIPOLYGON (((94.27572 15...\n4  -0.82214464       3.00000   -2.280608 MULTIPOLYGON (((95.20798 15...\n5  -0.03268604      40.66667   -1.404823 MULTIPOLYGON (((95.90674 18...\n6   0.58880265      29.00000   -1.734013 MULTIPOLYGON (((96.17964 19...\n7  -0.30143790      31.40000   -1.780915 MULTIPOLYGON (((95.70458 19...\n8  -0.03268604      35.60000   -2.095172 MULTIPOLYGON (((95.85173 18...\n9  -0.41901684      53.00000    1.584049 MULTIPOLYGON (((93.36931 24...\n10 -0.06628002      62.00000    3.747392 MULTIPOLYGON (((93.35213 23...\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe codes above performs three tasks.\n\nFirst, it converts the output vector (i.e. gi.fixed) into r matrix object by using as.matrix().\nNext, cbind() is used to join Battles_2023 and gi.fixed matrix to produce a new SpatialPolygonDataFrame called Battles_2023.gi.\nLastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n4.12.3.1 Mapping Gi values with Fixed distance weights\nThe code below plots the Gi values derived using fixed distance weight matrix, for event type==Battles in 2023.\n\nGimap &lt;-tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(Battles_2023.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"Fixed Distance\\nlocal Gi\") +\n  tm_borders(alpha = 0.5)\n\nGimap"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---adaptive-distance",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---adaptive-distance",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.4 Computing GI statistics - Adaptive distance",
    "text": "4.12.4 Computing GI statistics - Adaptive distance\nThe code below is used to compute the Gi values for Incidents of Battles in 2023 by using an adaptive distance weight matrix (i.e knb_lw).\n\ngi.adaptive &lt;- localG(Battles_2023$Incidents, knn_lw)\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\ndatatable(Battles_2023.gi)\n\n\n\n\n\n\n4.12.4.1 Mapping Gi values with Adaptive distance weights\nThe code below plots the Gi values derived using adaptive distance weight matrix for event type == Battles in 2023.\n\nGimap &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(Battles_2023.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"Adaptive Distance\\nlocal Gi\") + \n  tm_borders(alpha = 0.5)\n\nGimap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user inputs:\n\nYear\nEvent type\nData Classification type, and\nNumber of clusters for the Adaptive weight matrix"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html",
    "href": "R-ex/R-Ex3/R_Ex3.html",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "",
    "text": "DataVis Makeover\n\n\n\nRemaking a peer’s original design by improving the clarity and aesthetics of charts by creating an alternative design."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#loading-r-packages",
    "href": "R-ex/R-Ex3/R_Ex3.html#loading-r-packages",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\n\npacman::p_load(tidyverse, haven, ggrepel, ggthemes, hrbrthemes, patchwork, intsvy, ggdist, ggridges,colorspace, plotly)"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#importing-the-data-set",
    "href": "R-ex/R-Ex3/R_Ex3.html#importing-the-data-set",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.2 Importing the Data set",
    "text": "2.2 Importing the Data set\n\nstu_qqq_SG &lt;- \n  read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#removing-missing-values-and-converting-data-types",
    "href": "R-ex/R-Ex3/R_Ex3.html#removing-missing-values-and-converting-data-types",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.3 Removing missing values and Converting data types",
    "text": "2.3 Removing missing values and Converting data types\nRows with missing values for ESCS were removed before further analysis.\n\n\nShow the code\nstu_qqq_SG_clean &lt;- stu_qqq_SG[complete.cases(stu_qqq_SG[, \"ESCS\"]), ]\n\n\nCNTSCHID and CNTSTUID’s data types were changed to character.\n\n\nShow the code\nstu_qqq_SG_clean$CNTSCHID &lt;- as.character(stu_qqq_SG_clean$CNTSCHID)\nstu_qqq_SG_clean$CNTSTUID &lt;- as.character(stu_qqq_SG_clean$CNTSTUID)"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#binning-variable-escs",
    "href": "R-ex/R-Ex3/R_Ex3.html#binning-variable-escs",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.4 Binning Variable ESCS",
    "text": "2.4 Binning Variable ESCS\nThe ESCS variable was binned into quantiles using the mutate() and cut() function\n\n\nShow the code\nstu_qqq_SG_clean &lt;- stu_qqq_SG_clean %&gt;%\n  mutate(ESCS_recoded = cut(ESCS,breaks=quantile(ESCS,c(0,0.25,0.5,0.75,1)),labels=c(\"Very Low\",\"Low\",\"Medium\",\"High\"),include.lowest=TRUE))"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#recoding-st004d01t-gender-variable",
    "href": "R-ex/R-Ex3/R_Ex3.html#recoding-st004d01t-gender-variable",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.5 Recoding ST004D01T (Gender) variable",
    "text": "2.5 Recoding ST004D01T (Gender) variable\nVariable ST004D01T, was recoded to labels to “Female” and “Male” respectively.\n\n\nShow the code\nstu_qqq_SG_clean$ST004D01T &lt;- recode(stu_qqq_SG_clean$ST004D01T, \"1\" = \"Female\", \"2\" = \"Male\")"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#general-distribution-of-students-performance-in-math-science-and-reading",
    "href": "R-ex/R-Ex3/R_Ex3.html#general-distribution-of-students-performance-in-math-science-and-reading",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.1 General distribution of students’ performance in Math, Science and Reading",
    "text": "3.1 General distribution of students’ performance in Math, Science and Reading\nFirst, the author had created a data folder path to save any data files generated:\n\n\nShow the code\ndata_folder_path &lt;- file.path(getwd(), \"data\")\n\n\nThe code below uses the instvy package to extract the composite Means and Standard Deviations of PV values across the population of students that took the assessment.\n\n\nShow the code\npvmathgeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_general\",folder=data_folder_path)\n\npvmathgeneral$subject &lt;- \"Math\"\n\npvreadgeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_general\",folder=data_folder_path)\n\npvreadgeneral$subject &lt;- \"Reading\"\n\npvsciegeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_general\",folder=data_folder_path)\n\npvsciegeneral$subject &lt;- \"Science\"\n\nmergedgeneral &lt;- rbind(pvmathgeneral,pvreadgeneral,pvsciegeneral)\nmergedgeneral %&gt;% relocate(subject)\n\n\n  subject Freq   Mean s.e.     SD  s.e\n1    Math 6559 575.27 1.26 102.68 0.93\n2 Reading 6559 543.25 1.91 105.73 1.16\n3 Science 6559 561.97 1.33  99.02 1.10\n\n\nA plot to show the general distribution of Math, Reading and Science scores was generated using the code below.\n\nThe original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the distribution of Student scores across the 3 subjects.\n\n\n\np1&lt;- ggplot(mergedgeneral, aes(x = subject, y = Mean,colour=subject)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    aes(colour=subject),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD),\n    width = 0.5,\n    size=0.8,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Scores across Math, Reading and Science across all 15-year olds who took PISA 2022 in SGP\") +\n  ylab(\"Scores\") +\n  xlab(\"Subject\")+\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 50))+\n  coord_flip() +\n  theme_minimal(base_size = 12)\np1\n\n\n\n\n\n3.1.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across subjects, I am unclear as to which score this is showing. Is it PV1 or average of the PV values?\nGraph is titled as “Scores across Math, Reading and Science”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this plot only has 3 observations and 6 variables. Upon inspecting the data set, it is actually the composite mean score for each subject for the entire student cohort.\n\n\nThe code and subset used is not suitable to plot a Distribution of student scores across subjects. I will extract a new subset with all PV1 scores at the individual student level. This will then be used to plot the distribution of student scores for the different subjects.\n\n\n\nAesthetics\n\nThere is already a legend on the right which identifies subjects by colour, thus making the Y axis labels seem redundant.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated. This contributes to “extra” space and additional non-data ink.\nUnable to decipher the actual scores even with the X-axis tick marks.\nSince the subjects are clearly labelled in the Y axis, the label “Subject” can be removed, to minimize non-data ink.\n\n\nTo remove the legend or Y-Axis labels, to reduce non-data ink.\nShorten the X-axis, or extract new data points to populate the correct range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\nTo remove the label “Subject”, since it does not provide additional information to readers.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nUpon closer examination of the code used, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD),\n    #width = 0.5,\n    #size=0.8,\n    #position = position_dodge(width = 0.5)\n\nI would conclude that this plot is actually comparing the mean subject scores across subjects.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(mergedgeneral)\n\n  Freq   Mean s.e.     SD  s.e subject\n1 6559 575.27 1.26 102.68 0.93    Math\n2 6559 543.25 1.91 105.73 1.16 Reading\n3 6559 561.97 1.33  99.02 1.10 Science\n\n\nThe mergedgeneral subset table only has 3 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe code and data subset used by the author will not be able to plot a Distribution of student scores across subjects.\nI will revert to the original large data set to extract PV1 scores which are available for all students to create new plots to visualize the distribution of scores.\n\n\n\n\n3.1.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.1.3 Remake\nThe original data sub set and plot is unable to exhibit the distribution of student scores across subjects, due to the lack of scores at an individual student level.\nFirst, I will use the code below derive some reference lines (mean and median) for our histograms.\nTo populate the distribution of scores for the student cohort, I will use PV1 scores, which are available for all students.\n\n\nShow the code\nmean_pv1math &lt;- mean(stu_qqq_SG$PV1MATH)\nmedian_pv1math &lt;- median(stu_qqq_SG$PV1MATH)\nmean_pv1read &lt;- mean(stu_qqq_SG$PV1READ)\nmedian_pv1read &lt;- median(stu_qqq_SG$PV1READ)\nmean_pv1scie &lt;- mean(stu_qqq_SG$PV1SCIE)\nmedian_pv1scie &lt;- median(stu_qqq_SG$PV1SCIE)\n\n\nNext, I will use the code below and extract data points (PV1 scores) from the original large data set to create Histograms with Density plots for each subject.\n\n\nShow the code\np_1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightblue') +\n  geom_density(color = \"purple\") + \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Math Scores\", y = \"Density\") + # Change label to 'Density'\n  geom_vline(xintercept = mean_pv1math, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1math, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1math, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1math, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1math, y = 0.0025, label = paste(\"Median:\", round(median_pv1math, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightgreen') +\n  geom_density(color = \"purple\") + \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Reading Scores\", y = \"Density\") + \n  geom_vline(xintercept = mean_pv1read, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1read, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1read, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1read, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1read, y = 0.0025, label = paste(\"Median:\", round(median_pv1read, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightpink') +\n  geom_density(color = \"purple\") +  \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Science Scores\", y = \"Density\") + \n  geom_vline(xintercept = mean_pv1scie, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1scie, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1scie, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1scie, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1scie, y = 0.0025, label = paste(\"Median:\", round(median_pv1scie, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Combined Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\nFinally, using the code below, I will create a composite graph for all the histograms and density plots.\n\n\nShow the code\npatch1 &lt;- (p_1 + p_2) / (p_3 + p_4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\",\n                subtitle = \"All subjects exhibit a slight left-skewed distribution\" )\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe revised plot consists of a patchwork of 3 histograms with density curves, and a combined density curve overlay for the 3 subjects.\n\nReference lines for the mean and median scores are added to enable readers to infer the skewness of the distribution of student scores.\n\nThe revised plot will enable readers to quickly infer the nature of the distributions of the subject scores for the student cohort."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-across-schools",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-across-schools",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.2 Distribution of students’ performance in Math, Science and Reading across schools",
    "text": "3.2 Distribution of students’ performance in Math, Science and Reading across schools\nThe author has used the pisa.mean.pv function from intsvypackage, to obtain the composite Mean and Standard Deviation values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each school.\nThe code below generates the composite PV values.\n\n\nShow the code\npvmathsch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bysch\",folder=data_folder_path)\n\npvreadsch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bysch\",folder=data_folder_path)\n\npvsciesch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bysch\",folder=data_folder_path)\n\n\nA plot to show the distribution of students performance in Math, Reading and Science scores across schools was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the subject scores by each school id (164 schools).\n\n\n\np2&lt;- ggplot(pvmathsch, aes(x = as.factor(CNTSCHID), y = Mean)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=1.5) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    position = position_dodge(width = 0.75),\n    color = \"red\"\n  ) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD),\n    width = 0.8,\n    size=1,\n    position = position_dodge(width = 10)\n  ) +\n  labs(title = \"Math Scores by School\") +\n  ylab(\"Math Score\") +\n  xlab(\"School ID\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=100))+\n  coord_flip() +\n  theme_minimal(base_size=20)             \n\n\n\n\n\n3.2.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across schools, I am unclear as to which score this is showing. Is it showing me the range of student scores by school ID?\nGraph is titled as “Subject Scores by school”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this visualization only has 164 observations and 6 variables. Upon inspecting the data set, this subset is basically the composite mean subject scores for each school.\n\n\nThe original subset can still be used, but the code will need to be changed for the revised visualization. This new visualization will retain the original intent to show the different mean scores for each of the 164 schools to visualize the relative performance between schools.\n\n\n\nAesthetics\n\nThe use of a vertically long plot to compare the scores across schools makes it visually challenging and difficult to spot differences in scores, and identify any clusters.\nSince there are 164 schools, the use of a vertically long format makes it difficult to read the school IDs on the Y-axis. There are too many schools to make identification off the Y-axis easy.\nThe distance between the X-axis, which shows the scores is too far apart from the error bars. I am unable to decipher what are the actual scores or score range for each error bar.\nThe graph is stiched side by side, resulting in the Y-axis being repeated 3 times. The order of the School IDs are all the same, hence this creates additional non-data ink, that makes the graph too “busy”.\n\n\nTo revise the design of a vertically long format and re-make it to a horizontally compact visualization, by using bubble plots.\nTo minimize axis labels, we can drop axis labels for school id, and only identify and directly annotate the top & bottom 5 schools instead.\nTo remove the 2 other repetitive Y-axes to reduce non-data ink.\n\n\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(head(pvmathsch))\n\n  CNTSCHID Freq   Mean  s.e.    SD   s.e\n1 70200001   55 725.21  9.34 59.23  6.38\n2 70200002   37 536.19 17.09 90.27 14.50\n3 70200003   36 739.92 12.30 59.23  7.70\n4 70200004   56 509.61 12.84 86.63  7.71\n5 70200005   37 548.39 13.10 86.30  9.18\n6 70200006   36 485.30 13.90 76.47  8.86\n\n\nThe pvmathsch subset table only has 164 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe data subset can only be used to visualize the mean scores per school ID.\nThe original graph is not of a suitable design to help readers easily and quickly understand the disparity in performances between schools.\nI will use a new compact visualization to enable the reader to quickly see the differences in school performance.\n\n\n\n\n3.2.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.2.3 Remake\nThe original plot does not enable the reader to quickly capture information from the data points.\nUsing the exact same data set, I will use the code below to re-make the original graph into a Bubble plots instead. I will employ color as a means to differentiate the mean scores.\nI have also added reference lines to show the 10th and 90th percentile scores to help readers easily identify the schools belonging to these two opposite segments.\nLastly, I have also added annotations to help readers to quickly identify the top 5 and bottom 5 schools with the best and worst mean scores.\n\n\nShow the code\nlibrary(ggrepel)\n\n# Identify the top 5 and bottom 5 schools\ntop_5 &lt;- pvmathsch %&gt;% \n  arrange(desc(Mean)) %&gt;%\n  slice(1:5)\n\nbottom_5 &lt;- pvmathsch %&gt;% \n  arrange(Mean) %&gt;%\n  slice(1:5)\n\n# Base plot with points colored by the mean score\np1 &lt;- ggplot(pvmathsch, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(color = Mean), alpha = 0.5, size = 7) +\n  scale_color_gradient(low = \"red\", high = \"green\") +\n  labs(title = \"Mean Math Scores per School\",\n       subtitle = \"5 Schools with the Best & Worst mean scores\",\n       x = \"School ID\",\n       y = \"Mean Math Scores\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.grid.major = element_blank())  # Remove major grid lines\n       \n\n# Annotate the top 5 and bottom 5 schools\np1 &lt;- p1 +\n  geom_text_repel(\n    data = top_5, \n    aes(label = CNTSCHID, y = Mean), \n    color = \"blue\", \n    size = 3, \n    nudge_y = 12,   # Adjust nudge_y if necessary to move text up or down\n    direction = \"y\"\n  ) +\n  geom_text_repel(\n    data = bottom_5, \n    aes(label = CNTSCHID, y = Mean), \n    color = \"blue\", \n    size = 3, \n    nudge_y = -12,  # Adjust nudge_y if necessary to move text up or down\n    direction = \"y\"\n  )\n\n# Calculate the 10th and 90th percentiles\npercentile10 &lt;- quantile(pvmathsch$Mean, probs = 0.10, na.rm = TRUE)\npercentile90 &lt;- quantile(pvmathsch$Mean, probs = 0.90, na.rm = TRUE)\n\n# Add horizontal lines for the 10th and 90th percentiles to your plot\np1 &lt;- p1 + \n  geom_hline(yintercept = percentile10, linetype = \"dashed\", color = \"blue\", size = 0.5) +\n  geom_hline(yintercept = percentile90, linetype = \"dashed\", color = \"blue\", size = 0.5) +\n  geom_text(aes(x = Inf, y = percentile10, label = paste(\"10th Percentile:\", round(percentile10, 2))), \n            hjust = 1.05, vjust = 0, color = \"blue\", size = 3) +\n  geom_text(aes(x = Inf, y = percentile90, label = paste(\"90th Percentile:\", round(percentile90, 2))), \n            hjust = 1.05, vjust = 1, color = \"blue\", size = 3)\n\np1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe use of colour can help readers quickly differentiate the scores between schools.\n\nI have annotated school IDs for the top and bottom 5 schools for readers to quickly identify the “best” and “worst” schools.\nI have also added reference lines for the 10th and 90th Percentile. Here we can quickly see that while most of the 164 schools attain the same range of mean scores, there are a number of schools that outperform the rest.\nAnnotating all the 164 school IDs will add too much ink to the plot. Hence, we can consider having another interactive plot (below) for readers to examine the data points in more detail at their descretion.\n\n\nFor interactivity, I have used ggplotly() to convert this to an interactive graph. With the added interactivity, readers can use the tooltip function to get more information on the data points.\n\n\nShow the code\nlibrary(plotly)\n\n# Convert to an interactive plot\nggplotly(p1, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the previous annotations generated in the static plot may not appear after turning it into an interactive plot. Hence, we can use both static and interactive plots together if needed. For example, static charts to quickly summarize and communicate information to reader, and an interactive plot for readers to drill down in more detail, eg use of zoom, identifying specific data points via tooltip etc."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.3 Distribution of students’ performance in Math, Science and Reading by gender",
    "text": "3.3 Distribution of students’ performance in Math, Science and Reading by gender\nThe author had used the pisa.mean.pv function from intsvy package to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each gender.\nThe code below was used to generate the composite PV values for Math, Reading and Science by gender.\n\n\nShow the code\npvmathgenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bygenderonly\",folder=data_folder_path)\n\npvreadgenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bygenderonly\",folder=data_folder_path)\n\npvsciegenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bygenderonly\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores by gender was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the subject scores by gender.\n\n\n\nggplot(pvmathgenderonly, aes(x = as.factor(ST004D01T), y = Mean,fill=ST004D01T)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    aes(colour=ST004D01T),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    width = 0.5,\n    size=0.8,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Math Scores by Gender\") +\n  ylab(\"Math Score\") +\n  xlab(\"Gender\")+\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 100))+\n  scale_fill_discrete(name = \"Gender\") +\n  scale_color_discrete(name = \"Gender\") +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n3.3.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to see the differences in scores between the genders, I am unclear as to which score this is showing. Is it showing the range of PV1 scores or average of PV values?\nGraph is titled as “Subject Scores by Gender”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for the plot only has 2 observations and 6 variables. Upon inspecting the data set, this subset is actually the composite mean score for each gender across the subjects.\n\n\nThe code and subset used is not suitable to plot a range of scores for each gender across subjects.\nI will extract a new subset with all PV1 scores at the individual student level. This will then be used to plot the distribution and range of scores for the different genders across all subjects.\n\n\n\nAesthetics\n\nThe graphs are already well labelled in the x and y axes, and identifies the different genders clearly. The use of different colors per gender together with the legend is not necessary.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated.\nUnable to decipher the actual scores even with the granularity in X-axis tick marks.\n\n\nTo remove the legend since there are only two categories of gender and they are already differentiated by colour.\nShorten the X-axis, or extract new data points to correctly populate the range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\nThe new visualization will need to enable readers to quickly differentiate the performance between genders across subjects.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nupon closer examination of the code below, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    #width = 0.5,\n    #size=0.8,\n    #position = position_dodge(width = 0.5)\n\nThe original data set and plot is instead comparing the mean subject scores across different genders.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(pvmathgenderonly)\n\n  ST004D01T Freq   Mean s.e.     SD  s.e\n1    Female 3227 569.01 1.70  97.49 1.14\n2      Male 3332 581.30 1.74 107.09 1.36\n\n\nThe pvmathgenderonly data set only has 2 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThis data set can only be used to compare the overall mean scores for genders across the 3 subjects.\nThe visualization needs to be done differently to enable the reader to obtain more information regarding the performance between genders.\nI will extract a new subset and create new graphs to show the range and distribution of scores between the genders across subjects.\nI will use PV1 scores which are available for all students to create new plots to visualize the distribution of scores.\n\n\n\n\n3.3.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.3.3 Remake\nThe original data subset and plot is unable to exhibit the range and distribution of subject scores across genders, due to the lack of scores at an individual student level.\nTherefore, I will extract data from the original data set to create new subsets for the new plots using the code below.\nIn terms of data, I will use the PV1 scores which are available for all students.\n\n\nShow the code\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nMath_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1MATH, na.rm = TRUE)  # Calculate the mean of PV1MATH, removing NA values\n  )\n\nRead_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1READ, na.rm = TRUE)  # Calculate the mean of PV1READ, removing NA values\n  )\n\nSCIE_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1SCIE, na.rm = TRUE)  # Calculate the mean of PV1SCIE, removing NA values\n  )\n\n\nNext, I will use the code below to create new box plots for each subject. This will show the range of scores for the genders in different subjects.\n\n\nShow the code\n# Create the plot using the subset_data\nbxp1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust =4, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Math Scores\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +  \n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\nbxp2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust = 4.25, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Reading Scores\") +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightgreen\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\") \n  \n\nbxp3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust = 4, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Science Scores\") +\n  scale_fill_manual(values = c(\"lightpink\", \"lightpink\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\nBesides showing the differences in scores through box plots, I will also plot ridgeline plots, segmented into 4 quantiles, to display the distribution and difference in subject scores between genders across the subjects.\n\n\nShow the code\nrp1 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1MATH, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  )  +\n  labs(title = \"Math Scores\\nacross genders\")\n\nrp2 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1READ, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  )  +\n  labs(title = \"Reading Scores \\nacross genders\")\n\nrp3 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1SCIE, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE,\n    alpha = 0.5 ) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  ) +\n  labs(title = \"Science Scores\\nacross genders\")\n\n\nI will use the code below to create a composite graph.\n\n\nShow the code\npatch3 &lt;- (rp1+ bxp1)/(rp2+bxp2)/(rp3+bxp3) + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\",\n                subtitle = \"Female students outperform in Reading\")\n\npatch3 & theme(panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe revised plot consists of a patchwork of ridgeline plots and box plots for each subject.\nThe ridgeline plot been further segmented into 4 quantiles, differentiated by colour. This not only provides insights into the distribution of the scores but also enables readers to quickly understand the differences in performance between genders.\nThe box plots, further help readers to understand the differences in scores between genders. The range of scores can be seen along with the mean scores."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-math-reading-and-science-scores-by-socioeconomic-status",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-math-reading-and-science-scores-by-socioeconomic-status",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.4 Distribution of Math, Reading and Science scores by socioeconomic status",
    "text": "3.4 Distribution of Math, Reading and Science scores by socioeconomic status\nThe author had previously created a new variable, “ESCS_recoded”, which bins the socio-economic (ESCS) score into four quantiles - “Very Low”, “Low”, “Medium” and “High”. The author had used the pisa.mean.pv function from intsvypackage to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” and grouped by the ESCS score.\nThe code below was used to generate the composite PV values for Math, Reading and Science.\n\n\nShow the code\npvmathescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmathescs\",folder=data_folder_path)\n\npvreadescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvreadescs\",folder=data_folder_path)\n\npvscieescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscieescs\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores across schools by socioeconomic status was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars to represent the scores across the 4 categories of the ESCS score (High, Medium, Low, Very Low).\n\n\n\nggplot(pvscieescs, aes(x = ESCS_recoded, y = Mean,fill=ESCS_recoded))+\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.5),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 10,\n    size = 2,\n    aes(colour=ESCS_recoded),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ESCS_recoded),\n    width = 0.5,\n    size=1,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Science Scores by Socioeconomic Status\") +\n  ylab(\"Science Score\") +\n  xlab(\"Socioeconomic Status\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=200))+\n  coord_flip() +\n  theme_minimal(base_size=10)\n\n\n\n\n\n3.4.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores between the ESCS categories, I am unclear as to which score this is showing. Is it showing me the range of PV1 scores or average of PV values?\nGraph is titled as “Subject Scores by Socioeconomic Status”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for the plots only has 4 observations and 6 variables. Upon inspecting the data set, this is actually just the overall mean scores for each of the newly binned ESCS score category.\n\n\nThe code and subset used is not suitable to plot a range or distribution of scores for each ESCS score category.\nI will extract a new subset with all PV1 scores at the individual student level together with ESCS scores per student. I will then plot the relationship between the two continuous variables (subject scores and ESCS score) via scatter plots.\nA scatter plots will enable readers to quickly infer the relationships between subject scores and socioeconomic statuses.\n\n\n\nAesthetics\n\nThe Y-axis is already well labelled and identifies the different socioeconomic segments clearly, the use of different colors per segment together with legend is not necessary.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated.\nEven with the background grid lines and X axis tick marks, I am unable to decipher the actual scores clearly.\n\n\nTo remove the legend since the ESCS segments are already differentiated by colour and identifiable through the Y-axis labels.\nShorten the X-axis, or extract new data points to correctly populate the range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nUpon closer examination of the code below, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD,color=ESCS_recoded),\n    #width = 0.5,\n    #size=1,\n    #position = position_dodge(width = 0.5)\n\nHence the original plot can only be used to compare the mean subject scores across different ESCS segments.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(pvscieescs)\n\n  ESCS_recoded Freq   Mean s.e.    SD  s.e\n1     Very Low 1640 503.73 3.14 97.91 1.97\n2          Low 1640 546.33 2.61 93.23 1.75\n3       Medium 1640 584.61 2.31 86.07 2.04\n4         High 1639 610.62 3.23 84.01 2.21\n\n\nThe data subset used for the plots only has 4 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThis subset and the code used is not able to plot the range of scores per ESCS segment, and can only be used to compare the composite mean scores of each of the 4 ESCS segments.\nAlthough it is useful to compare the mean scores between different ESCS segments, a different set of data and plots can be used instead to communicate more information about the strength or weakness of the relationship between subject scores and socioeconomic statuses.\n\n\nAdditionally, the binning of the ESCS score, while helpful may be limited in usefulness, for the following reasons:-\n\nBinning the ESCS scores adds in an element of subjectivity. For example, why should there be 4 segments of socioeconomic statuses instead of 3, or 6?\nCould the number of socioeconomic segments be applicable for other countries? For example, would a “Low segment” in Singapore be equivalent to a “Low segment” in another country?\n\n\n\n3.4.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.4.4 Remake\nInstead of binning the ESCS score and transforming it to a categorical type, I will keep the ESCS score as a continuous variable.\nI will use scatter plots to visualize the strength or weaknesses of the relationships between the socioeconomic statuses and subject scores.\nDrawing inspiration from Prof Kam’s Lesson 3 and well as additional useful references, I will proceed to plot Scatter plots with Marginal Histograms.\nIn terms of data, I will use the PV1 scores which at the individual student level.\nFor this visualization, I will use the ggstatplot package. (Patil 2021)\nFor more information on this package and how it can be used to create graphics from statistical tests included in the plots themselves, please refer to this link.\n\nlibrary(ggstatsplot)\n\nCreating new data subset for visualization.\n\n\nShow the code\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\n\nWe will use the code below from ggstatplot package to generate the plot.\n\n\nShow the code\nggscatterstats(\n  data = subset_ESCS_PV1,                                          \n  x = ESCS,                                                  \n  y = PV1MATH,\n  xlab = \"Socioeconomic score (ESCS)\",\n  ylab = \"Math scores\",\n  marginal = TRUE,\n  marginal.type = \"histogram\",\n  centrality.para = \"mean\",\n  margins = \"both\",\n  title = \"Relationship between Socio-economic and Math scores\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe benefit of this plot is that it shows both the correlation between the two continous variables as well as their respective distributions, and includes important statistics like the Pearson coefficient.\nFrom the plot above, the pearson coefficient of 0.42 indicates that there is a weak positive relationship between ESCS scores and Math scores.\nThe marginal histograms for both variables also enables readers to additionally infer the distribution of these variables. For example, Math scores resemble a normal distribution, while ESCS scores resemble a left-skewed distribution."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender-across-schools",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender-across-schools",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.5 Distribution of students’ performance in Math, Science and Reading by gender across schools",
    "text": "3.5 Distribution of students’ performance in Math, Science and Reading by gender across schools\nThe author had intended to analyse whether there are differences in performances in Math, Reading and Science between genders at a more granular level - within schools.\nThe author had used the pisa.mean.pv function from intsvypackage to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each school and by gender.\nThe below code was used to generate the composite PV values.\n\n\nShow the code\npvmathgender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bygender\",folder=data_folder_path)\n\npvreadgender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bygender\",folder=data_folder_path)\n\npvsciegender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),by= c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bygender\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores across schools by gender, was generated using the code below.\n\nThe Original PlotThe code\n\n\n\nThe author had created a series of 2 error bars for each school ID across the 3 subjects to represent the subject scores for each gender for all schools (164 schools).\n\n\n\np8&lt;- ggplot(pvmathgender, aes(x = as.factor(CNTSCHID), y = Mean,fill=ST004D01T))+\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=1.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 10,\n    aes(colour=ST004D01T),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    width = 1,\n    size=2,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Math Scores by Gender across Schools\") +\n  ylab(\"Math Score\") +\n  xlab(\"School ID\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=200))+\n  coord_flip() +\n  theme_minimal(base_size=50)+\n  theme(legend.position=\"none\")\n\n\n\n\n\n3.5.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across schools, I am unclear as to which score this is showing. Is it showing me the range of gender scores by school ID?\nGraph is titled as “Subject Scores by Gender across school”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this visualization has 301 observations and 7 variables. Upon inspecting the data set, it is basically the overall mean subject scores for male and female students per school\n\n\nPrevious visualization had already analysed the differences per school. To further make it more granular by introducing gender, does not seem to add further value in terms of insights.\nWe can retain the original intent to show the granular difference in performance for the schools, by examining the actual disparity between the best and the worst schools instead.\n\n\n\nAesthetics\n\nThe use of a vertically long plot to compare the scores across schools makes it visually challenging and difficult to spot differences between schools, and identify any clusters.\nSince there are 164 schools, the use of a vertically long format makes it difficult to read the Y-axis labels. There are too many schools to make identification off the Y-axis easy. This is further compounded with the additional splitting of genders for every school.\nThe distance between the X-axis, which shows the scores is too far apart from the error bars. I am unable to decipher what are the actual scores or score range for each error bar.\nThe graph is patched side by side, resulting in the Y-axis being repeated 3 times. The order of the School IDs are all the same, hence this creates additional non-data ink, that makes the graph too “busy”.\n\n\nTo revise the design of a vertically long format and re-make it to a horizontally compact visualization, by using ridge line plots to show the differences in performance only between the best and worst schools.\nTo minimize axis labels, we will only identify and plot the top & bottom 5 schools instead.\n\n\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(head(pvmathgender))\n\n  CNTSCHID ST004D01T Freq   Mean  s.e.    SD   s.e\n1 70200001      Male   55 725.21  9.34 59.23  6.38\n2 70200002    Female   15 537.70 27.07 80.64 15.79\n3 70200002      Male   22 535.13 21.92 96.03 20.81\n4 70200003    Female    7 739.65 22.04 39.42 14.39\n5 70200003      Male   29 739.99 14.58 62.67  8.56\n6 70200004    Female   28 505.32 19.68 91.11 11.94\n\n\nThe pvmathgender subset table has 307 observations and 7 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe data subset can only be used to visualize the mean scores of the two genders for every school.\nThe original graph is not of a suitable design to help readers easily and quickly understand the disparity in performances between schools. By adding a further split into genders per school further complicates this visually.\nI will use a new compact visualization to enable the reader to quickly identify the disparity in school performance. Instead of analyzing all schools, we can narrow our focus just to the “best” and “worst” schools.\n\n\n\n\n3.5.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.5.3 Remake\nThe author had previously extracted the mean scores per school across the 3 subjects. I will use the same sub sets to extract the top and bottom 5 School IDs for each subject.\n\n\nShow the code\n# Identify the top and bottom 5 schools\ntop_bottom_math &lt;- pvmathsch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\ntop_bottom_read &lt;- pvreadsch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\ntop_bottom_scie &lt;- pvsciesch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\n\nNext I will filter the data set and obtain PV1 student scores from the top and bottom 5 schools only.\n\n\nShow the code\n# Filter the original dataset to include only the selected schools\n\ntop_bottom_mathsch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_math)\n\ntop_bottom_readsch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_read)\n\ntop_bottom_sciesch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_scie)\n\n\ntop_bottom_mathsch &lt;- top_bottom_mathsch %&gt;%\n  mutate(CNTSCHID = factor(CNTSCHID, levels = top_bottom_math))\n\n\nLastly, I will use the code below to create Ridgeline plots for the top and bottom 5 schools.\n\n\nShow the code\nmath_5 &lt;- ggplot(top_bottom_mathsch, aes(x = PV1MATH, y = factor(CNTSCHID), fill = after_stat(x))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_gradientn(\n    name = \"Math Scores\",\n    colors = c(\"red\", \"green\"),\n    limits = c(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE)),\n    breaks = seq(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE), length.out = 5)\n  ) +\n  coord_cartesian(xlim=c(0,1000)) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 15)  \n  )  +\n  labs(y = \"School ID\", x = NULL, title = \"Math Scores for the Top and Bottom 5 Schools\") +\n  theme(\n    axis.text.y = element_text(angle = 0, hjust = 0.5)  \n  )\n\n\nmath_5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised plot\n\n\n\nBy narrowing our focus to compare only the top and bottom 5 schools, we are able to clearly infer the actual difference between the “best” and “worst” schools.\nThe new revised plot is now more visually compact, and allows readers to quickly examine the difference in performance between the top and bottom 5 schools.\n\n\nBesides this, we can also examine the differences in performance between Male and Female students in these top and bottom 5 schools, using the code below.\n\n\nShow the code\n# Convert ST004D01T to a factor with more meaningful level names\ntop_bottom_mathsch$ST004D01T &lt;- factor(top_bottom_mathsch$ST004D01T, levels = c(1, 2), labels = c(\"Girls\", \"Boys\"))\n\n# Plot\nmath_gender_ridges &lt;- ggplot(top_bottom_mathsch, \n                             aes(x = PV1MATH, \n                                 y = interaction(CNTSCHID, ST004D01T),  # Create interaction between school ID and gender\n                                 fill = after_stat(x))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE\n  ) +\n  scale_fill_gradientn(\n    name = \"Math Scores\",\n    colors = c(\"red\", \"green\"),\n    limits = c(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE)),\n    breaks = seq(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE), length.out = 5)\n  ) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  theme_ridges() +\n  labs(y = \"School ID and Gender\", x = \"Math Scores\", title = \"Math Scores for the Top and Bottom 5 Schools by Gender\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 15),\n    axis.text.y = element_text(angle = 0, hjust = 0.5)  \n  )\n\nmath_gender_ridges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSchool IDs 70200001, 70200139 and 70200110 does not have any data on Female students and are likely a single-gender school."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#loading-r-packages",
    "href": "R-ex/R-Ex1/R_Ex1.html#loading-r-packages",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.1 Loading R packages",
    "text": "1.1 Loading R packages\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#importing-pisa-data",
    "href": "R-ex/R-Ex1/R_Ex1.html#importing-pisa-data",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.2 Importing PISA data",
    "text": "1.2 Importing PISA data\nThe code chunk below uses ‘read_sas()’ of haven to import PISA data into the R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\") \n\nUpon first import, as the student questionaire data file contains data from other countries, we will use filter() to filter the data file to only Singapore data.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%   \n  filter(CNT == \"SGP\")\n\nWe use write_rds() to save the filtered datafile to a seperate file called stu_qqq_SG\n\nwrite_rds(stu_qqq_SG,           \n          \"data/stu_qqq_SG.rds\")\n\nFor our analysis we shall read in data from stu_qqq_SG.rds using read_rds()\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#installing-and-loading-r-instvy",
    "href": "R-ex/R-Ex1/R_Ex1.html#installing-and-loading-r-instvy",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3 Installing and Loading R instvy",
    "text": "1.3 Installing and Loading R instvy\nThe R package intsvy allows R users to analyse PISA data among other international large-scale assessments. The use of PISA data via R requires data preparation, and intsvy offers a data transfer function to import data available in other formats directly into R. Intsvy also provides a merge function to merge the student, school, parent, teacher and cognitive databases.\nTo understand more about the packages available and the methodology to analyse the PISA data files, please refer to this link.\nThe analytical commands within intsvy enables users to derive mean statistics, standard deviations, frequency tables, correlation coefficients and regression estimates.\nAdditionally, intsvy deals with the calculation of point estimates and standard errors that take into account the complex PISA sample design with replicate weights, as well as the rotated test forms with plausible values.\nTo understand more about the instvy package, please refer to this link.\n\ninstall.packages(\"intsvy\",repos = \"http://cran.us.r-project.org\")\n\nWe will load the package using library()\n\nlibrary(\"intsvy\")"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#note",
    "href": "R-ex/R-Ex1/R_Ex1.html#note",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Note",
    "text": "Note\nFrom our initial checks, we noted that there are several variables with missing values.\nRather than checking and deleting all missing values, we will continue to maintain the original data file.\nThis will enable us to use several functions in the instvy package to compute and derive statistics from variables like Plausible Values. Subsequently we will prepare subsets from the data file for each EDA visualization.\n\n\n\n\n\n\nImportant\n\n\n\nThe columns in the PISA data set are named in a specific format. For more information on what each Variable means and how it is derived or calculated, please refer to the questionnaire or code book at this link.\nVariable values are both continuous and discrete."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#working-with-plausible-values",
    "href": "R-ex/R-Ex1/R_Ex1.html#working-with-plausible-values",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.2.1 Working with Plausible Values",
    "text": "1.2.1 Working with Plausible Values\nPISA reports student performance through plausible values (PVs), obtained from Item Response Theory models (for details, see Chapter 5 of the PISA Data Analysis Manual: SAS or SPSS, Second Edition or the associated guide “Scaling of Cognitive Data and Use of Students Performance Estimates”).\nAn accurate and efficient way of measuring proficiency estimates in PISA requires five steps:\n\nCompute estimates for each Plausible Values (PV)\nCompute final estimate by averaging all estimates obtained from (1)\nCompute sampling variance (unbiased estimate are providing by using only one PV)\nCompute imputation variance (measurement error variance, estimated for each PV and then average over the set of PVs)\nCompute final standard error by combining (3) and (4)\n\nFor more information, please refer to this link.\nFor example, in order to obtain single mean scores in Math, Reading and Science for the student cohort, we can use the pisa.mean.pv() function from 'instvy' package like below.\n\nMath_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", data=stu_qqq_SG)  \nRead_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", data=stu_qqq_SG)  \nSCIE_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", data=stu_qqq_SG)  \n\nBelow is an example of a single mean score for Math for the student cohort.\n\nprint(Math_mean_SG) \n\n  CNT Freq   Mean s.e.    SD  s.e\n1 SGP 6606 574.66 1.23 102.8 0.91\n\nprint(Read_mean_SG) \n\n  CNT Freq   Mean s.e.     SD  s.e\n1 SGP 6606 542.55 1.87 105.89 1.15\n\nprint(SCIE_mean_SG)\n\n  CNT Freq   Mean s.e.    SD s.e\n1 SGP 6606 561.43 1.33 99.09 1.1\n\n\nThese mean scores values can be corroborated at the following link (pages 310-315).\nFor a visual of the past performance of the mean scores for Singaporean students, please refer to this link.\nWe will use these calculated mean values from the 10 plausible values as a statistic (Mean) for some of our visualizations."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#distribution-of-students-performance-in-math-reading-and-science",
    "href": "R-ex/R-Ex1/R_Ex1.html#distribution-of-students-performance-in-math-reading-and-science",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.1 Distribution of student’s performance in Math, Reading and Science",
    "text": "1.3.1 Distribution of student’s performance in Math, Reading and Science\nThe objective of this visualization is to examine if subject scores are normally distributed in general within the student population sampled in the PISA test.\nSince there are 10 Plausible Values for the 3 subjects, we shall use the first plausible value, PV1 to visualize the distribution of scores for the subjects.\nFor an example of precedence of using only one Plausible Value, please refer to the article on “How to deal with Plausible Values from International Large-scale assessments.”\nWe will use the below code to plot our histograms to show the distribution of scores across subjects.\n\n\nShow the code\n# Create the histogram plot with an annotated mean line using Math_mean_SG \nplt1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +   \n  geom_histogram(binwidth = 20, color = \"white\", fill='lightblue') +   \n  labs(x = \"PV1 Math Score\",        \n       y = \"Frequency\") +   \n  geom_vline(xintercept = Math_mean_SG$Mean,              \n             col = 'black',              \n             size = 0.5,              \n             linetype = \"dashed\") +   \n  geom_text(aes(x = Math_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2))),             \n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()  \n\n\n# Create the histogram plot with an annotated mean line using Read_mean_SG \nplt2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +   \n  geom_histogram(binwidth = 20, color = \"white\", fill='lightgreen') +   \n  labs(x = \"PV1 Reading Score\",        \n       y = \"Frequency\") +   \n  geom_vline(xintercept = Read_mean_SG$Mean,              \n             col = 'black',              \n             size = 0.5,              \n             linetype = \"dashed\") +   \n  geom_text(aes(x = Read_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2))),             \n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position   \n  theme_minimal()   \n\n# Create the histogram plot with an annotated mean line using Science_mean_SG \n\nplt3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +   \n  geom_histogram(binwidth = 20, color = \"white\", fill='lightpink') +   \n  labs(x = \"PV1 Science Score\",        \n       y = \"Frequency\") +   \n  geom_vline(xintercept = SCIE_mean_SG$Mean,              \n             col = 'black',              \n             size = 0.5,              \n             linetype = \"dashed\") +   \n  geom_text(aes(x = SCIE_mean_SG$Mean, y = 100, \n                label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2))),             color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position   \n  theme_minimal()  \n\n\n# Create a single plot with density plots for Math, Reading, and Science scores \n\nplt4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +   \n  geom_density(alpha = 0.5) +   \n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +   \n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +   labs(x = \"Scores\",        \n            y = \"Density\") +   \n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +   \n  guides(fill = FALSE) +  # Remove the legend   \n  theme_minimal() \n\n\n\nCombined Visual of the distribution of scores in general\nWe will use patchwork to create a composite plot.\n\n\nShow the code\nlibrary(patchwork)  \n\npatch1 &lt;- (plt1+plt2) / (plt3+plt4)  +                \n  plot_annotation(                 \n    title = \"Distribution of student performance in Math, Reading and Science\")  \n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-1",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-1",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 1",
    "text": "Observation 1\nThe distribution of scores seem to resemble a normal distribution across all 3 subjects. Singaporean students seem to have a higher mean score In Mathematics relative to Reading and Science.\nFurther statistical tests like the Anderson-Darling or Shapiro-Wilk tests will need to be conducted to confirm the normality in distribution."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-schools",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-schools",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.2 Relationship between Scores and Schools",
    "text": "1.3.2 Relationship between Scores and Schools\nThe objective of this visualization is to examine the relationship between subject scores and the schools sampled in the PISA test.\nWe will use unique() and length() to obtain the number of unique schools in the data set.\n\nunique_values &lt;- unique(stu_qqq_SG$CNTSCHID)  \n\nlength(unique_values)\n\n[1] 164\n\n\nThere are 164 unique schools in this data set.\nNext, we use the code below to plot our scatter plots.\n\n\nShow the code\np1 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1MATH)) +   \n  geom_point(color = \"lightblue\", alpha = 0.5) +   \n  geom_hline(yintercept = Math_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +     \n  annotate(\"text\", x = Inf, y = Math_mean_SG$Mean, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2)),             \n           hjust = 1, vjust = -1) +     \n  labs(x = \"School ID\",        \n       y = \"PV1 Math Score\") +   \n  theme_minimal()  \n\np2 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1READ)) +   \n  geom_point(color = \"lightgreen\", alpha = 0.5) +   \n  geom_hline(yintercept = Read_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +     \n  annotate(\"text\", x = Inf, y = Read_mean_SG$Mean, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2)),             \n           hjust = 1, vjust = -1) +     \n  labs(x = \"School ID\",        \n       y = \"PV1 Reading Score\") +   \n  theme_minimal()  \n\np3 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1SCIE)) +   \n  geom_point(color = \"lightpink\", alpha = 0.5) +   \n  geom_hline(yintercept = SCIE_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +     annotate(\"text\", x = Inf, y = SCIE_mean_SG$Mean, label = paste(\"Mean =\",round(SCIE_mean_SG$Mean, 2)),             \n                                                                                                  hjust = 1, vjust = -1) +     \n  labs(x = \"School ID\",        \n       y = \"PV1 Science Score\") +   \n  theme_minimal() \n\n\n\nCombined Visual of the distribution of scores across Schools\nWe will use patchwork to create a composite plot.\n\n\nShow the code\npatch2 &lt;- p1/p2/p3 +                \n  plot_annotation(                 \n    title = \"Students seem to be performing equally across Schools\")  \n\npatch2 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-2",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-2",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 2",
    "text": "Observation 2\nAcross the 164 schools, students seem to be performing equally across the 3 subjects. There are no significant clusters that are different from each other, for example, a large number of schools which only have good scores or only poor scores.\nAdditional analysis could be done to examine if the highest and lowest performing students (in terms of scores) belong to the same type of schools.\nSince this data set only contains students on students, there is no additional information on either the type of school or its resources.. Further analysis could incorporate other data sets to build a more complete analysis."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-gender",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-gender",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.3 Relationship between Scores and Gender",
    "text": "1.3.3 Relationship between Scores and Gender\nThe objective of this visualization is to examine the relationship between subject scores and gender within the students sampled in the PISA test.\nThe gender column of the data set is named as “ST004D01T” with values of 1=Female and 2=Male.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by gender.\nIn the code below, we will create separate tables for the mean scores for each subject by different genders.\n\n#| code-fold: true \n#| code-summary: \"Show the code\"  \n\nMath_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"ST004D01T\", data = stu_qqq_SG)  \n\nMath_gender$ST004D01T &lt;- factor(Math_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))  \n\nRead_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"ST004D01T\", data = stu_qqq_SG)  \n\nRead_gender$ST004D01T &lt;- factor(Read_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))  \n\nSCIE_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"ST004D01T\", data = stu_qqq_SG)  \n\nSCIE_gender$ST004D01T &lt;- factor(SCIE_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))   \n\nBelow is how the new tables look like for mean Math score grouped by gender. We can use the mean statistic here as an additional statistic for our box plots.\n\nprint(Math_gender) \n\n  ST004D01T Freq   Mean s.e.     SD  s.e\n1    Female 3248 568.49 1.65  97.62 1.14\n2      Male 3358 580.59 1.75 107.20 1.33\n\n\nNext, we plot the PV1 scores by different genders to examine the performance of different genders across subjects.\n\n\nShow the code\n# Create a subset of the data with gender and PV1 score columns \nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%   \n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)  \n\n# Convert the \"ST004D01T\" column to a factor  \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))  \n\n# Create the plot using the subset_data \n\nbxp1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +   \n  geom_boxplot() +   \n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +   \n  geom_text(data = Math_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)),              \n            color = \"black\", vjust = -0.5, size = 3.5) +   \n  labs(x = \"Gender\",        \n       y = \"PV1 Math Score\") +   \n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +     theme_minimal() +   \n  theme(legend.position = \"none\")  \n\n# Remove the legend   \n\nbxp2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +   \n  geom_boxplot() +   \n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +   \n  geom_text(data = Read_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)),              \n            color = \"black\", vjust = -0.5, size = 3.5) +   \n  labs(x = \"Gender\",        \n       y = \"PV1 Reading Score\") +   \n  scale_fill_manual(values = c(\"lightgreen\", \"lightgreen\")) +  # Associate colors with factor levels   \n  theme_minimal() +   \n  theme(legend.position = \"none\")  \n\n# Remove the legend  \n\nbxp3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +   \n  geom_boxplot() +   \n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +   \n  geom_text(data = SCIE_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)),              \n            color = \"black\", vjust = -0.5, size = 3.5) +   \n  labs(x = \"Gender\",        \n       y = \"PV1 Science Score\") +   \n  scale_fill_manual(values = c(\"lightpink\", \"lightpink\")) +  # Associate colors with factor levels   \n  theme_minimal() +   \n  theme(legend.position = \"none\")  # Remove the legend \n\n\n\nCombined Visual of Performance across Genders\nWe will use the code below to create a composite plot for our box plots.\n\n\nShow the code\npatch3 &lt;- bxp1 + bxp2 + bxp3 +                \n  plot_annotation(                 \n    title = \"Male students outperform in Maths and Science\")  \n\npatch3 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-3",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-3",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 3",
    "text": "Observation 3\nMale students seem to outperform Female students in both Maths and Science with mean scores of 580.59 and 564.81 respectively. Female students seem to outperform Male students in Reading with a mean score of 552.55."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-socioeconomic-status-of-students",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-socioeconomic-status-of-students",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.4 Relationship between Scores and Socioeconomic status of students",
    "text": "1.3.4 Relationship between Scores and Socioeconomic status of students\nThe socioeconomic status of students is represented by the “ESCS” score in the PISA data set. The ESCS score is a continuous variable and is calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS). A higher ESCS score translates to a “better-off” student.\nFurther break down on the 3 main components of the ESCS score is shown in the diagram below. For further information on the computation methodology, please refer to the PISA 2022 Technical report: Chapter 19.\n\n\n\nComputation of ESCS in PISA 2022\n\n\nThe objective of this visualization is to examine the relationship between subject scores and a student’s socioeconomic status.\nFirst we check for any missing values in the ESCS column using the code below.\n\n# Check for NAs in the 'ESCS' column \nhas_nas &lt;- any(is.na(stu_qqq_SG$ESCS))  \nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the ESCS column, we shall delete the rows with missing ESCS values. We will create a new subset with ESCS and the PV1 scores for this visualization.\n\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%   \n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)  \n\n#omiting NA values \nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\nUsing our new table subset_ESCS_PV1, we will create scatter plots for ESCS versus each PV1 score for each subject using the code below.\n\n\nShow the code\nc_coeff_ESCS_Math &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1MATH)  \n\nC_plt1 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1MATH)) +   \n  geom_point(color = \"lightblue\") +   \n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +   \n  geom_text(     \n    x = max(subset_ESCS_PV1$ESCS),       \n    y = max(subset_ESCS_PV1$PV1MATH),       \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Math, 2)),     \n    hjust = 1,  # Adjust horizontal justification     \n    vjust = 1   # Adjust vertical justification   \n    ) +   \n  labs(x = \"Socio-Economic Status (ESCS)\",        \n       y = \"PV1 Math Score\") +   \n  theme_minimal()  \n\nc_coeff_ESCS_Read &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1READ)  \n\nC_plt2 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1READ)) +  \n  geom_point(color = \"lightgreen\") +   \n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +   \n  geom_text(     \n    x = max(subset_ESCS_PV1$ESCS),       \n    y = max(subset_ESCS_PV1$PV1READ),       \n    label = paste(\"Corr Coeff:\", \n                  round(c_coeff_ESCS_Read, 2)),     \n    hjust = 1,  # Adjust horizontal justification     \n    vjust = 1   # Adjust vertical justification   \n    ) +   labs(x = \"Socio-Economic Status (ESCS)\",        \n               y = \"PV1 Read Score\") +   \n  theme_minimal()  \n\nc_coeff_ESCS_Scie &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1SCIE)  \n\nC_plt3 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1SCIE)) +   \n  geom_point(color = \"lightpink\") +   \n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +   \n  geom_text(     \n    x = max(subset_ESCS_PV1$ESCS),       \n    y = max(subset_ESCS_PV1$PV1SCIE),       \n    label = paste(\"Corr Coeff:\", \n                  round(c_coeff_ESCS_Scie, 2)),     \n    hjust = 1,  # Adjust horizontal justification     \n    vjust = 1   # Adjust vertical justification   \n    ) +   labs(x = \"Socio-Economic Status (ESCS)\",        \n               y = \"PV1 Science Score\") +   \n  \n  theme_minimal() \n\n\n\nCombined Scatter plots of PV1 Scores Vs ESCS scores\nWe will use patchwork to create a composite plot for our scatter plots.\n\n\nShow the code\npatch4 &lt;- C_plt1 / C_plt2 / C_plt3 +                \n  plot_annotation(                 \n    title = \"Weak positive relationship between Scores and ESCS\")  \n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-4",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-4",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 4",
    "text": "Observation 4\nThere is a weak positive relationship between subject scores and Socioeconomic statuses. The ESCS score is a composite score calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS). It could be likely that the larger number of constituents has ‘diluted’ the score, where the effect is more prominent for developed countries like Singapore.\n\nFurther analysis could be conducted on the individual components of the ESCS score to check for their individual influence on student performance."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-the-years-of-education-for-parents",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-the-years-of-education-for-parents",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.5 Relationship between Scores and the years of Education for Parents",
    "text": "1.3.5 Relationship between Scores and the years of Education for Parents\nAs highlighted in the previous analysis, there is a weak positive relationship between student scores and ESCS scores. The objective of this visualization is to examine the relationship between one of the constituents, PAREDINT and student scores.\nPAREDINT is the index of the highest education of parents in years, based on the median cumulative years of education completed. The variable values are discrete and ranges from a scale of 3 to 16 years. For more information on this variable, please refer to the PISA 2022 Technical report: Chapter 19.\nFirst we check for any missing values in the ESCS column using the code below.\n\n# Check for NAs in the 'PAREDINT' column \nhas_nas &lt;- any(is.na(stu_qqq_SG$PAREDINT))  \nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the PAREDINT column, we shall delete the rows with missing PAREDINT values. We will then create a new table with PAREDINT and the Mean scores of the 10 Plausible Values for this visualization.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the years of Parents education (PARENDINT).\nIn the code below, we will create separate tables for the mean scores for each subject by different years of Education.\n\n\nShow the code\nParents_edu_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"PAREDINT\", data = stu_qqq_SG)  \n\nParents_edu_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"PAREDINT\", data = stu_qqq_SG)  \n\nParents_edu_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"PAREDINT\", data = stu_qqq_SG) \n\n\nWe examine the one of the new tables, Parents_edu_math.\n\nprint(Parents_edu_math)\n\n  PAREDINT Freq   Mean  s.e.     SD   s.e\n1        3    8 482.25 53.59 131.99 25.91\n2        6   62 500.76 12.97 102.52  9.03\n3        9  127 540.05  8.87  98.63  6.43\n4       12 1470 530.62  3.14 100.32  1.75\n5     14.5 1213 559.85  3.00  97.10  1.95\n6       16 3669 600.47  1.68  97.13  1.37\n7     &lt;NA&gt;   57 485.92 11.61  85.05  9.59\n\n\nSince there 57 rows with missing values, we will delete the rows with missing values.\n\nParents_edu_math &lt;- na.omit(Parents_edu_math) \nParents_edu_read &lt;- na.omit(Parents_edu_read) \nParents_edu_scie &lt;- na.omit(Parents_edu_scie)\n\nNext, we use the below code to plot dot plots for each subject.\n\n\nShow the code\n# Create a dot plot with annotations \n\nDp1 &lt;- ggplot(Parents_edu_math, aes(x = as.factor(PAREDINT), y = Mean)) +   geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +   geom_line(aes(group = 1), color = \"lightblue\", size = 1, alpha = 0.5) +   \n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels   \n  labs(title = \"Math\",        \n       x = \"Education (Yrs)\")+    \n  theme_minimal()   \n\nDp2 &lt;- ggplot(Parents_edu_read, aes(x = as.factor(PAREDINT), y = Mean)) +   geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +   geom_line(aes(group = 1), color = \"lightgreen\", size = 1, alpha = 0.5) +   geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels   \n  labs(title = \"Reading\",        \n       x = \"Education (Yrs)\")+    \n  theme_minimal()  \n\nDp3 &lt;- ggplot(Parents_edu_scie, aes(x = as.factor(PAREDINT), y = Mean)) +   geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +   geom_line(aes(group = 1), color = \"lightpink\", size = 1, alpha = 0.5) +   \n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels   \n  labs(title = \"Science\",        \n       x = \"Education (Yrs)\")+    \n  theme_minimal() \n\n\n\nCombined dot plots of Subject Mean Scores Vs Parents Education years\nWe will use the code below to create a composite plot.\n\n\nShow the code\npatch4 &lt;- (Dp1 + Dp2 + Dp3                \n           ) +                \n  plot_annotation(                 \n    title = \"Parents with more education years seem to have children with higher scores\")  \n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-5",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-5",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 5",
    "text": "Observation 5\nIn general, students seem to have performed better across all subjects the more their parents have been educated. However this factor alone is likely insufficient to cause a better performance.\nAlso, there is a slight drop off in mean scores from 9-12 years of Parents Education\nAdditional analyses taking into account the state of the study environment, both at home and in school, as well as the emotional aspects and motivation of students could be further analysed to derive more complete insights on the factors that could influence performance."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#additional-eda",
    "href": "R-ex/R-Ex1/R_Ex1.html#additional-eda",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.6 Additional EDA",
    "text": "1.3.6 Additional EDA\n\n1) Examining closer into Mean scores per School\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the School ID (CNTSCHID).\nIn the code below, we will create separate tables for the mean scores for each subject by different School Ids.\n\n\nShow the code\nSchoolid_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), \n                  by = \"CNTSCHID\", data = stu_qqq_SG)  \n\nSchoolid_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), \n                  by = \"CNTSCHID\", data = stu_qqq_SG)  \n\nSchoolid_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), \n                  by = \"CNTSCHID\", data = stu_qqq_SG)\n\n\nWe examine the new tables created. In these new tables we are able to extract the number of students and the mean scores per School.\n\nhead(Schoolid_math) \n\n  CNTSCHID Freq   Mean  s.e.    SD   s.e\n1 70200001   55 725.21  9.34 59.23  6.38\n2 70200002   38 534.22 16.75 89.96 13.84\n3 70200003   36 739.92 12.30 59.23  7.70\n4 70200004   56 509.61 12.84 86.63  7.71\n5 70200005   38 546.52 12.95 86.04  8.86\n6 70200006   36 485.30 13.90 76.47  8.86\n\n\nNext, we use the below code to plot bubble plots to examine the number of students and their mean scores for each school. We will also use the plotly package for added interactivity.\n\n\nShow the code\nlibrary(ggplot2) \nlibrary(plotly)  \n\np_1 &lt;- ggplot(Schoolid_math, aes(x = CNTSCHID, y = Mean)) +   \n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +   \n  scale_size_area(max_size = 10) +   \n  scale_color_gradient(low = \"skyblue\", high = \"darkblue\") +   \n  labs(title = \"Mean Math Scores by School ID\",     \n       y = \"Mean Math Scores\",      \n       size = \"Number of Students\",      \n       color = \"Number of Students\") +   \n  theme_minimal() +   \n  theme(axis.text.x = element_blank(),         \n        axis.ticks.x = element_blank(),         \n        axis.title.x = element_blank(),         \n        panel.grid.major = element_blank(),  # Remove major grid lines         \n        panel.grid.minor = element_blank())  # Remove minor grid lines  \n\n# Convert to an interactive plot \n\nggplotly(p_1, tooltip = c(\"x\", \"y\", \"size\", \"color\"))  \n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_2 &lt;- ggplot(Schoolid_read, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"yellow\", high = \"darkorchid\") +\n  labs(title = \"Mean Reading Scores by School ID\",\n    y = \"Mean Reading Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_2, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_3 &lt;- ggplot(Schoolid_scie, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"lightpink\", high = \"darkred\") +\n  labs(title = \"Mean Science Scores by School ID\",\n    y = \"Mean Science Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_3, tooltip = c(\"x\", \"y\", \"size\", \"color\"))"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-6",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-6",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 6",
    "text": "Observation 6\nThe ability to extract and assign Mean scores to individual schools enables us to further explore and examine the disparity in performance between schools. For example, looking at the two extremes of score results, we note that Schools (70200001 & 70200003) out perform other schools in all subjects. On the other hand, Schools (7020115 & 70200149) under perform other schools in all subjects.\n\nThis shows that there are still marked differences between the ‘’best” schools and the’‘worst’’ schools. Additional analysis should be done to identify the differences between these two sets of schools in terms of resources, teaching quality, and students attitudes or motivation to fully understand the differences between the scores.\n\n2) Examining the Breakdown of scores per Subject\nPreviously we had only examined the distribution of marks for the student population across the 3 subjects (whether resembles normal distribution).\nWe can further examine the percentage of students per score range for each subject. This might help us examine whether there are specific strengths or weaknesses in the student cohort.\nFirst, we use the pisa.ben.pv() function from the instvy package which calculates student scores from the 10 plausible values and calculates the percentage of students at each proficiency level (Score range) as defined by PISA.\nIn the code below, we will create separate tables for the percentage breakdown of scores for each subject.\n\n\nShow the code\nMath_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nRead_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nScie_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\n\nWe examine the new tables created. In these new tables we are able to obtain the percentage breakdown of students per score range for our next visualization.\n\nprint(Math_Breakdown)\n\n  CNT       Benchmarks Percentage Std. err.\n1 SGP        &lt;= 357.77       2.17      0.22\n2 SGP (357.77, 420.07]       5.85      0.38\n3 SGP (420.07, 482.38]      11.25      0.59\n4 SGP (482.38, 544.68]      17.59      0.61\n5 SGP (544.68, 606.99]      22.62      0.69\n6 SGP  (606.99, 669.3]      21.96      0.69\n7 SGP          &gt; 669.3      18.56      0.52\n\n\nWe will plot bar charts for the percentage of students per Score range for each subject.\n\nMath Scores breakdownReading Scores breakdownScience Scores breakdown\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Math_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Math\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Read_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Reading\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Scie_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightpink\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Science\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we can combine these tables and plots into one plot to show the percentage of students per Score Range for all subjects.\n\n\nShow the code\n# Creating a new combined table\n\nMath_Breakdown$Subject &lt;- 'Math'\nRead_Breakdown$Subject &lt;- 'Reading'\nScie_Breakdown$Subject &lt;- 'Science'\n\nCombined_Breakdown &lt;- bind_rows(Math_Breakdown, Read_Breakdown, Scie_Breakdown)\n\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_Breakdown &lt;- Combined_Breakdown %&gt;%\n  mutate(Benchmarks = fct_inorder(Benchmarks))\n\n# Now plot using ggplot\np &lt;- ggplot(Combined_Breakdown, aes(x = Benchmarks, y = Percentage, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Dodge position for the bars\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percentage)),  # This will format the label to have 1 decimal place and a percentage sign\n    position = position_dodge(width = 0.9),  # Match the position of the text with the dodged bars\n    vjust = -0.25,   \n    size = 2  \n  ) +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"For lower Score ranges, students seem to do better in Reading\",\n       x = \"Score Range\",\n       y = \"Percentage of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to interactive plotly object\np_interactive &lt;- ggplotly(p, tooltip = c(\"x\", \"y\", \"fill\", \"text\"))\n\n# Print the interactive plot\np_interactive"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-7",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-7",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 7",
    "text": "Observation 7\n1) 63.14 % of students scored 544.68 and above for Math\n2) 52.45% of students scored 544.68 and above for Reading\n3) 59.65 % of students scored 544.68 and above for Science\n\nIf we were to select a baseline score as 544.68, we can see that students generally do better in Math relative to Science and Reading.\nWhile individual plots are useful to visualize the break down of students across scores, we can also combine them into a single bar plot to obtain insights on relative performance. From the combined plot, we can see that at lower score ranges, students seem to do better in Reading relative to Math and Science. However, at higher score ranges, students do worse in Reading relative to Math and Science.\n\n3) Violin plots for Gender performance across subjects\nPreviously we had used box plots to visualize the difference in performance between the genders across subjects. The code below ‘switches’ to violin plots instead.\n\n\nShow the code\n# Create the plot using the subset_data\nvx_plot1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = Math_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Math Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\nvx_plot2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = Read_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Reading Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightgreen\", \"Male\" = \"lightgreen\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\nvx_plot3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = SCIE_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Science Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightpink\", \"Male\" = \"lightpink\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\n\nWe will use the code below to do a composite plot for all 3 subjects\n\n\nShow the code\npatch5 &lt;- vx_plot1 + vx_plot2 + vx_plot3 + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\")\n\npatch5 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#note-1",
    "href": "R-ex/R-Ex1/R_Ex1.html#note-1",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Note",
    "text": "Note\nViolin plots may provide a fuller picture of the data distribution, revealing density and multimodality that box plots may obscure. This could be useful for detecting data patterns and clusters.\nViolin plots could also emphasize the concentration of data points and potential outliers, showcasing the entire range of the data including peaks, valleys, and tails that may not be evident from box plots.\n\n4) Relationship between Scores and the times spent on studying or homework before or after school\nThe objective of this visualization is to examine the relationship between student scores and the time spent on studying or on homework (STUDYHMW).\nThe variable “STUDYHMW” measures how many times during a typical school week students studied for school or homework before going to school and/or after leaving school. Values on this index range from 0 (no studying) to 10 (10 or more times of studying per week).\nFirst we check for any missing values in the STUDYHMW column using the code below.\n\n# Check for NAs in the 'STUDYHMW' column\nhas_nas &lt;- any(is.na(stu_qqq_SG$STUDYHMW))\n\nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the STUDYHMW column, we shall delete the rows with missing values. We will then create a new table with STUDYHMW and the Mean scores of the 10 Plausible Values for this visualization.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the time spent on studying (STUDYHMW).\nIn the code below, we will create separate tables for the mean scores for each subject.\n\n\nShow the code\nhomework_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\nhomework_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\nhomework_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\n\nWe examine the new tables created.\n\nprint(homework_math)\n\n   STUDYHMW Freq   Mean  s.e.     SD  s.e\n1         0  274 514.96  7.21 111.29 4.17\n2         1  154 537.23  9.42 112.48 6.16\n3         2  422 540.72  5.89 110.16 3.82\n4         3  500 566.21  4.89 105.07 3.27\n5         4  602 569.18  4.54 104.79 3.12\n6         5 1750 616.13  2.13  90.56 1.64\n7         6  568 551.80  4.29 103.99 3.02\n8         7  415 577.30  4.96  97.93 3.69\n9         8  416 561.92  4.83  94.31 3.86\n10        9  163 554.53  8.07  91.50 5.98\n11       10 1296 570.87  2.45  95.76 2.11\n12     &lt;NA&gt;   46 486.26 11.65  77.81 8.28\n\n\nSince there 46 rows with missing values, we will delete the rows with missing values.\n\nhomework_math &lt;- na.omit(homework_math) \nhomework_read &lt;- na.omit(homework_read) \nhomework_scie &lt;- na.omit(homework_scie)\n\nNext, we can combine these tables to be able to plot one graph to show the mean scores across the time spent on homework and studying, for all subjects.\n\nhomework_math$Subject &lt;- 'Math'\nhomework_read$Subject &lt;- 'Reading'\nhomework_scie$Subject &lt;- 'Science'\n\nCombined_homework &lt;- bind_rows(homework_math, homework_read, homework_scie)\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_homework &lt;- Combined_homework %&gt;%\n  mutate(STUDYHMW = fct_inorder(STUDYHMW))\n\n# Now plot using ggplot\np2 &lt;- ggplot(Combined_homework, aes(x = STUDYHMW, y = Mean, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"Students who study more may not necessarily score higher\",\n       x = \"Number of times in a week\",\n       y = \"Mean Scores\") +\n  theme_minimal() \n\n# Convert to interactive plotly object\np_interactive2 &lt;- ggplotly(p2, tooltip = c(\"x\", \"y\", \"fill\", \"text\"))\n\n# Print the interactive plot\np_interactive2"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-8",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-8",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 8",
    "text": "Observation 8\nFrom the combined plot, it appears that the optimal number of times to study in a week is 5 times for all subjects. More times spent on homework and/or studying does not seem to translate to higher scores for this PISA test. However this factor alone is insufficient to conclude causality, and we should do additional analysis on other factors that may impact a student’s ability to prepare for this test."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Imran",
    "section": "",
    "text": "Having spent many years lost in a traditional finance career, I’ve finally found the courage to start learning about Data Science.\nNow, I’m on my second wind, and hope to become someone more than just to “increase shareholder value”.\n\nNever settle, and always strive to be a better version of yourself every day. It’s never to late to start something or learn something new. Your ability is not determined by your age.\n\n“Never say never, because limits, like fears, are often just an illusion” - Michael Jordan.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Imran’s Data Science website",
    "section": "",
    "text": "Welcome to my website.\nHere, you will find some of my post-graduate research, assignments and projects.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "R-ex/R-Ex2/R_Ex2.html",
    "href": "R-ex/R-Ex2/R_Ex2.html",
    "title": "Take-home Exercise 1a- 5 Exploratory Data Analyses on Pisa data",
    "section": "",
    "text": "Setting the Scene\nOECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several of Singapore’s ministers for Education also started an “every school a good school” slogan. The general public, however, believes that there are still disparities that exist, especially between “elite” and neighborhood schools, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\n\n\nThe Task\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA’s global education survey runs every three years to assess education systems worldwide through the testing 15 year old students in the subjects of mathematics, reading, and science.\nIn this take-home exercise, we will use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\n\n\n\nGetting Started\nLoading R packages\n\npacman::p_load(tidyverse, haven, ggdist, ggridges, ggthemes,\n               colorspace)\n\nImporting the Data\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\nLoading the instvy package.\n\n# install.packages(\"intsvy\",repos = \"http://cran.us.r-project.org\")\n\nlibrary(\"intsvy\")\n\nExtracting the overall student mean score values for each subject\n\nMath_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", data=stu_qqq_SG)\n\nRead_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", data=stu_qqq_SG)\n\nSCIE_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", data=stu_qqq_SG)\n\n\n\n1) Distribution of scores in the student cohort\nWe will use the code below to plot histograms to show the distribution of scores across the 3 subjects.\n\n\nShow the code\n# Create the histogram plot with an annotated mean line using Math_mean_SG\nplt1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightblue') +\n  labs(x = \"Math Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = Math_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Math_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create the histogram plot with an annotated mean line using Read_mean_SG\nplt2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightgreen') +\n  labs(x = \"Reading Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = Read_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Read_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n\n# Create the histogram plot with an annotated mean line using Science_mean_SG\nplt3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightpink') +\n  labs(x = \"Science Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = SCIE_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = SCIE_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create a single plot with density plots for Math, Reading, and Science scores\nplt4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Subject Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\nWe use the code below to create a composite plot.\n\n\nShow the code\nlibrary(patchwork)\n\npatch1 &lt;- (plt1+plt2) / (plt3+plt4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\")\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 1\n\n\n\nThe distribution of scores seem to resemble a normal distribution across all 3 subjects. Singaporean students seem to have a higher mean score In Mathematics relative to Reading and Science.\nFurther statistical tests like the Anderson-Darling or Shapiro-Wilk tests will need to be conducted to confirm the normality in distribution.\n\n\n\n\n2) Relationship between Scores and School ID\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the School ID (CNTSCHID).\nIn the code below, we will create separate tables for the mean scores for each subject by different School Ids.\n\nSchoolid_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nNext, we use the below code to plot bubble plots to examine the number of students and their mean scores for each school. We will also use the plotly package for added interactivity.\n\nMean Math scores across SchoolsMean Reading scores across SchoolsMean Science scores across Schools\n\n\n\n\nShow the code\nlibrary(plotly)\n\nbest_sch_math &lt;- Schoolid_math %&gt;% filter(Mean == max(Mean))\nworst_sch_math &lt;- Schoolid_math %&gt;% filter(Mean == min(Mean))\n\n\np_1 &lt;- ggplot(Schoolid_math, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"skyblue\", high = \"darkblue\") +\n  labs(title = \"Mean Math Scores per School\",\n    y = \"Mean Math Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_1 &lt;- p_1 + \n  geom_text(data = best_sch_math, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_math, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n\n# Convert to an interactive plot\nggplotly(p_1, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_sch_read &lt;- Schoolid_read %&gt;% filter(Mean == max(Mean))\nworst_sch_read &lt;- Schoolid_read %&gt;% filter(Mean == min(Mean))\n\np_2 &lt;- ggplot(Schoolid_read, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"yellow\", high = \"darkorchid\") +\n  labs(title = \"Mean Reading Scores per School\",\n    y = \"Mean Reading Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_2 &lt;- p_2 + \n  geom_text(data = best_sch_read, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_read, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n# Convert to an interactive plot\nggplotly(p_2, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_sch_scie &lt;- Schoolid_scie %&gt;% filter(Mean == max(Mean))\nworst_sch_scie &lt;- Schoolid_scie %&gt;% filter(Mean == min(Mean))\n\n\np_3 &lt;- ggplot(Schoolid_scie, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"lightpink\", high = \"darkred\") +\n  labs(title = \"Mean Science Scores per School\",\n    y = \"Mean Science Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_3 &lt;- p_3 + \n  geom_text(data = best_sch_scie, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_scie, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n# Convert to an interactive plot\nggplotly(p_3, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 2\n\n\n\nThe ability to extract and assign Mean scores to individual schools enables us to further explore and examine the disparity in performance between schools. For example, looking at the two extremes of score results, we note that Schools (70200001 & 70200003) out perform other schools in Math and Science. On the other hand, Schools (7020115 & 70200149) under perform other schools in Math and Science.\nThis seems to indicate that there are still marked differences between the ‘’best” schools and the’‘worst’’ schools. Additional analysis could be done to identify the differences between these two sets of schools in terms of resources, teaching quality, and students attitudes or motivation etc, in order to fully understand the reason behind the difference in the scores.\n\n\n\n\n3) Relationship Between Gender and Scores\nFirst we create a subset of Gender and PV1 scores using the below code. We also convert the levels from 1 and 2, to Female and Male respectively.\n\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nNext we plot the ridgeline plots with quantile lines.\n\n\nShow the code\nrp1 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1MATH, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Math Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\nrp2 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1READ, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Reading Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\nrp3 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1SCIE, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Science Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\n\nWe use the code below to create a composite plot via patchwork\n\n\nShow the code\nlibrary(patchwork)\n\npatch6 &lt;- rp1 / rp2 / rp3 + \n              plot_annotation(\n                title = \"Male students seem to perform better in Math and Science\")\n\npatch6 & theme(panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 3\n\n\n\nMales students seem to outperform Female students in both Maths and Science. Female students seem to outperform Male students in Reading.\n\n\n\n\n4) Relationship between Scores and Socioeconomic status of students\nWe will create a new subset with ESCS and the PV1 scores for this visualization.\n\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\nUsing our new table subset_ESCS_PV1, we will create scatter plots for ESCS versus each PV1 score for each subject using the code below.\n\n\nShow the code\nc_coeff_ESCS_Math &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1MATH)\n\nC_plt1 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1MATH)) +\n  geom_point(color = \"lightblue\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1MATH),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Math, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n    y = \"Math Scores\") +\n  theme_minimal()\n\nc_coeff_ESCS_Read &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1READ)\n\nC_plt2 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1READ)) +\n  geom_point(color = \"lightgreen\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1READ),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Read, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n    y = \"Reading Scores\") +\n  theme_minimal()\n\nc_coeff_ESCS_Scie &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1SCIE)\n\nC_plt3 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1SCIE)) +\n  geom_point(color = \"lightpink\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1SCIE),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Scie, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n       y = \"Science Scores\") +\n  theme_minimal()\n\n\nWe will use patchwork to create a composite plot for our scatter plots.\n\n\nShow the code\npatch4 &lt;- C_plt1 / C_plt2 / C_plt3 + \n              plot_annotation(\n                title = \"Weak positive relationship between Scores and ESCS\")\n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 4\n\n\n\nThere is a weak positive relationship between subject scores and Socioeconomic statuses. The ESCS score is a composite score calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS).\nFurther analysis could be conducted on the individual components of the ESCS score to check for their individual influence on student performance.\n\n\n\n\n5) Examining the Breakdown of scores per Subject\nWe can further examine the percentage of students per score range for each subject. This might help us examine whether there are specific strengths or weaknesses in the student cohort.\nFirst, we use the pisa.ben.pv() function from the instvy package which calculates student scores from the 10 plausible values and calculates the percentage of students at each proficiency level (Score range) as defined by PISA.\nIn the code below, we will create separate tables for the percentage breakdown of scores for each subject.\n\n\nShow the code\nMath_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nRead_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nScie_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\n\nNext, we can combine these tables and plots into one plot to show the percentage of students per Score Range for all subjects.\n\n\nShow the code\n# Creating a new combined table\n\nMath_Breakdown$Subject &lt;- 'Math'\nRead_Breakdown$Subject &lt;- 'Reading'\nScie_Breakdown$Subject &lt;- 'Science'\n\nCombined_Breakdown &lt;- bind_rows(Math_Breakdown, Read_Breakdown, Scie_Breakdown)\n\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_Breakdown &lt;- Combined_Breakdown %&gt;%\n  mutate(Benchmarks = fct_inorder(Benchmarks))\n\n# Now plot using ggplot\nggplot(Combined_Breakdown, aes(x = Benchmarks, y = Percentage, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Dodge position for the bars\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percentage)),  # This will format the label to have 1 decimal place and a percentage sign\n    position = position_dodge(width = 0.9),  # Match the position of the text with the dodged bars\n    vjust = -0.25,   \n    size = 2  \n  ) +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"For lower Score ranges, students seem to do better in Reading\",\n       x = \"Score Range\",\n       y = \"Percentage of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 5\n\n\n\nCombined bar plots can allow us to obtain insights on relative performance. For example, for scores below 544.68, we can see that at lower score ranges, students seem to do better in Reading relative to Math and Science. However, at higher score ranges, students do worse in Reading relative to Math and Science.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html",
    "href": "R-ex/R-Ex4/R_Ex4.html",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to an official report as shown in the infographic below,\n\nThe intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer, and\nThe contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#installing-and-loading-r-packages",
    "href": "R-ex/R-Ex4/R_Ex4.html#installing-and-loading-r-packages",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "2.1 Installing and Loading R packages",
    "text": "2.1 Installing and Loading R packages\n\npacman::p_load(ungeviz, plotly, crosstalk, patchwork,\n               DT, ggdist, ggridges, ggstatsplot,ggthemes,\n               colorspace, gganimate, tidyverse, dplyr, \n               readr)"
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#importing-data-and-data-preparation",
    "href": "R-ex/R-Ex4/R_Ex4.html#importing-data-and-data-preparation",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "2.2 Importing Data and Data Preparation",
    "text": "2.2 Importing Data and Data Preparation\nI will download and import data for July (Dry) and December (Wet) for 1983, 1993, 2003, 2013, 2023.\n\nI have chosen Changi weather station, as it is one of the older weather stations with daily rainfall data recorded since 1981. For more information on the weather stations and the historical data available, please refer to this link.\n\n\nShow the code\n# List of file names\nDecember_files &lt;- c(\"data/DAILYDATA_S24_198312.csv\", \"data/DAILYDATA_S24_199312.csv\", \n                \"data/DAILYDATA_S24_200312.csv\", \"data/DAILYDATA_S24_201312.csv\", \n                \"data/DAILYDATA_S24_202312.csv\")\n\nJuly_files &lt;- c(\"data/DAILYDATA_S24_198307.csv\", \"data/DAILYDATA_S24_199307.csv\", \n                \"data/DAILYDATA_S24_200307.csv\", \"data/DAILYDATA_S24_201307.csv\", \n                \"data/DAILYDATA_S24_202307.csv\")\n\n\nUpon inspecting the csv files for the 5 years, the below columns were removed as they were only recorded since 2014:\n\nHighest  30-min Rainfall (mm)\nHighest  60-min Rainfall (mm)\nHighest 120-min Rainfall (mm)\n\nI will use the code below to import the csv files into our R environment\n\n# Reading and combining the CSV files for December\nDecember_data &lt;- lapply(December_files, read_csv, col_names = FALSE,\n                        col_select = c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13),\n                        skip = 1) %&gt;%\n  bind_rows(.id = \"file\")\n\n\n# Reading and combining the CSV files for July\nJuly_data &lt;- lapply(July_files, read_csv, col_names = FALSE,\n                        col_select = c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13),\n                        skip = 1) %&gt;%\n  bind_rows(.id = \"file\")\n\nI will use the code below to rename the column names for the data sets.\n\n\nShow the code\n# Renaming the columns\ncolnames(December_data) &lt;- c(\"ID\", \"Station\", \"Year\", \"Month\", \"Day\", \n                              \"Daily_Rainfall_Total_mm\", \"Mean_Temperature_C\", \n                              \"Maximum_Temperature_C\", \"Minimum_Temperature_C\", \n                              \"Mean_Wind_Speed_km_h\", \"Max_Wind_Speed_km_h\")\n\nDecember_data$Year &lt;- as.factor(December_data$Year)\n\ncolnames(July_data) &lt;- c(\"ID\", \"Station\", \"Year\", \"Month\", \"Day\", \n                              \"Daily_Rainfall_Total_mm\", \"Mean_Temperature_C\", \n                              \"Maximum_Temperature_C\", \"Minimum_Temperature_C\", \n                              \"Mean_Wind_Speed_km_h\", \"Max_Wind_Speed_km_h\")\n\nJuly_data$Year &lt;- as.factor(July_data$Year)\n\n\nI will use the code below to combine the July and December data sets.\n\nJuly_data$Month &lt;- 'July'\nDecember_data$Month &lt;- 'December'\n\ncombined_data &lt;- rbind(July_data, December_data)\n\nI will use the datatable() function to inspect the combined data set.\n\nData TableData StructureMissing Values?\n\n\n\nDT::datatable(combined_data, class= \"compact\")\n\n\n\n\n\n\n\n\nstr(combined_data)\n\ntibble [310 × 11] (S3: tbl_df/tbl/data.frame)\n $ ID                     : chr [1:310] \"1\" \"1\" \"1\" \"1\" ...\n $ Station                : chr [1:310] \"Changi\" \"Changi\" \"Changi\" \"Changi\" ...\n $ Year                   : Factor w/ 5 levels \"1983\",\"1993\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Month                  : chr [1:310] \"July\" \"July\" \"July\" \"July\" ...\n $ Day                    : num [1:310] 1 2 3 4 5 6 7 8 9 10 ...\n $ Daily_Rainfall_Total_mm: num [1:310] 8.7 0 5.3 6.2 39.5 14.9 0 0 10.5 55.5 ...\n $ Mean_Temperature_C     : num [1:310] 27.5 28.7 27.9 28 25.4 27.1 26.2 28.2 27.8 25.4 ...\n $ Maximum_Temperature_C  : num [1:310] 33 32.6 32 31.9 27.4 31.1 28.1 31.9 31.1 27.2 ...\n $ Minimum_Temperature_C  : num [1:310] 24.4 25.5 24.4 25.7 21.4 24.1 22.6 25.6 26 23 ...\n $ Mean_Wind_Speed_km_h   : num [1:310] 3.7 10.5 5.8 7.6 3.6 8.4 5 11.8 9 5.3 ...\n $ Max_Wind_Speed_km_h    : num [1:310] 28.8 38.2 44.6 51.8 36 31.3 42.5 39.6 43.9 46.8 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   X1 = col_character(),\n  ..   X2 = col_double(),\n  ..   X3 = col_double(),\n  ..   X4 = col_double(),\n  ..   X5 = col_double(),\n  ..   X6 = col_skip(),\n  ..   X7 = col_skip(),\n  ..   X8 = col_skip(),\n  ..   X9 = col_double(),\n  ..   X10 = col_double(),\n  ..   X11 = col_double(),\n  ..   X12 = col_double(),\n  ..   X13 = col_double()\n  .. )\n\n\n\n\n\nsum(is.na(combined_data))\n\n[1] 0\n\n\nThere are no missing values in our data set."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#frequency-and-intensity-of-rainfall-events",
    "href": "R-ex/R-Ex4/R_Ex4.html#frequency-and-intensity-of-rainfall-events",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.1 Frequency and Intensity of Rainfall events",
    "text": "3.1 Frequency and Intensity of Rainfall events\nFirst, I will use the code below to create a static line plot to visualize July’s and December’s rainfall over the 5 years.\n\n\nShow the code\nggplot(combined_data, aes(x = Day, \n        y = Daily_Rainfall_Total_mm, group = Month)) +\n  \n  # Use geom_area for one of the months, July in this case\n  geom_area(data = subset(combined_data, Month == \"July\"), \n            aes(fill = Year), alpha = 0.3) +\n  \n  # Use geom_line for both months to ensure the line is on top of the fill\n  geom_line(aes(color = Year, \n                linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year (1983 - 2023)\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       fill = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  \n  guides(fill = guide_legend(\"Year\"), color = guide_legend(\"Year\"), \n         linetype = guide_legend(\"Month\"))\n\n\n\n\n\n\n\n\n\nNext, to enable readers to specifically examine the rainfall measures in more detail, I will use the code below to add interactivity.\n\n\nShow the code\np3 &lt;- ggplot(combined_data, aes(x = Day, y = Daily_Rainfall_Total_mm)) +\n  \n  # Use geom_line for both months, specifying linetype based on Month\n  geom_line(aes(color = Year, linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year (1983-2023)\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  \n  guides(fill = guide_legend(\"Year\"), color = guide_legend(\"Year\"), \n         linetype = guide_legend(\"Month\")) + scale_linetype_manual(values = c(\"July\" = \"dotted\", \"December\" = \"solid\"))\n\n\n\nggplotly(p3)\n\n\n\n\n\n\n\n\n\n\n\nObservation 1 - Frequency and Intensity of Rainfall events show no substantial increase\n\n\n\nThe infographics had reported that the Intensity and frequency of heavy rain fall events is expected to increase as the world gets warmer.\nFrom the visualisations above, in general, there are visibly more rain days in December relative to July over the years.\nHowever, there seems to be no substantial increase in the intensity of heavy rain events. For example, we can see that the highest recorded daily rainfall event occured in December 1983 at 164.4 mm. However, this level of intensity has not been surpassed in subsequent years.\n\nAdditionally, the volume of rainfall for both July and December 2023 is the least within the 5 years.\nThis further challenges the claim that the intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#one-way-anova-test-on-daily-rainfall-for-the-same-month-by-year",
    "href": "R-ex/R-Ex4/R_Ex4.html#one-way-anova-test-on-daily-rainfall-for-the-same-month-by-year",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.2 One-way Anova test on Daily Rainfall for the same month, by year",
    "text": "3.2 One-way Anova test on Daily Rainfall for the same month, by year\nThe infographics had reported that the Intensity and frequency of heavy rain fall events is expected to increase as the world gets warmer.\nOur visualisations in the previous section have showed otherwise.\nWe can validate this statistically and examine if there are any significant statistical differences between the daily rainfall for each July, and each December, across the five years.\nFirst, we will need to ascertain the nature of the distribution, and see if it is normal or non-normal distributed.\nWe can first visualise the distribution of daily rainfall using ridgeline plots, using the code below.\n\n\nShow the code\nggplot(July_data, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\"),\n    alpha = 0.6,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Daily Rainfall (mm)\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Distribution of the Daily Rainfall for July (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(December_data, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\"),\n    alpha = 0.6,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Daily Rainfall (mm)\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Distribution of the Daily Rainfall for December (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the ridgeline plots above, we can see that the distribution for both July and December rainfall do not resemble a normal distribution, and that there is a right skewness.\n\n\nSince the distribution of rainfall is non-normal, I will conduct non-parametric tests.\nThe Hypothesis will be as such:\nH0: There is no difference between the median daily rainfall for the same month across the 5 years.\nH1: There is a difference between the median daily rainfall for the same month across the 5 years.\nI will use ggstatsbetween() from the ggstatplot package.\n\n\nShow the code\nggbetweenstats(\n  data = July_data,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  ggtitle(\"Daily Rainfall for July across the years (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggbetweenstats(\n  data = December_data,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  ggtitle(\"Daily Rainfall for December across the years (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation results\n\n\n\nIn both the tests above, the p-value is &gt; 0.05.\nHence, we have insufficient evidence to reject the Null Hypothesis and can conclude that there is no strong evidence to indicate that there is a difference in the Daily rainfall for the same month across the years.\nThis supports Observation 1, where we concluded that there seems to be no substantial increase in the frequency and intensity of heavy rain fall events."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#differences-between-july-and-december-across-the-years-dry-vs-wet-months",
    "href": "R-ex/R-Ex4/R_Ex4.html#differences-between-july-and-december-across-the-years-dry-vs-wet-months",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.3 Differences between July and December across the years (Dry Vs Wet Months)",
    "text": "3.3 Differences between July and December across the years (Dry Vs Wet Months)\nTo visualise the differences for the June-December periods, I will use the codes below to examine:\n\nThe differences in the number of Rain days, and\nThe differences in the Daily Rainfall, between the two months, across the years\n\n\n\nShow the code\nrain_fall_summary &lt;- combined_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarize(\n    MeanRainfall = mean(Daily_Rainfall_Total_mm, na.rm = TRUE),\n    RainyDays = sum(Daily_Rainfall_Total_mm &gt; 0, na.rm = TRUE), # Count days with rain\n    .groups = 'drop'\n  )\n\n\n\n\nShow the code\np_1 &lt;- ggplot(rain_fall_summary, aes(x = Year, y = RainyDays, group = Month, color = Month)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"The largest difference in the number of Rain Days between\\nJuly and December was in 2023\",\n       x = \"Year\",\n       y = \"Number of Rainy Days\",\n       color = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np_2 &lt;- ggplot(rain_fall_summary, aes(x = Year, y = MeanRainfall, group = Month, color = Month)) +\n  geom_line() +\n  geom_point() +\n  \n  labs(title = \"Both July's and December's mean Daily Rainfall have\\ntrended lower over the Years\",\n       x = \"Year\",\n       y = \"Mean Daily Rainfall (mm)\",\n       color = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_3 &lt;- ggplot(combined_data, aes(x = Year, y = Daily_Rainfall_Total_mm, fill = Month)) +\n  geom_boxplot() +\n  labs(title = \"The gap between July and December has reduced in 2023 \",\n       subtitle = \"The intensity of December's heavy rainfall events seem to be diminishing\",\n       x = \"Year\",\n       y = \"Daily Rainfall (mm)\",\n       fill = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\nUsing Patchwork to combine the plots.\n\n\nShow the code\ncombined_plot &lt;- (p_1 + p_2) / p_3 + \n  plot_annotation(\n    title = \"The contrast between July and December has become more pronounced over the years\",\n    theme = theme(\n      plot.title = element_text(size = 20, face = \"bold\")\n    )\n  ) +\n  theme(plot.title.position = \"plot\")\n\ncombined_plot\n\n\n\n\n\n\n\n\n\nNext, to enable readers to specifically examine the differences in the rainfall between the two months in more detail, I will use the code below to add interactivity.\n\n\nShow the code\ncombined_mean &lt;- combined_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(MeanRainfall = round(mean(Daily_Rainfall_Total_mm, na.rm = TRUE), 2)) %&gt;%\n  ungroup()\n\n# Create the ggplot object\np_combined &lt;- ggplot(combined_data, aes(x = Year, y = Daily_Rainfall_Total_mm, color = Month)) +\n  geom_jitter(aes(text = paste('Day:', Day, 'Month:', Month)), width = 0.2, alpha = 0.5) + \n  geom_line(data = combined_mean, aes(x = Year, y = MeanRainfall, group = Month), \n            size = 0.5, linetype = \"dotted\") + \n  geom_point(data = combined_mean, aes(x = Year, y = MeanRainfall), \n             size = 3, show.legend = FALSE) + \n  scale_color_manual(values = c(\"July\" = \"blue\", \"December\" = \"red\")) +\n  labs(title = \"Daily Rainfall in July and December (1983-2023)\",\n       x = \"Year\",\n       y = \"Daily Rainfall Total (mm)\") +\n  theme_minimal() +\n  \n  \n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(title = \"Month\"))\n\n\n\n\nShow the code\np_plotly &lt;- ggplotly(p_combined) %&gt;%\n  layout(hovermode = 'closest') \n\np_plotly\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 2 - Contrast between Dry and Wet Months are likely to be more pronounced\n\n\n\nThe infographics had reported that the contrast between Dry and Wet months is likely to be more pronounced.\nOur plots above show that:-\n\nThe difference in the number of rain days between July and December, in 2023, was at its highest, relative to previous years. The number of rain days in July has decreased, and the number of rain days in December has increased, over the years.\nThe mean daily rain fall for both July and December 2023 was at its lowest relative to other years.\nIn 2023, the gap of heavy rain fall events have been reduced. In previous years, there were more days with heavy rain fall in December relative to July.\n\nHence, this lends credence to the claim that the contrast between Dry and Wet months is likely to be more pronounced."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#two-sample-mean-test-july-vs-december-by-year",
    "href": "R-ex/R-Ex4/R_Ex4.html#two-sample-mean-test-july-vs-december-by-year",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.4 Two-sample mean test: July Vs December, by Year",
    "text": "3.4 Two-sample mean test: July Vs December, by Year\nThe infographics had reported that the contrast between wet months and dry months is likely to be more pronounced.\nOur visualisations in the previous section have corroborated this claim.\nWe can validate this statistically and examine if there is any evidence to suggest that there is indeed a difference in the daily rainfall between the two months (July and December).\nThe Hypothesis will be as such:\nH0: There is no difference in the median daily rainfall between July and December across the 5 years\nH1: There is a difference in the median daily rainfall between July and December across the 5 years\nSince we have rain fall data for 5 different years, I will use grouped_ggbetweenstats() from the ggstatplot package.\n\n\nShow the code\ngroup_plot &lt;- grouped_ggbetweenstats(\n  data = combined_data,\n  x = Month,\n  y = Daily_Rainfall_Total_mm,\n  grouping.var = Year,\n  type = \"np\", # for non-parametric\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\", \n  output = \"plot\" \n) \n\ngroup_plot + plot_annotation(\n    title = \"Differences between July and December over the years (1983-2023)\",\n    theme = theme(\n      plot.title = element_text(size = 20, face = \"bold\")\n    )\n  ) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Results\n\n\n\nIn the tests above, the p-value is &gt; 0.05 for the years 1983,1993,2003 and 2013.\n\nHowever the p-value is &lt; 0.05 for year 2023.\nHence we can reject the Null Hypothesis and can conclude that there is some evidence to suggest that there is some difference in the daily rainfall between July to December, across the 5 years.\nThis supports Observation 2, where we concluded that the contrast between Dry and Wet months is likely to be more pronounced."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#interactive-error-bars",
    "href": "R-ex/R-Ex4/R_Ex4.html#interactive-error-bars",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "4.1 Interactive Error Bars",
    "text": "4.1 Interactive Error Bars\nWe can also visualise the uncertainty of point estimates by plotting interactive error bars for the 99% confidence interval of mean Daily Rainfall in July and December by year as shown in the figures below.\n\n\nShow the code\nshared_df = SharedData$new(July_rain)\n\nbscols(widths = c(6,6),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(Year, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=Year, \n                     y=mean, \n                     text = paste(\"Year:\", `Year`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Rainfall:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Year\") + \n                   ylab(\"Average Daily Rainfall (mm)\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of&lt;br&gt;Avg Rainfall in July by Year\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 14,\n                                    scrollX=T), \n                     colnames = c(\"No. of observations\", \n                                  \"Avg Daily Rainfall\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nshared_df2 = SharedData$new(Dec_rain)\n\nbscols(widths = c(6,6),\n       ggplotly((ggplot(shared_df2) +\n                   geom_errorbar(aes(\n                     x=reorder(Year, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=Year, \n                     y=mean, \n                     text = paste(\"Year:\", `Year`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Rainfall:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Year\") + \n                   ylab(\"Average Daily Rainfall (mm)\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of&lt;br&gt;Avg Rainfall in Dec by Year\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df2, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 14,\n                                    scrollX=T), \n                     colnames = c(\"No. of observations\", \n                                  \"Avg Daily Rainfall\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations from Interactive Error bars\n\n\n\nJuly Trends:\n\n2023’s Average rainfall was the lowest, at 5.10 mm.\nThe increasing standard deviations (sd) from 2013 to 2023, indicates a trend of more variability in daily rainfall relative to the past.\n\nDecember Trends:\n\n2023’s Average rainfall was the lowest, at 8.26 mm.\nThe standard deviation shows a decrease over the 40-year span, indicating less variability in daily rainfall in December over time.\n\nJuly vs. December Relationship:\nThe differences in standard deviation (sd) between December and July for each respective year are as follows:\n\n1983: 18.92 mm\n1993: 4.24 mm\n2003: 10.12 mm\n2013: 15.59 mm\n2023: 0.82 mm\n\nThe difference in variability between December and July was much higher in 1983, but has significantly decreased by 2023.\nThe year 2023 stands out, with the variability in December being almost similar to that in July, indicating a convergence in the variability of rainfall between the two months in that year."
  }
]