[
  {
    "objectID": "Write-ex/Write-Ex4/Write_Ex4.html",
    "href": "Write-ex/Write-Ex4/Write_Ex4.html",
    "title": "Predictive Analytics on Loan Default Behaviour",
    "section": "",
    "text": "Overview of Project\n\n\n\nImportant Note:\nSAS and all other SAS Institute Inc. product or service names are registered trademarks or trademarks of SAS Institute Inc. in the USA and other countries. ® indicates USA registration.\nOther brand and product names are trademarks of their respective companies.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "",
    "text": "In recent years, the prevalence of digital payments in the gambling industry has raised questions about how this technology may influence individuals’ gambling behaviours and its potential to contribute to harm. There is a research gap in terms of understanding how these digital payments might contribute to harm, and how they might provide opportunities for harm prevention."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#removing-clients-with-no-deposits-made-to-their-wagering-accounts",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#removing-clients-with-no-deposits-made-to-their-wagering-accounts",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.1 Removing clients with no deposits made to their wagering accounts",
    "text": "3.1 Removing clients with no deposits made to their wagering accounts\nThere are 77 accounts with no deposits made to their wagering accounts during the time period. These accounts will be removed. We use SQL code in SAS Studio to identify these accounts.\n\n\n\nFig 5: Creating a table for account identifiers with no ‘LOYALTYCARDDEBIT’ transactions\n\n\n\n\n\nFig 6 : Generating a report to show the accounts with no deposits made"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#converting-timestamp-to-datetime-and-creating-dates-and-time-columns",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#converting-timestamp-to-datetime-and-creating-dates-and-time-columns",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.2 Converting timestamp to datetime and creating dates and time columns",
    "text": "3.2 Converting timestamp to datetime and creating dates and time columns\nThe timestamp column in the transaction dataset was set with a varchar format.\n\n\n\nFig 7: Data type of variables in Online_sports_DIB dataset\n\n\nWe convert the timestamp variable to a datetime format and rename the column ‘Timestamp’.\n\nSteps:\nIn SAS Data Studio -Prepare Data &gt; Column Transforms &gt; Convert column &gt; Conversion\nIn SAS Data Studio -Prepare Data &gt; Column Tranforms &gt; Rename column &gt; Name of new column\n\n\n\nFig 8: Converting to datetime format and renaming to ‘Timestamp’\n\n\nTo further understand of the extent of transaction activities intraday, we decompose the timestamp to date and time.\n\nSteps:\nIn SAS Data Studio -Prepare Data &gt; Column Transforms &gt; Convert column &gt; Conversion (select date & time)\n\n\n\nFig 9: Deriving distinct dates and time from timestamp\n\n\n\n\n\nFig 10: New Time and Date columns are generated"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#filtering-out-transactions-beyond-the-sample-cut-off-date",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#filtering-out-transactions-beyond-the-sample-cut-off-date",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.3 Filtering out transactions beyond the sample cut-off date",
    "text": "3.3 Filtering out transactions beyond the sample cut-off date\nThere are 3,268 transactions which were recorded beyond the sample date cut off of 29/02/2020. These were filtered out in SAS data studio-Prepare Data.\n\n\n\nFig 11: Dataset contains 3,268 transactions beyond the sample cut-off date of 29/02/2020\n\n\nSteps:\nIn SAS Data Studio -Prepare Data &gt; Row Transforms &gt; Filter &gt; column (TimeStamp), operator (less than), and value (1/3/2020)\n\n\n\nFig 12: Filtering out transactions beyond cut-off date of 29/02/2020"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#filtering-out-transactions-with-na-status",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#filtering-out-transactions-with-na-status",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.4 Filtering out transactions with N/A status",
    "text": "3.4 Filtering out transactions with N/A status\nFrom SAS Visual Analytics, we noted that there are 6 transactions with ‘NA’ status. This will not bring any meaningful insight and will be filtered out.\n\n\n\nFig 13: There are 6 transactions with ‘NA’ status\n\n\nSteps:\nIn SAS Data Studio -Prepare Data &gt; Row Transforms &gt; Filter &gt; column (Status), operator (Not Contains), and value (NA).\n\n\n\nFig 14: Filtering out transactions with ‘NA’ status"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#removing-duplicates-in-data-set",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#removing-duplicates-in-data-set",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.5 Removing Duplicates in data set",
    "text": "3.5 Removing Duplicates in data set\nUsing SQL code in SAS studio we noted that there are 4 duplicate entries in the dataset.\n\n\n\nFig 15: SQL code to check for duplicates in dataset\n\n\n\n\n\nFig 16: 4 Duplicates were found\n\n\nThe duplicates were subsequently removed in SAS Data studio.\nSteps: SAS Data Studio -Prepare Data &gt; Data Quality Transforms &gt; Remove Duplicates\n\n\n\nFig 17: Removing duplicates from the dataset"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#renaming-transaction-types",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#renaming-transaction-types",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.6 Renaming Transaction types",
    "text": "3.6 Renaming Transaction types\nThere are 4 transaction types, namely, ‘LOYALTYCARDDEBIT’,’LOYALTYCARDCREDIT’, ’LOYALTYCARDCREDITCL’ and ‘LOYALTYCARDCREDITACH’. We rename them to easily be able to infer the direction of funds. This is done through SQL code.\n\n‘LOYALTYCARDDEBIT’ to ‘Deposit into Wager’\n‘LOYALTYCARDCREDIT’ to ‘Withdrawal from Wager’\n‘LOYALTYCARDCREDITCL’ to ‘Credit Line Deposit’\n‘LOYALTYCARDCREDITACH’ to ‘Electronic Deposit’\n\nFig 18: Renaming Transaction types by SQL code"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-categorical-variables-for-date-and-time",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-categorical-variables-for-date-and-time",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.7 Creating new categorical variables for date and time",
    "text": "3.7 Creating new categorical variables for date and time\nSince transactions are recorded on an intraday basis, we re-categorize it into day type (weekday or weekend) and segments of the day (day, evening, and dawn) to help us analyse transaction patterns. This is done through SQL code.  \n\n\n\nFig 19: Categorizing transactions into day segments and days of the week"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-time-variables-for-betting-activity",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-time-variables-for-betting-activity",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.8 Creating new time variables for betting activity",
    "text": "3.8 Creating new time variables for betting activity\nTo better understand when clients place their bets online, we create new variables using SQL code. From previously created categories of time, we sum the frequency counts of transactions. This is then grouped by bets placed during the day, night, dawn, weekday, and weekend.  A new table ‘BetTimeCounts_Percentages’ will be created.\n\n\n\nFig 20: SQL code to create new time variables\n\n\n\n\n\nFig 21: A new table ‘BetTimeCounts_Percentages’ is created\n\n\nAssumption:\nWe assume that a successful deposit into the wagering account is a betting attempt i.e., an ‘Approved’ transaction type of ‘LOYALTYCARDDEBIT’."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-variables-to-analyse-the-winning-data-of-clients",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-variables-to-analyse-the-winning-data-of-clients",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.9 Creating new variables to analyse the ‘Winning’ data of clients",
    "text": "3.9 Creating new variables to analyse the ‘Winning’ data of clients\nTo analyse if clients are actually winning from their betting activities. A new table ‘WinMetrics’ will be created using SQL code. This will include the frequency count of withdrawals from wager account, total winnings, and net winnings.\nAssumption:\nWe assume that a successful withdrawal from the wagering account is a win from betting.\n\n\n\nFig 22: SQL code to create new variables that capture the ‘win’ information\n\n\n\n\n\nFig 23: A new table ‘WinMetrics’ is created to capture the ‘win’ data"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#capturing-information-on-declined-transactions",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#capturing-information-on-declined-transactions",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.10 Capturing information on Declined transactions",
    "text": "3.10 Capturing information on Declined transactions\nThere were 10,876 declined transactions, mainly made up of declined deposits to the digital wallet and declined deposits to wager accounts. It would be worthwhile to analyse which types of customers are getting declined, as this could be an indication of financial stress or over-gambling. A new table ‘DeclineData’ is created to capture the frequency counts of declined transactions.\n\n\n\nFig 24: Bar chart showing 4,627 declined deposits into wager accounts (SAS Visual Analytics)\n\n\n\n\n\nFig 25: SQL code to create new variables to capture declined transactions\n\n\n\n\n\nFig 26: A new table ‘Decline Data’ is created"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-recency-frequency-and-monetary-variables-and-merging-tables",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#creating-new-recency-frequency-and-monetary-variables-and-merging-tables",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "3.11 Creating new Recency, Frequency and Monetary variables and Merging tables",
    "text": "3.11 Creating new Recency, Frequency and Monetary variables and Merging tables\nNew recency, frequency, and monetary (RFM) variables (Fig 27) are created, using SQL code.\n\n\n\nFig 27: New ‘RFM’ Variables to measure Betting Activity\n\n\n\n\n\nFig 28: SQL code to create new RFM variables\n\n\nLastly, we use the Join method in SQL code to merge all our tables to a combined dataset. This will be used for our subsequent clustering analysis. Our final dataset has 5575 distinct Account holders.\n\n\n\nFig 29: SQL code to join all our created tables to a merged dataset\n\n\n\n\n\nFig 30: The consolidated dataset to analyse gambling behaviour and patterns"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#univariate-analysis",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#univariate-analysis",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4.1 Univariate Analysis",
    "text": "4.1 Univariate Analysis\nIn SAS Visual Analytics, we populate the histograms of our variables. The following were observed:\n\nMost variables are heavily skewed , meaning that transformation (e.g., log transform) would be required.\nSome variables have some ‘0’ values because of the way they are calculated.\nThese values will be applied with a constant when doing log transformation as log of zero is undefined in real numbers.\n\n\n\n\nFig 31: Examining the distribution of our variables from the merged dataset\n\n\n\n\n\nFig 32: 4 Variables had more than 90% zero values\n\n\n4 Variables, ‘DeclinedAmt’, Withdrawal_Count’, ‘DeclinedCount’ and ‘ROI’  will be dropped due to a high proportion of ‘0’ values"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#bivariate-analysis",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#bivariate-analysis",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4.2 Bivariate Analysis",
    "text": "4.2 Bivariate Analysis\nNext, we analyse the correlations between the remaining variables to check for variable pairs with strong correlation &gt; 0.8.\n\n\n\nFig 33: Correlation matrix of our variables from the merged dataset\n\n\nFrom the correlation matrix, the following 9 pairs of variables were observed to have a high correlation &gt; 0.8 (Fig 34).\n\n\n\n\nFig 34: Variables pairs with high correlation &gt; 0.8\n\n\n7 variables: ‘Average_deposit’, ‘Deposit_Count’, ‘BetWeekdaypercentage’, ‘Max_Wager’, ‘Net_Cash_Flow’ , ‘Total_Deposit_Amount’ and ‘BetActiveDays’ were dropped.\nThe remaining 12 variables were retained for our clustering analysis.\n\n\n\nFig 35: Scatter plot of the remaining variables\n\n\nFig 36 describes our 12 clustering variables.\n\n\n\nFig 36: Finalized clustering variables and their description"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#log-transformation-of-clustering-variables",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#log-transformation-of-clustering-variables",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4.3 Log Transformation of Clustering variables",
    "text": "4.3 Log Transformation of Clustering variables\nMost of our clustering variables are heavily skewed. We will do a log transformation to reduce the skewness. This is because K-means is highly sensitive to highly skewed data.\n\n\n\nFig 37: Distribution of the 12 clustering variables.\n\n\nNext, we apply log transformation to transform highly skewed variables. As some variables have some ‘0’ values, we will need to apply a constant before doing a log transform.\nSteps:\nSAS Visual Analytics &gt; Data &gt; New Data Item&gt; Calculated item &gt; select variable &gt; Operators &gt; Numeric (simple)&gt; x+y &gt; Numeric (advanced) &gt; Log &gt; insert constant and log\n\n\n\nSteps for log transformation\n\n\nThe results of our log transformation can be seen in Fig 38.\n\nWe conduct another correlation analysis to check if any of our correlations have changed. (Fig 39)\n\n\n\nFig 38: Visual analysis of our distributions after log transformation.\n\n\n\n\n\nFig 39: Correlation matrix of variables after log transform.\n\n\nUpon checking, variable pair ‘Log 10 withdrawal’ & ‘Log 10 average withdrawal’ were found to have a high correlation of &gt; 0.8. ‘Log 10 withdrawal’ was subsequently removed.\nWe retain the remaining 11 variables and proceed with our K-means clustering."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#k-means-clustering-interation-one-11-variables-4-clusters",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#k-means-clustering-interation-one-11-variables-4-clusters",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4.4 K-means clustering interation one (11 variables & 4 clusters)",
    "text": "4.4 K-means clustering interation one (11 variables & 4 clusters)\nOur first K-means iteration uses our finalized 11 clustering variables and 4 clusters.\nSteps: SAS Visual Analytics &gt; Objects &gt; Statistics &gt; Cluster\n\nSteps: Options &gt; Add clustering variables\n\nThe number of clusters is automatically assigned by selecting the Automatic Aligned Box Criterion (ABC) function. We specify a range of 2 and 8 as the min and max number of clusters. Seed number is inputted as 1234 for reproducibility.\nAs we have log transformed variables, we apply ’None” under standardization.\nSteps: SAS Visual Analytics &gt; Options &gt; Cluster\n\nWe also enable ‘ABC Statistics’ to evaluate the automatic assignment of cluster number.\nSteps: SAS Visual Analytics, Options&gt; Model Display &gt; General &gt;Select ABC Statistics\n\nFrom the ABC Statistics graph(Fig 40), we analyse the Gap statistics further. We observe that the Gap statistic for 4 clusters (0.1003) is higher than that for 5 clusters (-0.0250) i.e., after 4 clusters, the Gap statistic starts to decrease, indicating diminishing returns in cluster quality or compactness.\nThis can typically be interpreted as the optimal number of clusters for the dataset.\n\nAs K-means is an unsupervised learning method, we will just use this auto-assignment as a start for our first clustering analysis. Subsequently, we will change the number of clusters to observe the difference in the quality of the clustering operation.\n\n\n\nFig 40: Recommended number of clusters based on Automatic (Aligned box criterion)\n\n\nCluster visualization\nAs can be seen from Fig 41, there is some overlapping among some clusters, which suggest that some data points might have similar attributes across clusters. Some clusters are larger in terms of their spread (width and height of the ellipses), indicating a higher variance within those clusters.\n\n\n\nFig 41: Cluster Diagram using 11 variables and 4 clusters\n\n\nCluster Summary Statistics\nFrom the cluster summary (Fig 42) we observe the following:-\n\nCluster 3 has the majority of data points (2833) and has overlaps with all the other 3 clusters.\nCluster 2 is the most compact with the lowest RMS of STD.\nCluster 4, while having the fewest observations, is the most spread-out. As can be seen from its high RMS of STD.\nCluster 1 shows a moderate spread compared to other clusters\n\n\n\n\n\nFig 42: Cluster summary for 4 clusters\n\n\nThe parallel coordinate graphs (Fig 43 & 44) provide a way to visually inspect how each cluster differs across the different variables. Visually it is evident that the clusters differ across some variables but converge on others i.e., the similarities and dissimilarities between each cluster is quite even in proportion.\n\n\n\nFig 43: Parallel plot of 4 clusters with 11 variables\n\n\n\n\n\nFig 44: By selecting individual clusters, we can dim other clusters to visually see the influence of each cluster for each variable\n\n\nNext, we use the ‘derive cluster ID’ function in Visual Analytics to create a new categorical data role. This will enable us to analyse each cluster on an individual variable basis using bar charts on a separate page.\nSteps: Cluster graph Options &gt; Derive cluster ID items &gt; define cluster id name\n\n\n\n\n\n\nFig 45: using the ‘Derive Cluster ID items’ function to create a new categorical data role\nThis new cluster ID will populate in Data roles. We can select this role to populate individual bar charts for each clustering variable (Fig 46).\n\n\n\nFig 46: Bar charts of the clustering variables by Cluster ID\n\n\n\n\n\nFig 47: Cluster mean for 4 clusters\n\n\nUsing the parallel plots of the 4 clusters (Fig 44), and bar charts of the clustering variables (fig 46), we summarize the ‘ranking’ of the 4 clusters in the table below(Fig 48).\n\n\n\nFig 48: Ranking table of the 4 cluster operation\n\n\nRanking for 1st to 4th is by order of magnitude. For wager recency, a ranking of 1st is equivalent to the most recent betting activity.\n\nAn explanation of the 4-cluster profiles and characteristics can be found in Appendix B."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#k-means-clustering-iteration-two-11-variables-6-clusters",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#k-means-clustering-iteration-two-11-variables-6-clusters",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4.5 K-means clustering Iteration two (11 variables & 6 clusters)",
    "text": "4.5 K-means clustering Iteration two (11 variables & 6 clusters)\nNext, we change the number of clusters to 6, to see if there is any difference in the quality of the clustering operation.\nSteps: Options &gt; General &gt; Number of Clusters (input 6)\n\n\n\n\nFig 49: Cluster diagram using 6 clusters\n\n\nCluster visualization\nAs can be seen from Fig 49, with the addition of 2 more clusters, there is more overlapping among the clusters, which suggest that adding more clusters may not have improved the distinctiveness between clusters.\nHowever, this can only be confirmed after examining the characteristics of the clusters on an individual variable basis.\nCluster Summary Statistics\nFrom the cluster summary (Fig 50) we observe the following:-\n\nCluster 1: Balanced size with 1,154 observations; nearest to Cluster 4.\nCluster 2: Smallest cluster at 263 observations; closest relationship with Cluster 3.\nCluster 3: Mid-sized (485 observations) with a moderate spread; leans towards Cluster 4.\nCluster 4: Largest cluster (2,716 observations) and most compact; central reference for Clusters 1 and 3.\nCluster 5: Most diverse with highest RMS STD (44.69); closest to Cluster 1.\nCluster 6: Mid-size with 440 observations; also has a tendency towards Cluster 1.\n\n\n\n\nFig 50: Cluster summary of the 6 clusters\n\n\n\n\n\nFig 51: Parallel plot graph of the 6 clusters\n\n\nSimilarly, we use the ‘derive cluster ID’ function to create a new categorical data role. This will enable us to analyse the characteristics of each cluster on an individual variable basis using bar charts on a separate page (Fig 52).\n\n\n\nFig 52: Bar charts of the clustering variables grouped by Cluster ID\n\n\nUsing the parallel plots (Fig 51) and the bar charts (Fig 52), we summarize the ‘ranking’ of the 6 clusters in the table below (Fig 53).\n\n\n\nFig 53: Ranking table of 6 Cluster operation\n\n\nRanking for 1st to 6th is by order of magnitude. For wager recency, a ranking of 1st is equivalent to the most recent betting activity.\nAn explanation of the 6-cluster profiles and characteristics can be found in Appendix C."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#k-means-clustering-iteration-three-9-variables-5-clusters",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#k-means-clustering-iteration-three-9-variables-5-clusters",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4.6 K-means clustering Iteration three (9 variables & 5 clusters)",
    "text": "4.6 K-means clustering Iteration three (9 variables & 5 clusters)\nIn the previous 2 iterations, we had used 11 clustering variables. From the previous collinearity matrix, there were 2 variable pairs of correlation close to 0.8 which were retained. Specifically, ‘BetDayPercentage’ & ‘BetDawnPercentage’, and ‘Log 10 Wager Frequency’ & ‘Log 10 Total Wager had correlations of above 0.75’.  \nIn iteration 3, we drop 2 variables i.e., ‘BetDayPercentage’ and ‘Log 10 Total Wager’ to see if this improves our clustering operation. Similarly, the number of clusters (5) is derived from the Automatic Aligned Box Criterion (ABC) function in SAS visual Analytics (Fig 55).\n\n\nFig 54: The remaining 9 variables do not exceed 0.7 in terms of correlation\n\n\n\nFig 55: We enable the automatic ABC criteria to derive 5 clusters\n\n\n\n\n\nFig 56: Cluster diagram of 9 variables and 5 clusters\n\n\nCluster visualization\nAs can be seen from Fig 56, with the removal of 2 variables, there is more overlapping among the clusters compared to the previous 2 iterations. This suggest that this had reduced the distinctiveness between clusters. Cluster 3 is the ‘central’ cluster which captures most of the characteristics of other clusters.\nCluster 3 dominates with more than 50% of the observations and is the common cluster linking other clusters.\n\nFig 57: Cluster summary of the 5 clusters\nThe dominance of cluster 3 becomes more apparent as we examine the Parallel coordinates graph of the clusters (Fig 58). By selecting cluster 3 and dimming the others, we observe that cluster 3 occupies a large range of the percentile segments for each variable.\n\n\n\nFig 58: Parallel coordinates graph of the 5 clusters, and Cluster 3’s influence on variables\n\n\nAs a result, we will proceed to delve further into Iterations 1 & 2 instead for our next section."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#data-preparation-change-log",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#data-preparation-change-log",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "Data preparation Change log",
    "text": "Data preparation Change log"
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#cluster-profile-and-characteristics",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#cluster-profile-and-characteristics",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "4-Cluster Profile and Characteristics",
    "text": "4-Cluster Profile and Characteristics\n\n\nCluster 1 (Dawn Predominant)\n\nProfile:\n\nHigh activity during dawn, with average wagering amounts and frequency.\nModerate wager recency indicates they bet not too distantly in the past.\nHave the lowest minimum wager amount, suggesting some caution.\n\nPossible Markers of Harm:\n\nThe propensity to bet during dawn might indicate disrupted sleep patterns or prioritizing gambling over other morning activities.\n\n\nCluster 2 (Consistent High Rollers)\n\nProfile:\n\nHighest average wager amounts and highest total wagered in the period.\nModerately recent betting activity and consistent frequency.\nBets are spread fairly evenly across the day, night, and weekends.\n\nPossible Markers of Harm:\n\nThe consistently high amounts wagered could be indicative of potential gambling problems.\nRegularity across different times might indicate dependency or habitual gambling.\n\nCluster 3 (Frequent Bettors)\n\n\nProfile:\n\nLargest cluster indicating a significant segment of the gambler population.\nHigh withdrawal amounts, most recent betting activity, and most frequent wagers.\nModerate betting amounts but leaning towards higher total wagers due to the frequency.\n\nPossible Markers of Harm:\n\nFrequent betting might indicate compulsive gambling behaviours.\nThe high withdrawal amount may signify dependency on winnings for daily expenses or immediate rewards.\n\n\nCluster 4 (Night Enthusiasts)\n\nProfile:\n\nSmallest cluster in terms of population.\nPredominantly active during nighttime with moderate average wagers.\nLess recent betting activity, suggesting they don’t bet as frequently as others.\n\nPossible Markers of Harm:\n\nPredominant night-time activity could suggest using gambling as an escape from daily stresses or other issues. A less frequent betting pattern might indicate periodic but intense gambling sessions."
  },
  {
    "objectID": "Write-ex/Write-Ex2/Write_Ex2.html#cluster-profile-and-characteristics-1",
    "href": "Write-ex/Write-Ex2/Write_Ex2.html#cluster-profile-and-characteristics-1",
    "title": "Assignment 2 - Be Customer wise or otherwise",
    "section": "6-cluster Profile and Characteristics",
    "text": "6-cluster Profile and Characteristics\n\n\nCluster 1 (High Rollers)\n\nProfile:\n\nThese customers have the highest average wagers and also have high withdrawal amounts.\nTheir betting is consistent across different times of the day.\n\nPossible Markers of Harm:\n\nThe consistently high amounts wagered might indicate potential gambling problems or financial distress.\nTheir constant betting behaviour, irrespective of the time, may indicate a lack of self-control or compulsive gambling tendencies.\n\n\nCluster 2 (Dawn & Weekend Enthusiasts)\n\nProfile:\n\nPredominantly active during early mornings and weekends with smaller wager amounts.\n\nPossible Markers of Harm:\n\nBetting during unusual hours (early morning) may signify disrupted sleep patterns or prioritizing gambling over other essential activities.\n\n\nCluster 3 (Steady Gamblers)\n\nProfile:\n\nA balanced approach to betting, with high average wagers and withdrawals.\nActive both during the day and at night.\n\nPossible Markers of Harm:\n\nHigh withdrawal amounts might indicate dependency on winnings for daily expenses.\nConstant activity during both day and night may indicate extended gambling sessions, leading to potential burnout or overspending.\n\n\nCluster 4 (Frequent Bettors)\n\nProfile:\n\nRegular bettors with the highest number of wagers.\nTheir activity is prominent during the day.\n\nPossible Markers of Harm:\n\nThe frequency of their bets might indicate compulsive betting behaviour.\nReliance on consistent withdrawals may point to financial dependency on gambling.\n\n\nCluster 5 (Night Owls)\n\nProfile:\n\nPredominantly active during nighttime with moderate wager amounts.\n\nPossible Markers of Harm:\n\nBetting predominantly at night may point towards using gambling as an escape mechanism from daily stresses or potential insomnia issues.\n\n\nCluster 6 (Daytime Weekenders)\n\nProfile:\n\nActive during the day and weekends with good average wager and withdrawal amounts.\n\nPossible Markers of Harm:\n\nRegular betting during work breaks might indicate prioritizing gambling over work or other commitments.\nConsistent weekend activity may show a lack of balance between leisure and betting."
  },
  {
    "objectID": "R-ex/R-Ex9/R_Ex9.html",
    "href": "R-ex/R-Ex9/R_Ex9.html",
    "title": "Decoding Chaos - Analysing Armed conflicts in Myammar",
    "section": "",
    "text": "Overview of Project\n\n\n\nWebsite\nhttps://decoding-chaos.netlify.app/\n\n\nShiny app user guide\nhttps://decoding-chaos.netlify.app/RShiny/UserGuide.pdf\n\n\n\n\n Back to top"
  },
  {
    "objectID": "R-ex/R-Ex7/R_Ex7.html",
    "href": "R-ex/R-Ex7/R_Ex7.html",
    "title": "Geospatial Analysis2 - Emerging Hot Spot Analysis",
    "section": "",
    "text": "In this page, I will be exploring the codes for the plots in our Geospatial Analysis module of our Shiny Application. Specifically, I will be plotting the Emerging Hot Spot Map."
  },
  {
    "objectID": "R-ex/R-Ex7/R_Ex7.html#data-wrangle-for-quarterly-data",
    "href": "R-ex/R-Ex7/R_Ex7.html#data-wrangle-for-quarterly-data",
    "title": "Geospatial Analysis2 - Emerging Hot Spot Analysis",
    "section": "Data Wrangle for quarterly data",
    "text": "Data Wrangle for quarterly data\nAs per project requirements, we will sync the time frame for this analysis to be the same as our previous LISA analysis. Therefore, we will set up the data set to be for 2021-2023, and in quarterly periods\nI won’t repeat the data prep steps again, as this has already been done in previous prototype page. I will read in the previously prepared quarterly data for 2021-2023 instead.\n\nEvents_2 &lt;- read_csv(\"data/df1_complete.csv\")\n\nSince this data set has been filled up for missing values, using tidyr::complete() , I can proceed to use the standard spacetime constructor ie spacetime()"
  },
  {
    "objectID": "R-ex/R-Ex7/R_Ex7.html#computing-gi-1",
    "href": "R-ex/R-Ex7/R_Ex7.html#computing-gi-1",
    "title": "Geospatial Analysis2 - Emerging Hot Spot Analysis",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\n#for Quarterly admin 2\ngi_stars3 &lt;- Quarterly_nb %&gt;% \n  group_by(quarter) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    Incidents, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)\n\n\ngi_stars3\n\n# A tibble: 960 × 15\n# Groups:   quarter [12]\n   quarter DT      Incidents nb    wt    gi_star cluster    e_gi  var_gi std_dev\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   20211 Hintha…         0 &lt;int&gt; &lt;dbl&gt;  -0.938 Low     0.00926 1.46e-4 -0.766 \n 2   20211 Labutta         0 &lt;int&gt; &lt;dbl&gt;  -0.645 Low     0.00598 1.91e-4 -0.432 \n 3   20211 Maubin          0 &lt;int&gt; &lt;dbl&gt;  -0.938 Low     0.00941 1.52e-4 -0.764 \n 4   20211 Myaung…         0 &lt;int&gt; &lt;dbl&gt;  -0.801 Low     0.00758 1.48e-4 -0.624 \n 5   20211 Pathein         0 &lt;int&gt; &lt;dbl&gt;  -0.801 Low     0.00844 1.61e-4 -0.665 \n 6   20211 Pyapon          0 &lt;int&gt; &lt;dbl&gt;  -0.801 Low     0.00890 1.96e-4 -0.636 \n 7   20211 Bago            1 &lt;int&gt; &lt;dbl&gt;   0.321 Low     0.0113  1.51e-4  0.450 \n 8   20211 Taungoo         8 &lt;int&gt; &lt;dbl&gt;   0.432 High    0.0217  1.42e-4 -0.320 \n 9   20211 Pyay            0 &lt;int&gt; &lt;dbl&gt;  -0.337 Low     0.00911 1.62e-4 -0.113 \n10   20211 Thayar…         0 &lt;int&gt; &lt;dbl&gt;  -0.270 Low     0.00925 1.75e-4 -0.0466\n# ℹ 950 more rows\n# ℹ 5 more variables: p_value &lt;dbl&gt;, p_sim &lt;dbl&gt;, p_folded_sim &lt;dbl&gt;,\n#   skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;"
  },
  {
    "objectID": "R-ex/R-Ex7/R_Ex7.html#performing-emerging-hotspot-analysis",
    "href": "R-ex/R-Ex7/R_Ex7.html#performing-emerging-hotspot-analysis",
    "title": "Geospatial Analysis2 - Emerging Hot Spot Analysis",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e quarterly_spt), and the quoted name of the variable of interest (i.e. Incidents) for .var argument.\nThe k argument is used to specify the number of time lags which is set to 1 by default.\nLastly, nsim map numbers of simulation to be performed.\n\nehsa3 &lt;- emerging_hotspot_analysis(\n  x = Quarterly_spt, \n  .var = \"Incidents\", \n  k = 1,\n  nsim = 99\n)\n\n\nprint(ehsa3)\n\n# A tibble: 80 × 4\n   location       tau p_value classification     \n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;              \n 1 Hinthada    0.152   0.537  no pattern detected\n 2 Labutta    -0.242   0.304  no pattern detected\n 3 Maubin     -0.364   0.115  sporadic hotspot   \n 4 Myaungmya  -0.364   0.115  no pattern detected\n 5 Pathein     0.545   0.0164 no pattern detected\n 6 Pyapon      0       1      sporadic coldspot  \n 7 Bago        0       1      no pattern detected\n 8 Taungoo     0.212   0.373  oscilating hotspot \n 9 Pyay        0.0303  0.945  sporadic hotspot   \n10 Thayarwady -0.152   0.537  sporadic coldspot  \n# ℹ 70 more rows\n\n\n\nVisualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.\nAdmin2 districts - quarterly\n\n#| fig-width: 12\n#| fig-height: 7\n#| column: body-outset-right\n\nggplot(data = ehsa3,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nVisualising EHSA\nIn this section, we will visualise the geographic distribution EHSA classes. However, before we can do so, we need to join (mmr_shp_mimu2 & ehsa3) together by using the code chunk below.\n\nmmr3_ehsa &lt;- mmr_shp_mimu_2 %&gt;%\n  left_join(ehsa3,\n            by = join_by(DT == location))\n\n\nprint(mmr3_ehsa)\n\nSimple feature collection with 80 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID          ST ST_PCODE         DT   DT_PCODE      DT_MMR PCode_V\n1         1  Ayeyarwady   MMR017   Hinthada MMR017D002    ဟင်္သာတခရိုင်     9.4\n2         2  Ayeyarwady   MMR017    Labutta MMR017D004    လပွတ္တာခရိုင်     9.4\n3         3  Ayeyarwady   MMR017     Maubin MMR017D005     မအူပင်ခရိုင်     9.4\n4         4  Ayeyarwady   MMR017  Myaungmya MMR017D003 မြောင်းမြခရိုင်     9.4\n5         5  Ayeyarwady   MMR017    Pathein MMR017D001      ပုသိမ်ခရိုင်     9.4\n6         6  Ayeyarwady   MMR017     Pyapon MMR017D006     ဖျာပုံခရိုင်     9.4\n7         7 Bago (East)   MMR007       Bago MMR007D001      ပဲခူးခရိုင်     9.4\n8         8 Bago (East)   MMR007    Taungoo MMR007D002    တောင်ငူခရိုင်     9.4\n9         9 Bago (West)   MMR008       Pyay MMR008D001      ပြည်ခရိုင်     9.4\n10       10 Bago (West)   MMR008 Thayarwady MMR008D002   သာယာဝတီခရိုင်     9.4\n           tau   p_value      classification                       geometry\n1   0.15151514 0.5371338 no pattern detected MULTIPOLYGON (((95.12637 18...\n2  -0.24242422 0.3036732 no pattern detected MULTIPOLYGON (((95.04462 15...\n3  -0.36363631 0.1147567    sporadic hotspot MULTIPOLYGON (((95.38231 17...\n4  -0.36363631 0.1147567 no pattern detected MULTIPOLYGON (((94.6942 16....\n5   0.54545450 0.0163933 no pattern detected MULTIPOLYGON (((94.27572 15...\n6   0.00000000 1.0000000   sporadic coldspot MULTIPOLYGON (((95.20798 15...\n7   0.00000000 1.0000000 no pattern detected MULTIPOLYGON (((95.90674 18...\n8   0.21212119 0.3726914  oscilating hotspot MULTIPOLYGON (((96.17964 19...\n9   0.03030303 0.9453299    sporadic hotspot MULTIPOLYGON (((95.70458 19...\n10 -0.15151514 0.5371338   sporadic coldspot MULTIPOLYGON (((95.85173 18...\n\n\nNext, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.\n\n#| fig-width: 10\n#| fig-height: 7\n#| column: body-outset-right\n\nehsa_sig3 &lt;- mmr3_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntm_shape(mmr3_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig3) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "R-ex/R-Ex7/R_Ex7.html#references",
    "href": "R-ex/R-Ex7/R_Ex7.html#references",
    "title": "Geospatial Analysis2 - Emerging Hot Spot Analysis",
    "section": "References",
    "text": "References\nMain reference: Kam, T.S. (2024). Emerging Hot Spot Analysis: sfdep methods"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html",
    "href": "R-ex/R-Ex5/R_Ex5.html",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "",
    "text": "In this take-home exercise, we are required to select one of the modules of our proposed Shiny application (Group Project) and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for our Shiny application are supported in R CRAN,\nTo prepare and test that the specific R codes can run and returns the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications,\nTo select the appropriate Shiny UI components for exposing the parameters determined above, and\nTo include a section called UI design for the different components of the UIs for the proposed design."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#loading-r-packages",
    "href": "R-ex/R-Ex5/R_Ex5.html#loading-r-packages",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.2.1 Loading R packages",
    "text": "4.2.1 Loading R packages\nThe below R packages will be used in this exercise and for the Shiny application\n\npacman::p_load(sf, tidyverse, tmap, dplyr,\n               spatstat, spdep,\n               lubridate, leaflet,\n               plotly, DT, viridis,\n               ggplot2, sfdep)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#importing-and-loading-the-acled-data",
    "href": "R-ex/R-Ex5/R_Ex5.html#importing-and-loading-the-acled-data",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.2.2 Importing and loading the ACLED data",
    "text": "4.2.2 Importing and loading the ACLED data\nCountry specific data from the Armed Conflict Location & Event Data Project (ACLED) can be downloaded at https://acleddata.com/data-export-tool/\nLoading the ACLED data set for Myanmar.\n\nACLED_MMR &lt;- read_csv(\"data/MMR.csv\")"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#downloading-and-loading-the-shape-files-for-country",
    "href": "R-ex/R-Ex5/R_Ex5.html#downloading-and-loading-the-shape-files-for-country",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.2.3 Downloading and loading the shape files for country",
    "text": "4.2.3 Downloading and loading the shape files for country\nShape files were downloaded from the Myanmmar Information Management Unit (MIMU) website.\nI chose this source over GADM and GeoBoundaries due to its updated administrative region information and map levels.\n\n\n\n\n\n\nNote - Data Quality Issues\n\n\n\nACLED captures event data from national, sub-national and other media sources, and populates event locations based on the last known information.\n\nHowever, some names of administrative areas were found to have changed; either disaggregated into new administrative areas or previously active but now defunct. Some administrative areas were also aggregated into higher administrative areas.\nAs part of our data cleaning and preparation process, I had to identify discrepancies in both admin1 & 2 (administrative levels) and re-name some administrative areas to sync with the downloaded shape files from MIMU."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#loading-admin-1-2-administrative-regionarea-shape-files",
    "href": "R-ex/R-Ex5/R_Ex5.html#loading-admin-1-2-administrative-regionarea-shape-files",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.3.1 Loading Admin 1 & 2 (administrative region/area) shape files",
    "text": "4.3.1 Loading Admin 1 & 2 (administrative region/area) shape files\n\nmmr_shp_mimu_1 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda2_adm1_250k_mimu_1\")\n\nReading layer `mmr_polbnda2_adm1_250k_mimu_1' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex5\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 18 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex5\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nclass(mmr_shp_mimu_2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nThe Shape file for admin2 level map, is an SF object, with geometry type: Multipolygon.\n\nst_geometry(mmr_shp_mimu_2)\n\nGeometry set for 80 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\n\nst_crs(mmr_shp_mimu_2)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]]]\n\n\nNext, I will check the unique district names in this shape file (admin2)\n\nunique_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\nunique_regions_mimu2\n\n [1] \"Hinthada\"                        \"Labutta\"                        \n [3] \"Maubin\"                          \"Myaungmya\"                      \n [5] \"Pathein\"                         \"Pyapon\"                         \n [7] \"Bago\"                            \"Taungoo\"                        \n [9] \"Pyay\"                            \"Thayarwady\"                     \n[11] \"Falam\"                           \"Hakha\"                          \n[13] \"Matupi\"                          \"Mindat\"                         \n[15] \"Bhamo\"                           \"Mohnyin\"                        \n[17] \"Myitkyina\"                       \"Puta-O\"                         \n[19] \"Bawlake\"                         \"Loikaw\"                         \n[21] \"Hpa-An\"                          \"Hpapun\"                         \n[23] \"Kawkareik\"                       \"Myawaddy\"                       \n[25] \"Gangaw\"                          \"Magway\"                         \n[27] \"Minbu\"                           \"Pakokku\"                        \n[29] \"Thayet\"                          \"Kyaukse\"                        \n[31] \"Maungdaw\"                        \"Mrauk-U\"                        \n[33] \"Sittwe\"                          \"Thandwe\"                        \n[35] \"Hkamti\"                          \"Kale\"                           \n[37] \"Kanbalu\"                         \"Katha\"                          \n[39] \"Kawlin\"                          \"Mawlaik\"                        \n[41] \"Monywa\"                          \"Naga Self-Administered Zone\"    \n[43] \"Sagaing\"                         \"Shwebo\"                         \n[45] \"Tamu\"                            \"Yinmarbin\"                      \n[47] \"Kengtung\"                        \"Monghsat\"                       \n[49] \"Tachileik\"                       \"Hopang\"                         \n[51] \"Kokang Self-Administered Zone\"   \"Kyaukme\"                        \n[53] \"Lashio\"                          \"Matman\"                         \n[55] \"Mongmit\"                         \"Muse\"                           \n[57] \"Pa Laung Self-Administered Zone\" \"Danu Self-Administered Zone\"    \n[59] \"Langkho\"                         \"Loilen\"                         \n[61] \"Pa-O Self-Administered Zone\"     \"Taunggyi\"                       \n[63] \"Dawei\"                           \"Kawthoung\"                      \n[65] \"Mandalay\"                        \"Meiktila\"                       \n[67] \"Myingyan\"                        \"Nyaung-U\"                       \n[69] \"Pyinoolwin\"                      \"Yamethin\"                       \n[71] \"Mawlamyine\"                      \"Thaton\"                         \n[73] \"Det Khi Na\"                      \"Oke Ta Ra\"                      \n[75] \"Kyaukpyu\"                        \"Myeik\"                          \n[77] \"Yangon (East)\"                   \"Yangon (North)\"                 \n[79] \"Yangon (South)\"                  \"Yangon (West)\"                  \n\n\nThere are 80 admin2 levels or districts in mmr_shp_mimu_2\nLets compare with our admin2 levels in our main dataset ACLED_MMR\n\nunique_acled_regions2 &lt;- unique(ACLED_MMR$admin2)\n\nunique_acled_regions2\n\n [1] \"Maungdaw\"                        \"Bago\"                           \n [3] \"Shwebo\"                          \"Kyaukme\"                        \n [5] \"Pyinoolwin\"                      \"Muse\"                           \n [7] \"Sittwe\"                          \"Yinmarbin\"                      \n [9] \"Thaton\"                          \"Yangon-North\"                   \n[11] \"Pa-O Self-Administered Zone\"     \"Hpapun\"                         \n[13] \"Kyaukpyu\"                        \"Yangon-West\"                    \n[15] \"Mongmit\"                         \"Bhamo\"                          \n[17] \"Mrauk-U\"                         \"Yangon-East\"                    \n[19] \"Yangon-South\"                    \"Monywa\"                         \n[21] \"Gangaw\"                          \"Pathein\"                        \n[23] \"Katha\"                           \"Taungoo\"                        \n[25] \"Kanbalu\"                         \"Lashio\"                         \n[27] \"Mawlamyine\"                      \"Myitkyina\"                      \n[29] \"Kawkareik\"                       \"Loilen\"                         \n[31] \"Mandalay\"                        \"Kawlin\"                         \n[33] \"Kyaukse\"                         \"Magway\"                         \n[35] \"Meiktila\"                        \"Pakokku\"                        \n[37] \"Taunggyi\"                        \"Tamu\"                           \n[39] \"Nay Pyi Taw\"                     \"Mohnyin\"                        \n[41] \"Kale\"                            \"Det Khi Na\"                     \n[43] \"Myingyan\"                        \"Loikaw\"                         \n[45] \"Matupi\"                          \"Pyay\"                           \n[47] \"Sagaing\"                         \"Myeik\"                          \n[49] \"Dawei\"                           \"Thayarwady\"                     \n[51] \"Thandwe\"                         \"Mawlaik\"                        \n[53] \"Bawlake\"                         \"Pyapon\"                         \n[55] \"Hinthada\"                        \"Thayet\"                         \n[57] \"Pa Laung Self-Administered Zone\" \"Mindat\"                         \n[59] \"Hkamti\"                          \"Kokang Self-Administered Zone\"  \n[61] \"Hpa-An\"                          \"Danu Self-Administered Zone\"    \n[63] \"Myawaddy\"                        \"Maubin\"                         \n[65] \"Hakha\"                           \"Falam\"                          \n[67] \"Minbu\"                           \"Monghsat\"                       \n[69] \"Puta-O\"                          \"Hopang\"                         \n[71] \"Nyaung-U\"                        \"Kawthoung\"                      \n[73] \"Yamethin\"                        \"Yangon\"                         \n[75] \"Myaungmya\"                       \"Mong Pawk (Wa SAD)\"             \n[77] \"Oke Ta Ra\"                       \"Matman\"                         \n[79] \"Kengtung\"                        \"Naga Self-Administered Zone\"    \n[81] \"Labutta\"                         \"Langkho\"                        \n[83] \"Tachileik\"                      \n\n\nI will write a simple function below to identify the discrepancies between the shape file and our state/district names in our main dataset.\n\n# Find the unique region names that are in 'unique_acled_regions2' but not in 'unique_regions_mimu2'\n\nmismatched_admin2 &lt;- setdiff(unique_acled_regions2, unique_regions_mimu2)\n\nif (length(mismatched_admin2) &gt; 0) {\n  print(\"The following region names from 'acled_mmr' do not match any in 'mimu2':\")\n  print(mismatched_admin2)\n} else {\n  print(\"All unique region names in 'acled_mmr' match the unique region names in 'mimu2.'\")\n}\n\n[1] \"The following region names from 'acled_mmr' do not match any in 'mimu2':\"\n[1] \"Yangon-North\"       \"Yangon-West\"        \"Yangon-East\"       \n[4] \"Yangon-South\"       \"Nay Pyi Taw\"        \"Yangon\"            \n[7] \"Mong Pawk (Wa SAD)\"\n\n\nLets harmonize the names in both data files. I will re-save it to a new data set called ACLED_MMR_1\nFixing our admin 1 names.\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\nFixing our admin 2 names.\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\nChecking if our changes are successful.\n\n# Get unique admin 2 district names from 'ACLED_MMR_1'\nunique_acled_regions2 &lt;- unique(ACLED_MMR_1$admin2)\n\n# Get unique district names from 'mmr_shp_mimu_2'\nunique_map_regions_mimu2 &lt;- unique(mmr_shp_mimu_2$DT)\n\n# Find the unique district names that are in 'unique_acled_regions2' but not in 'unique_map_regions_mimu2'\n\nmismatched_regions2 &lt;- setdiff(unique_acled_regions2, unique_map_regions_mimu2)\n\nif (length(mismatched_regions2) &gt; 0) {\n  print(\"The following district names from 'acled_mmr_1' do not match any in 'mmr_shp_mimu_2':\")\n  print(mismatched_regions2)\n} else {\n  print(\"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\")\n}\n\n[1] \"All unique district names in 'acled_mmr_1' match the unique district names in 'mmmr_shp_mimu_2.'\"\n\n\nLets do a sample plot to see how our country map looks like at the admin2 (districts) level.\n\nplot(mmr_shp_mimu_2)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#data-wrangling",
    "href": "R-ex/R-Ex5/R_Ex5.html#data-wrangling",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.3.2 Data Wrangling",
    "text": "4.3.2 Data Wrangling\nFor the purposes of plotting choropleth maps, I will first create attributes subsets to summarise the number of incidents and fatalities, grouped by year, admin region, event type and sub event type.\n\nData2 &lt;- ACLED_MMR_1 %&gt;%\n    group_by(year, admin2, event_type, sub_event_type) %&gt;%\n    summarise(Incidents = n(),\n              Fatalities = sum(fatalities, na.rm = TRUE)) %&gt;%\n              \n    ungroup()\n\nChecking the total sum of incidents and fatalities\n\ntotal_incidents2 &lt;- sum(Data2$Incidents)\ntotal_fatalities2 &lt;- sum(Data2$Fatalities)\n\ntotal_incidents2\n\n[1] 57198\n\ntotal_fatalities2\n\n[1] 57593\n\n\nNext, I will do a spatial join between my shape files and attribute files\n\nACLED_MMR_admin2 &lt;- left_join(mmr_shp_mimu_2, Data2,\n                            by = c(\"DT\" = \"admin2\"))\n\nRemoving the variables I don’t require.\n\nACLED_MMR_admin2 &lt;- ACLED_MMR_admin2 %&gt;%\n                      select(-OBJECTID, -ST, -ST_PCODE)\n\n\nclass(ACLED_MMR_admin2)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNext, I will just double check that total sum of incidents and fatalities in our SF files are correct as per our original datasets.\n\ntotal_incidents_check &lt;- sum(ACLED_MMR_admin2$Incidents)\ntotal_fatalities_check &lt;- sum(ACLED_MMR_admin2$Fatalities)\n\ntotal_incidents_check \n\n[1] 57198\n\ntotal_fatalities_check\n\n[1] 57593"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "href": "R-ex/R-Ex5/R_Ex5.html#choropleth-map-of-incidents-fatalities-by-admin2-level-by-district",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.4.1 Choropleth map of Incidents & Fatalities by Admin2 level (by District)",
    "text": "4.4.1 Choropleth map of Incidents & Fatalities by Admin2 level (by District)\nThe below codes will be used to create the choropleth maps.\n\nFatalities in Battles in 2023, by Districts (Quantile)Incidents of Violence against civilians in 2021, by Districts (Equal)\n\n\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Fatalities\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\nACLED_MMR_admin2 %&gt;%\n  filter(year == 2021, event_type == \"Violence against civilians\") %&gt;%\n  tm_shape() +\n  tm_fill(\"Incidents\",\n          n = 5,\n          style = \"equal\",\n          palette = \"Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nAdding Interactivity by using tmap_leaftlet()\n\ndata_filtered &lt;- ACLED_MMR_admin2 %&gt;%\n  filter(year == 2022, event_type == \"Battles\")\n\ntm_map &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(data_filtered) +\n  tm_fill(col = \"Incidents\", n = 5, style = \"equal\", palette = \"Reds\", title = \"Incidents\") +\n  tm_borders(alpha = 0.5)\n\ntmap_leaflet(tm_map)\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nFrom the codes above, below are the variables we can expose as user inputs:-\n\nYear\nEvent type (Battles, Violence against civilians, protests, riots, explosions/remote violence)\nCount type: number of Incidents or Fatalities\nData classification type: eg quantile, equal, jenks, kmeans, pretty etc"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#visualising-the-location-of-conflict-events",
    "href": "R-ex/R-Ex5/R_Ex5.html#visualising-the-location-of-conflict-events",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.5.1 Visualising the location of conflict events",
    "text": "4.5.1 Visualising the location of conflict events\nUsing the leaflet package, I will use the Geometry points from our SF data sets to plot the points of event types in the maps.\nIn this case, I will use admin 1 level regions, to achieve a better aesthetics for users.\nThis is because, visually dividing the country map into more smaller districts (admin2) would likely make the map look “too busy”.\n\nBattles from 2010 to presentViolence against civilians from 2010 to presentProtests from 2010 to present\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Battles) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Battles&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Violence_CV) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Violence on Civillians&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\n                   fillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\nscaleFactor &lt;- 2  \n\nleaflet(Protests) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(data = mmr_shp_mimu_1, color = \"#444444\", weight = 1, fillOpacity = 0.5) %&gt;% # Adding borders\n  \n  addCircleMarkers(popup = ~paste(\"Event: Protests&lt;br&gt;State/Region:\", admin1, \n                                  \"&lt;br&gt;Actor1:\", actor1, \"&lt;br&gt;Actor2:\", actor2,\n                                  \"&lt;br&gt;Year:\", year, \"&lt;br&gt;Fatalities:\", fatalities),\n                   radius = ~sqrt(fatalities) * scaleFactor,\nfillColor = \"red\", fillOpacity = 0.4, color = \"#FFFFFF\", weight = 1) %&gt;% \n  setView(lng = 96.1603, lat = 19.745, zoom = 6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nThe plots above are sufficiently interactive as users can hover to any “circle” to get more information on the event and location.\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#filtering-the-event-and-year-event-type-battles-in-2023",
    "href": "R-ex/R-Ex5/R_Ex5.html#filtering-the-event-and-year-event-type-battles-in-2023",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.7.1 Filtering the Event and Year (Event type = Battles, in 2023)",
    "text": "4.7.1 Filtering the Event and Year (Event type = Battles, in 2023)\nThe below subset will serve as our reference data subset for our subsequent codes.\n\nBattles_2023 &lt;- Events_admin2 %&gt;%\n  filter(year == 2023, event_type == \"Battles\")"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-contiguity-spatial-weights",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-contiguity-spatial-weights",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.7.2 Computing Contiguity Spatial Weights",
    "text": "4.7.2 Computing Contiguity Spatial Weights\nBefore we can compute any spatial statistics, we need to construct spatial weights of the study area.\nThe spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. admin2) in the study area (Myanmar).\nIn the code below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\nBy default this function will return a list of first order neighbours using the Queen criteria.\nHowever, we can also pass a “queen” argument that takes TRUE or FALSE as options.\n\nwm_q &lt;- poly2nb(Battles_2023, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 356 \nPercentage nonzero weights: 6.501096 \nAverage number of links: 4.810811 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8 10 \n 3  8 11 11 15  9  9  6  2 \n3 least connected regions:\n2 16 58 with 1 link\n2 most connected regions:\n19 56 with 10 links\n\n\nThe summary report above shows that there are 74 area units for this subset (Battles occurring in 2023).\n\nThere are 2 most connected area units with 10 neighbours, and there are 3 area units with only 1 neighbour."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#row-standardised-weights-matrix",
    "href": "R-ex/R-Ex5/R_Ex5.html#row-standardised-weights-matrix",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.7.3 Row-standardised weights matrix",
    "text": "4.7.3 Row-standardised weights matrix\nNext, we assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring admin2 (district) and then summing the weighted income values.\nThis has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons and thus potentially over or under estimating the true nature of the spatial autocorrelation in the data.\nHowever, for this example, I will stick with the style=“W” option for simplicity’s sake. Other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 356 \nPercentage nonzero weights: 6.501096 \nAverage number of links: 4.810811 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 74 5476 74 36.81702 310.0455"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-local-morans-i",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-local-morans-i",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.1 Computing Local Moran’s I",
    "text": "4.8.1 Computing Local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a list object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips &lt;- order(Battles_2023$DT)\nlocalMI &lt;- localmoran(Battles_2023$Incidents, rswm_q)\nhead(localMI)\n\n           Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.46274887 -9.006432e-03 0.1247560902  1.3356292      0.1816705\n2  0.71317847 -1.016877e-02 0.7448368248  0.8381394      0.4019524\n3  0.69218038 -9.386041e-03 0.3392457987  1.2045132      0.2283913\n4  0.68518101 -9.386041e-03 0.3392457987  1.1924960      0.2330668\n5  0.00627746 -1.483579e-05 0.0001702656  0.4822206      0.6296493\n6 -0.23004674 -4.814215e-03 0.0464274459 -1.0453066      0.2958813\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code below lists the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=Battles_2023$DT[fips]),\n  check.names=FALSE)\n\n                                         Ii        E.Ii      Var.Ii        Z.Ii\nBago                             6.2775e-03 -1.4836e-05  1.7027e-04  4.8222e-01\nBawlake                         -1.7909e-02 -4.6941e-04  1.1252e-02 -1.6441e-01\nBhamo                            2.4621e-01 -1.7367e-03  1.9897e-02  1.7577e+00\nDanu Self-Administered Zone      3.3657e-01 -8.2707e-03  1.9670e-01  7.7752e-01\nDawei                            9.4559e-01 -1.4130e-02  5.0825e-01  1.3462e+00\nDet Khi Na                       2.3024e-01 -6.5686e-03  6.3234e-02  9.4170e-01\nFalam                           -7.5203e-03 -2.4381e-03  5.8326e-02 -2.1044e-02\nGangaw                           6.2609e-01 -6.9289e-03  6.6679e-02  2.4514e+00\nHakha                           -1.1347e-02 -6.1003e-05  1.0815e-03 -3.4318e-01\nHinthada                         4.6275e-01 -9.0064e-03  1.2476e-01  1.3356e+00\nHkamti                          -5.3716e-02 -3.0598e-03  3.5009e-02 -2.7074e-01\nHopang                          -2.3647e-01 -9.7735e-03  3.5311e-01 -3.8150e-01\nHpa-An                          -1.3231e-01 -3.0598e-03  1.9751e-02 -9.1968e-01\nHpapun                          -4.1969e-02 -4.2526e-03  5.9189e-02 -1.5503e-01\nKale                             1.2532e-01 -7.7384e-04  6.4571e-03  1.5692e+00\nKanbalu                         -3.2905e-02 -3.4002e-05  3.9022e-04 -1.6640e+00\nKatha                           -8.8073e-03 -1.8682e-02  1.5309e-01  2.5238e-02\nKawkareik                        2.9062e-02 -1.3663e-02  3.2318e-01  7.5156e-02\nKawlin                          -7.2541e-02 -1.4063e-03  3.3678e-02 -3.8762e-01\nKawthoung                       -6.7102e-01 -3.2826e-03  2.4212e-01 -1.3570e+00\nKokang Self-Administered Zone    9.6449e-04 -1.1447e-08  2.7452e-07  1.8408e+00\nKyaukme                         -6.9939e-02 -9.0471e-03  8.6877e-02 -2.0659e-01\nKyaukpyu                         4.7922e-01 -6.8933e-03  1.2137e-01  1.3953e+00\nKyaukse                         -1.4472e-01 -9.0064e-03  7.4533e-02 -4.9713e-01\nLabutta                          7.1318e-01 -1.0169e-02  7.4484e-01  8.3814e-01\nLangkho                         -9.6769e-03 -8.6347e-03  2.0528e-01 -2.3004e-03\nLashio                           2.0219e-01 -4.2805e-03  4.8917e-02  9.3352e-01\nLoikaw                          -4.7652e-01 -2.3869e-02  3.2568e-01 -7.9318e-01\nLoilen                          -2.8804e-02 -5.6413e-03  7.8408e-02 -8.2718e-02\nMagway                           9.4077e-02 -5.9426e-03  4.9330e-02  4.5033e-01\nMandalay                        -2.2955e-01 -3.0598e-03  7.3153e-02 -8.3742e-01\nMatupi                          -1.7986e-02 -3.2117e-04  4.4878e-03 -2.6370e-01\nMaungdaw                         1.2575e-01 -2.6375e-03  6.3084e-02  5.1117e-01\nMawlaik                         -3.2557e-02 -2.6375e-03  3.0190e-02 -1.7220e-01\nMawlamyine                       2.4766e-01 -3.3072e-03  5.8440e-02  1.0381e+00\nMeiktila                         3.8976e-01 -8.2707e-03  7.9484e-02  1.4118e+00\nMinbu                           -1.9321e-01 -6.2516e-03  6.0203e-02 -7.6195e-01\nMindat                           2.2211e-02 -8.7518e-04  1.5503e-02  1.8541e-01\nMohnyin                          1.1002e-01 -8.8788e-04  1.5727e-02  8.8440e-01\nMongmit                         -4.9751e-01 -8.6347e-03  1.1965e-01 -1.4133e+00\nMonywa                           3.7540e+00 -3.3925e-02  5.8106e-01  4.9692e+00\nMrauk-U                          2.1574e-01 -3.9983e-03  4.5705e-02  1.0278e+00\nMuse                             2.6578e-01 -1.6313e-01  2.4204e+00  2.7569e-01\nMyawaddy                         1.8035e-02 -6.4391e-05  2.3492e-03  3.7341e-01\nMyeik                            3.6057e-01 -2.5739e-02  9.1496e-01  4.0386e-01\nMyingyan                         4.3732e-02 -1.0008e-04  1.3987e-03  1.1720e+00\nMyitkyina                       -1.9413e-01 -6.2855e-03  8.7306e-02 -6.3572e-01\nNaga Self-Administered Zone     -8.8212e-02 -1.0169e-02  3.6725e-01 -1.2878e-01\nNyaung-U                        -5.4115e-01 -8.6347e-03  1.5176e-01 -1.3669e+00\nOke Ta Ra                        5.7519e-01 -9.0064e-03  1.5824e-01  1.4686e+00\nPa-O Self-Administered Zone      1.9941e-01 -5.6413e-03  5.4359e-02  8.7950e-01\nPa Laung Self-Administered Zone -5.3309e-01 -5.0623e-03  7.0402e-02 -1.9901e+00\nPakokku                          1.9341e+00 -2.2766e-01  1.4683e+00  1.7840e+00\nPathein                          6.9218e-01 -9.3860e-03  3.3925e-01  1.2045e+00\nPuta-O                          -4.8052e-01 -6.8933e-03  5.0659e-01 -6.6543e-01\nPyapon                           6.8518e-01 -9.3860e-03  3.3925e-01  1.1925e+00\nPyay                             1.0545e-01 -1.2618e-03  1.7615e-02  8.0407e-01\nPyinoolwin                       4.6151e-01 -2.7025e-02  2.1958e-01  1.0426e+00\nSagaing                          9.5824e-01 -1.0212e-02  9.7948e-02  3.0944e+00\nShwebo                           2.1412e+00 -5.0075e-02  5.4593e-01  2.9657e+00\nSittwe                           2.3135e-01 -3.0598e-03  1.1130e-01  7.0266e-01\nTamu                             3.2174e-02 -1.8902e-04  3.3506e-03  5.5911e-01\nTaunggyi                        -1.0168e-01 -1.1395e-03  7.3697e-03 -1.1711e+00\nTaungoo                         -2.3005e-01 -4.8142e-03  4.6427e-02 -1.0453e+00\nThandwe                          5.6456e-01 -1.0169e-02  1.4069e-01  1.5322e+00\nThaton                          -6.7767e-02 -3.0835e-03  5.4499e-02 -2.7708e-01\nThayarwady                       9.0973e-03 -1.4836e-05  2.0737e-04  6.3277e-01\nThayet                           2.9530e-01 -5.3479e-03  5.1546e-02  1.3242e+00\nYamethin                         4.3920e-01 -9.7735e-03  1.3528e-01  1.2207e+00\nYangon (East)                    6.1100e-01 -7.5664e-03  1.8008e-01  1.4576e+00\nYangon (North)                   4.4954e-01 -9.3860e-03  1.0671e-01  1.4049e+00\nYangon (South)                   5.2023e-01 -8.6347e-03  1.1965e-01  1.5289e+00\nYangon (West)                    6.6585e-01 -9.7735e-03  2.3209e-01  1.4024e+00\nYinmarbin                        4.5788e+00 -9.9115e-02  1.2481e+00  4.1872e+00\n                                Pr.z....E.Ii..\nBago                                    0.6296\nBawlake                                 0.8694\nBhamo                                   0.0788\nDanu Self-Administered Zone             0.4369\nDawei                                   0.1782\nDet Khi Na                              0.3463\nFalam                                   0.9832\nGangaw                                  0.0142\nHakha                                   0.7315\nHinthada                                0.1817\nHkamti                                  0.7866\nHopang                                  0.7028\nHpa-An                                  0.3577\nHpapun                                  0.8768\nKale                                    0.1166\nKanbalu                                 0.0961\nKatha                                   0.9799\nKawkareik                               0.9401\nKawlin                                  0.6983\nKawthoung                               0.1748\nKokang Self-Administered Zone           0.0656\nKyaukme                                 0.8363\nKyaukpyu                                0.1629\nKyaukse                                 0.6191\nLabutta                                 0.4020\nLangkho                                 0.9982\nLashio                                  0.3506\nLoikaw                                  0.4277\nLoilen                                  0.9341\nMagway                                  0.6525\nMandalay                                0.4024\nMatupi                                  0.7920\nMaungdaw                                0.6092\nMawlaik                                 0.8633\nMawlamyine                              0.2992\nMeiktila                                0.1580\nMinbu                                   0.4461\nMindat                                  0.8529\nMohnyin                                 0.3765\nMongmit                                 0.1576\nMonywa                                  0.0000\nMrauk-U                                 0.3040\nMuse                                    0.7828\nMyawaddy                                0.7088\nMyeik                                   0.6863\nMyingyan                                0.2412\nMyitkyina                               0.5250\nNaga Self-Administered Zone             0.8975\nNyaung-U                                0.1716\nOke Ta Ra                               0.1419\nPa-O Self-Administered Zone             0.3791\nPa Laung Self-Administered Zone         0.0466\nPakokku                                 0.0744\nPathein                                 0.2284\nPuta-O                                  0.5058\nPyapon                                  0.2331\nPyay                                    0.4214\nPyinoolwin                              0.2972\nSagaing                                 0.0020\nShwebo                                  0.0030\nSittwe                                  0.4823\nTamu                                    0.5761\nTaunggyi                                0.2415\nTaungoo                                 0.2959\nThandwe                                 0.1255\nThaton                                  0.7817\nThayarwady                              0.5269\nThayet                                  0.1854\nYamethin                                0.2222\nYangon (East)                           0.1449\nYangon (North)                          0.1601\nYangon (South)                          0.1263\nYangon (West)                           0.1608\nYinmarbin                               0.0000"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#mapping-the-local-morans-i",
    "href": "R-ex/R-Ex5/R_Ex5.html#mapping-the-local-morans-i",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.2 Mapping the Local Moran’s I",
    "text": "4.8.2 Mapping the Local Moran’s I\nBefore mapping the local Moran’s I map, I will need to append the local Moran’s I dataframe (i.e. localMI) onto the Battles_2023’s SF DataFrame.\n\nBattles_2023.localMI &lt;- cbind(Battles_2023,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nBattles_2023.localMI\n\nSimple feature collection with 74 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DT   DT_PCODE    DT_MMR PCode_V year event_type Incidents Fatalities\n1    Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023    Battles         4          3\n2     Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023    Battles         1          1\n3     Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023    Battles         3          1\n4      Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023    Battles         3          2\n5        Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023    Battles        50        270\n6     Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023    Battles        87        493\n7        Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023    Battles        34         59\n8  Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023    Battles        50         93\n9       Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023    Battles        27         89\n10      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023    Battles        48        210\n             Ii          E.Ii       Var.Ii        Z.Ii     Pr.Ii\n1   0.462748867 -9.006432e-03 0.1247560902  1.33562921 0.1816705\n2   0.713178468 -1.016877e-02 0.7448368248  0.83813940 0.4019524\n3   0.692180375 -9.386041e-03 0.3392457987  1.20451317 0.2283913\n4   0.685181011 -9.386041e-03 0.3392457987  1.19249602 0.2330668\n5   0.006277460 -1.483579e-05 0.0001702656  0.48222056 0.6296493\n6  -0.230046740 -4.814215e-03 0.0464274459 -1.04530664 0.2958813\n7   0.105454317 -1.261774e-03 0.0176145487  0.80407054 0.4213562\n8   0.009097304 -1.483579e-05 0.0002073682  0.63277490 0.5268807\n9  -0.007520292 -2.438086e-03 0.0583263538 -0.02104359 0.9832109\n10 -0.011346560 -6.100301e-05 0.0010814666 -0.34317564 0.7314663\n                         geometry\n1  MULTIPOLYGON (((95.12637 18...\n2  MULTIPOLYGON (((95.04462 15...\n3  MULTIPOLYGON (((94.27572 15...\n4  MULTIPOLYGON (((95.20798 15...\n5  MULTIPOLYGON (((95.90674 18...\n6  MULTIPOLYGON (((96.17964 19...\n7  MULTIPOLYGON (((95.70458 19...\n8  MULTIPOLYGON (((95.85173 18...\n9  MULTIPOLYGON (((93.36931 24...\n10 MULTIPOLYGON (((93.35213 23..."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-values",
    "href": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-values",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.3 Mapping Local Moran’s I values",
    "text": "4.8.3 Mapping Local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code below.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-p-values",
    "href": "R-ex/R-Ex5/R_Ex5.html#mapping-local-morans-i-p-values",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.4 Mapping Local Moran’s I p-values",
    "text": "4.8.4 Mapping Local Moran’s I p-values\nThe choropleth above shows there is evidence for both positive and negative Ii values. However, we will also need to consider the p-values for each of these values.\nThe code below produces a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#data-table-for-the-morans-i-values",
    "href": "R-ex/R-Ex5/R_Ex5.html#data-table-for-the-morans-i-values",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.8.6 Data table for the Moran’s I values",
    "text": "4.8.6 Data table for the Moran’s I values\nFor the sake of readability, it may also be a good idea to add a data table of the values, for users to make sense of both maps.\nThe below code will be used to generate the data table.\n\ndatatable(Battles_2023.localMI)"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#plotting-the-moran-scatter-plot",
    "href": "R-ex/R-Ex5/R_Ex5.html#plotting-the-moran-scatter-plot",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.1 Plotting the Moran Scatter plot",
    "text": "4.10.1 Plotting the Moran Scatter plot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code below plots the Moran scatterplot of Battles in 2023 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(Battles_2023$Incidents, rswm_q,\n                  labels=as.character(Battles_2023$DT), \n                  xlab=\"Battles_2023\", \n                  ylab=\"Spatially Lagged Events,Year\")\n\n\n\n\n\n\n\n\nThe plot is split in 4 quadrants. The top right corner belongs to areas that have high incidents of events and are surrounded by other areas that have higher than the average level/number of battles This is the high-high locations.\n\n\n\n\n\n\nNote\n\n\n\nThe Moran scatterplot is divided into four areas, with each quadrant corresponding with one of four categories: (1) High-High (HH) in the top-right quadrant; (2) High-Low (HL) in the bottom right quadrant; (3) Low-High (LH) in the top-left quadrant; (4) Low- Low (LL) in the bottom left quadrant."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "R-ex/R-Ex5/R_Ex5.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.2 Plotting Moran scatterplot with standardised variable",
    "text": "4.10.2 Plotting Moran scatterplot with standardised variable\nFirst, I will use scale() to centre and scale the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centred) variable by their standard deviations.\n\nBattles_2023$Z.Incidents &lt;- scale(Battles_2023$Incidents) %&gt;% \n  as.vector \n\nThe as.vector() is added to the end is to make sure that the data type is a vector, that maps neatly into our dataframe.\nNext, we plot the Moran scatterplot again by using the code below.\n\nnci2 &lt;- moran.plot(Battles_2023$Z.Incidents, rswm_q,\n                   labels=as.character(Battles_2023$DT),\n                   xlab=\"z-Battles in 2023\", \n                   ylab=\"Spatially Lag z-Battles in 2023\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1) High-High (HH): indicates high spatial correlation where incidents of Battles are clustered closely together.\n2) High-Low (HL): where areas of high frequency of incidents of Battles occurred are located next to areas where there is low frequency of incidents of Battles occurred.\n3) Low-High (LH): these are areas of low frequency of incidents where Battles occurred that are located next to areas where high frequency of Battles.\n4) Low-Low (LL): these are clusters of low frequency of incidents of Battles occurred."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#preparing-lisa-map-classes",
    "href": "R-ex/R-Ex5/R_Ex5.html#preparing-lisa-map-classes",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.3 Preparing LISA map classes",
    "text": "4.10.3 Preparing LISA map classes\nAccording to Anselin (1995), LISA can be used to locate “hot spots” or local spatial clusters where the occurrence of Event types is statistically significant.\nIn addition to the four categories described in the Moran Scatterplot, the LISA analysis includes an additional category: (5) Insignificant: where there are no spatial autocorrelation or clusters where event types have occurred.\nThe code below shows the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. Incidents) and centre the spatially lagged variable around its mean.\n\nBattles_2023$lag_Incidents &lt;- lag.listw(rswm_q, Battles_2023$Incidents)\nDV &lt;- Battles_2023$lag_Incidents - mean(Battles_2023$lag_Incidents)     \n\nThis is followed by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThe code below define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, we place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#plotting-lisa-map",
    "href": "R-ex/R-Ex5/R_Ex5.html#plotting-lisa-map",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.10.4 Plotting LISA Map",
    "text": "4.10.4 Plotting LISA Map\nThe below code is used to create the LISA map.\n\nBattles_2023.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"lightyellow\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n\ntm_shape(Battles_2023.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user input: Year and Event type."
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#deriving-distance-based-weight-matrix",
    "href": "R-ex/R-Ex5/R_Ex5.html#deriving-distance-based-weight-matrix",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.1 Deriving distance-based weight matrix",
    "text": "4.12.1 Deriving distance-based weight matrix\nWhist the spatial autocorrelation in the previous section considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n4.12.1.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph.\nWe need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length.\nOur input vector will be the geometry column of Battles_2023 dataset. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid() function over the geometry column of our Battles_2023 dataset and access the longitude value through double bracket notation [[]] and 1.\nThis allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[1]])\n\n\nclass(longitude)\n\n[1] \"numeric\"\n\n\n\nlongitude\n\n [1] 95.19035 94.99369 94.74008 95.53705 96.58767 96.34709 95.30186 95.75119\n [9] 93.71488 93.56355 93.22544 93.81953 97.16218 96.49805 97.41891 97.78205\n[17] 97.42880 97.33311 97.45091 97.36866 98.22657 98.51991 94.18202 95.31595\n[25] 94.45732 94.73063 95.05170 96.22693 92.48276 93.35447 92.90601 94.50024\n[33] 95.47237 94.40944 95.49546 96.05576 95.53962 94.74604 95.26376 95.66196\n[41] 95.64449 95.40740 94.38739 94.71565 98.96838 98.65407 97.13646 98.19561\n[49] 96.66042 98.01229 97.21860 96.52970 98.02872 97.92760 97.03704 96.92822\n[57] 98.47076 98.77515 96.16459 95.97341 95.54920 95.15564 96.25009 96.06338\n[65] 97.84218 97.26005 96.16788 96.13921 93.92534 98.93770 96.24776 96.03014\n[73] 96.28511 96.13431\n\n\nWe do the same for latitude by accessing the second value per centroid with [[2]].\n\nlatitude &lt;- map_dbl(Battles_2023$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\nclass(coords)\n\n[1] \"matrix\" \"array\" \n\n\n\ncoords\n\n      longitude latitude\n [1,]  95.19035 17.87515\n [2,]  94.99369 16.16236\n [3,]  94.74008 16.86214\n [4,]  95.53705 16.17363\n [5,]  96.58767 17.74287\n [6,]  96.34709 18.81260\n [7,]  95.30186 18.76076\n [8,]  95.75119 18.10024\n [9,]  93.71488 23.42199\n[10,]  93.56355 22.51244\n[11,]  93.22544 21.57539\n[12,]  93.81953 21.26333\n[13,]  97.16218 24.25814\n[14,]  96.49805 25.30215\n[15,]  97.41891 26.01108\n[16,]  97.78205 27.27547\n[17,]  97.42880 18.91417\n[18,]  97.33311 19.48546\n[19,]  97.45091 17.75720\n[20,]  97.36866 18.07659\n[21,]  98.22657 16.00305\n[22,]  98.51991 16.54942\n[23,]  94.18202 21.80335\n[24,]  95.31595 20.26548\n[25,]  94.45732 20.38303\n[26,]  94.73063 21.43135\n[27,]  95.05170 19.43717\n[28,]  96.22693 21.61640\n[29,]  92.48276 21.02760\n[30,]  93.35447 20.47696\n[31,]  92.90601 20.40055\n[32,]  94.50024 18.53159\n[33,]  95.47237 25.34589\n[34,]  94.40944 23.07978\n[35,]  95.49546 23.32423\n[36,]  96.05576 24.25502\n[37,]  95.53962 23.93661\n[38,]  94.74604 23.96919\n[39,]  95.26376 22.25730\n[40,]  95.66196 26.41610\n[41,]  95.64449 21.98775\n[42,]  95.40740 22.74644\n[43,]  94.38739 24.17969\n[44,]  94.71565 22.24865\n[45,]  98.96838 23.02589\n[46,]  98.65407 23.83364\n[47,]  97.13646 22.50271\n[48,]  98.19561 22.76825\n[49,]  96.66042 23.45728\n[50,]  98.01229 23.68574\n[51,]  97.21860 23.21080\n[52,]  96.52970 21.22388\n[53,]  98.02872 20.26507\n[54,]  97.92760 21.39398\n[55,]  97.03704 20.46234\n[56,]  96.92822 20.85826\n[57,]  98.47076 14.08664\n[58,]  98.77515 10.98880\n[59,]  96.16459 21.96431\n[60,]  95.97341 20.95960\n[61,]  95.54920 21.47201\n[62,]  95.15564 20.94652\n[63,]  96.25009 22.62753\n[64,]  96.06338 20.49193\n[65,]  97.84218 15.79499\n[66,]  97.26005 17.13186\n[67,]  96.16788 19.63929\n[68,]  96.13921 20.04539\n[69,]  93.92534 19.67628\n[70,]  98.93770 12.36708\n[71,]  96.24776 16.89759\n[72,]  96.03014 17.26904\n[73,]  96.28511 16.66221\n[74,]  96.13431 16.82926\n\n\n\n\n4.12.1.2 Determining the cut-off distance\nFor the fixed distance weights, first, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\n\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.26   49.33   66.03   71.79   82.19  196.85 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 196.85 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\nwm_d197 &lt;- dnearneigh(coords, 0, 197, longlat = TRUE)\nwm_d197\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm197_lw &lt;- nb2listw(wm_d197, style = 'B')\nsummary(wm197_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 828 \nPercentage nonzero weights: 15.12053 \nAverage number of links: 11.18919 \n2 disjoint connected subgraphs\nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 3  1  2  3  3  5  3  3  7  3  4  4  6  5  5  4  1  5  4  3 \n3 least connected regions:\n16 57 58 with 1 link\n3 most connected regions:\n39 42 62 with 20 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 828 1656 45160"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-adaptive-distance-weight-matrix",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-adaptive-distance-weight-matrix",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.2 Computing Adaptive distance weight matrix",
    "text": "4.12.2 Computing Adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours.\nHaving many neighbours smoothes the neighbour relationship across more neighbours.\nHowever, it is also possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 74 \nNumber of nonzero links: 592 \nPercentage nonzero weights: 10.81081 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n74 \n74 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n74 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 74 5476 592 1036 19636"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---fixed-distance",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---fixed-distance",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.3 Computing GI statistics - Fixed distance",
    "text": "4.12.3 Computing GI statistics - Fixed distance\n\ngi.fixed &lt;- localG(Battles_2023$Incidents, wm197_lw)\ngi.fixed\n\n [1] -2.16251076 -2.26945742 -2.36608309 -2.28060756 -1.40482297 -1.73401341\n [7] -1.78091543 -2.09517182  1.58404920  3.74739192  1.77357701  1.43327980\n[13]  1.76425408  0.23868264 -0.46980525  0.66543351 -0.74168975 -1.33781214\n[19] -0.51148627 -0.52707475  0.62973486  0.75874852  2.45091315 -0.79794591\n[25] -0.46714759  0.78360503 -2.26619716  1.90728203 -0.19741884  0.53880930\n[31] -1.05349256 -1.42802753 -0.14931467  3.57984196  2.13682831  0.42422030\n[37]  1.15690841  2.06391206  2.02017424  0.24781966  2.12585948  2.66825096\n[43]  0.59177302  2.14376526  1.90967745  1.33161281  1.56642737  0.66429027\n[49]  3.18467135 -0.08013071  2.14013808  0.26220228 -0.29653380 -0.70655951\n[55] -1.52151868 -1.55549239  1.38510282  1.35703308  1.80376535  0.80349295\n[61]  1.58567210  0.37028741  1.42615140 -0.19363169  0.39364611 -0.86461442\n[67] -1.28913769 -1.59126895 -1.85739540  0.40386411 -2.04421053 -1.82676914\n[73] -1.82291155 -2.01994548\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)       Z(Gi) Pr(z != E(Gi))\n [1,] 0.068229167 0.17808219 0.0025805215 -2.16251076   0.0305788284\n [2,] 0.007285974 0.09589041 0.0015242874 -2.26945742   0.0232405238\n [3,] 0.020046863 0.12328767 0.0019038942 -2.36608309   0.0179774092\n [4,] 0.006769071 0.09589041 0.0015270818 -2.28060756   0.0225716798\n [5,] 0.094095941 0.16438356 0.0025033091 -1.40482297   0.1600739255\n [6,] 0.110194304 0.20547945 0.0030195729 -1.73401341   0.0829157041\n [7,] 0.065091864 0.15068493 0.0023098862 -1.78091543   0.0749262673\n [8,] 0.091196626 0.20547945 0.0029752444 -2.09517182   0.0361557216\n [9,] 0.193083573 0.12328767 0.0019414335  1.58404920   0.1131825243\n[10,] 0.289515279 0.12328767 0.0019676510  3.74739192   0.0001786828\n[11,] 0.202220460 0.12328767 0.0019806822  1.77357701   0.0761331435\n[12,] 0.267664828 0.19178082 0.0028030997  1.43327980   0.1517778927\n[13,] 0.219305224 0.13698630 0.0021770936  1.76425408   0.0776892110\n[14,] 0.091077575 0.08219178 0.0013859604  0.23868264   0.8113516776\n[15,] 0.040245203 0.05479452 0.0009590683 -0.46980525   0.6384941605\n[16,] 0.023995827 0.01369863 0.0002394576  0.66543351   0.5057732553\n[17,] 0.090454904 0.12328767 0.0019596135 -0.74168975   0.4582753304\n[18,] 0.074313409 0.13698630 0.0021946699 -1.33781214   0.1809576830\n[19,] 0.139005236 0.16438356 0.0024618296 -0.51148627   0.6090105985\n[20,] 0.125490196 0.15068493 0.0022849420 -0.52707475   0.5981416802\n[21,] 0.058130190 0.04109589 0.0007317001  0.62973486   0.5288680718\n[22,] 0.078141499 0.05479452 0.0009468162  0.75874852   0.4480030085\n[23,] 0.340266667 0.20547945 0.0030244162  2.45091315   0.0142494332\n[24,] 0.200730880 0.24657534 0.0033008582 -0.79794591   0.4249018788\n[25,] 0.167275574 0.19178082 0.0027517563 -0.46714759   0.6403942853\n[26,] 0.303858068 0.26027397 0.0030935821  0.78360503   0.4332719050\n[27,] 0.062418386 0.17808219 0.0026049511 -2.26619716   0.0234393145\n[28,] 0.355729167 0.24657534 0.0032752773  1.90728203   0.0564840763\n[29,] 0.061812467 0.06849315 0.0011451559 -0.19741884   0.8434997910\n[30,] 0.146966527 0.12328767 0.0019313068  0.53880930   0.5900184491\n[31,] 0.043455497 0.08219178 0.0013519884 -1.05349256   0.2921153017\n[32,] 0.030184751 0.08219178 0.0013263280 -1.42802753   0.1532839350\n[33,] 0.076701571 0.08219178 0.0013519884 -0.14931467   0.8813053367\n[34,] 0.363684489 0.17808219 0.0026880602  3.57984196   0.0003438021\n[35,] 0.322002635 0.20547945 0.0029736197  2.13682831   0.0326119589\n[36,] 0.171367177 0.15068493 0.0023769086  0.42422030   0.6714051589\n[37,] 0.252951981 0.19178082 0.0027957315  1.15690841   0.2473097820\n[38,] 0.232058669 0.13698630 0.0021219065  2.06391206   0.0390260550\n[39,] 0.396593674 0.27397260 0.0036842794  2.02017424   0.0433653170\n[40,] 0.047619048 0.04109589 0.0006928579  0.24781966   0.8042739429\n[41,] 0.387329591 0.26027397 0.0035720591  2.12585948   0.0335149615\n[42,] 0.435444414 0.27397260 0.0036621834  2.66825096   0.0076247282\n[43,] 0.134509081 0.10958904 0.0017733202  0.59177302   0.5540025915\n[44,] 0.370217451 0.24657534 0.0033264297  2.14376526   0.0320517005\n[45,] 0.132483082 0.06849315 0.0011228022  1.90967745   0.0561747560\n[46,] 0.113924051 0.06849315 0.0011639833  1.33161281   0.1829874538\n[47,] 0.307425214 0.21917808 0.0031738082  1.56642737   0.1172486006\n[48,] 0.137802607 0.10958904 0.0018038490  0.66429027   0.5065045498\n[49,] 0.339932274 0.17808219 0.0025828347  3.18467135   0.0014491849\n[50,] 0.092809365 0.09589041 0.0014784223 -0.08013071   0.9361332989\n[51,] 0.287356322 0.17808219 0.0026070606  2.14013808   0.0323436091\n[52,] 0.261594581 0.24657534 0.0032811258  0.26220228   0.7931654986\n[53,] 0.071372753 0.08219178 0.0013311533 -0.29653380   0.7668224621\n[54,] 0.080156658 0.10958904 0.0017352153 -0.70655951   0.4798402603\n[55,] 0.123498695 0.20547945 0.0029031487 -1.52151868   0.1281297272\n[56,] 0.131920530 0.21917808 0.0031468082 -1.55549239   0.1198288467\n[57,] 0.035637728 0.01369863 0.0002508843  1.38510282   0.1660210309\n[58,] 0.034807642 0.01369863 0.0002419663  1.35703308   0.1747706992\n[59,] 0.366230366 0.26027397 0.0034505972  1.80376535   0.0712681006\n[60,] 0.292600313 0.24657534 0.0032811258  0.80349295   0.4216898674\n[61,] 0.354370214 0.26027397 0.0035214197  1.58567210   0.1128137120\n[62,] 0.295910393 0.27397260 0.0035100061  0.37028741   0.7111683500\n[63,] 0.299541655 0.21917808 0.0031753179  1.42615140   0.1538246447\n[64,] 0.222019781 0.23287671 0.0031438461 -0.19363169   0.8464642818\n[65,] 0.066967845 0.05479452 0.0009563271  0.39364611   0.6938423348\n[66,] 0.108660999 0.15068493 0.0023623728 -0.86461442   0.3872504579\n[67,] 0.124184712 0.19178082 0.0027494435 -1.28913769   0.1973502243\n[68,] 0.131770833 0.21917808 0.0030172252 -1.59126895   0.1115490605\n[69,] 0.041992697 0.12328767 0.0019156610 -1.85739540   0.0632549198\n[70,] 0.036378335 0.02739726 0.0004945225  0.40386411   0.6863126506\n[71,] 0.063607925 0.16438356 0.0024302999 -2.04421053   0.0409327537\n[72,] 0.096329081 0.19178082 0.0027302372 -1.82676914   0.0677344872\n[73,] 0.085438916 0.17808219 0.0025828347 -1.82291155   0.0683167878\n[74,] 0.065070276 0.16438356 0.0024173270 -2.01994548   0.0433890436\nattr(,\"cluster\")\n [1] Low  Low  Low  Low  Low  High Low  Low  Low  Low  High Low  High High High\n[16] Low  Low  High Low  Low  High High High Low  Low  High Low  Low  Low  Low \n[31] Low  Low  Low  High Low  High Low  Low  High Low  High High Low  High Low \n[46] High High High Low  High Low  Low  Low  Low  Low  High High Low  Low  Low \n[61] High Low  High Low  High High Low  Low  Low  High Low  Low  Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = Battles_2023$Incidents, listw = wm197_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nNext, we will join the Gi values to their corresponding sf data frame by using the code below.\n\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nBattles_2023.gi\n\nSimple feature collection with 74 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 99.66532 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DT   DT_PCODE    DT_MMR PCode_V year event_type Incidents Fatalities\n1    Hinthada MMR017D002  ဟင်္သာတခရိုင်     9.4 2023    Battles         4          3\n2     Labutta MMR017D004  လပွတ္တာခရိုင်     9.4 2023    Battles         1          1\n3     Pathein MMR017D001    ပုသိမ်ခရိုင်     9.4 2023    Battles         3          1\n4      Pyapon MMR017D006   ဖျာပုံခရိုင်     9.4 2023    Battles         3          2\n5        Bago MMR007D001    ပဲခူးခရိုင်     9.4 2023    Battles        50        270\n6     Taungoo MMR007D002  တောင်ငူခရိုင်     9.4 2023    Battles        87        493\n7        Pyay MMR008D001    ပြည်ခရိုင်     9.4 2023    Battles        34         59\n8  Thayarwady MMR008D002 သာယာဝတီခရိုင်     9.4 2023    Battles        50         93\n9       Falam MMR004D001   ဖလမ်းခရိုင်     9.4 2023    Battles        27         89\n10      Hakha MMR004D003 ဟားခါးခရိုင်     9.4 2023    Battles        48        210\n   Z.Incidents lag_Incidents gstat_fixed                       geometry\n1  -0.80534765      18.20000   -2.162511 MULTIPOLYGON (((95.12637 18...\n2  -0.85573862       3.00000   -2.269457 MULTIPOLYGON (((95.04462 15...\n3  -0.82214464       2.50000   -2.366083 MULTIPOLYGON (((94.27572 15...\n4  -0.82214464       3.00000   -2.280608 MULTIPOLYGON (((95.20798 15...\n5  -0.03268604      40.66667   -1.404823 MULTIPOLYGON (((95.90674 18...\n6   0.58880265      29.00000   -1.734013 MULTIPOLYGON (((96.17964 19...\n7  -0.30143790      31.40000   -1.780915 MULTIPOLYGON (((95.70458 19...\n8  -0.03268604      35.60000   -2.095172 MULTIPOLYGON (((95.85173 18...\n9  -0.41901684      53.00000    1.584049 MULTIPOLYGON (((93.36931 24...\n10 -0.06628002      62.00000    3.747392 MULTIPOLYGON (((93.35213 23...\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe codes above performs three tasks.\n\nFirst, it converts the output vector (i.e. gi.fixed) into r matrix object by using as.matrix().\nNext, cbind() is used to join Battles_2023 and gi.fixed matrix to produce a new SpatialPolygonDataFrame called Battles_2023.gi.\nLastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\n\n4.12.3.1 Mapping Gi values with Fixed distance weights\nThe code below plots the Gi values derived using fixed distance weight matrix, for event type==Battles in 2023.\n\nGimap &lt;-tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(Battles_2023.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"Fixed Distance\\nlocal Gi\") +\n  tm_borders(alpha = 0.5)\n\nGimap"
  },
  {
    "objectID": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---adaptive-distance",
    "href": "R-ex/R-Ex5/R_Ex5.html#computing-gi-statistics---adaptive-distance",
    "title": "Visual Analytics exercise on Armed conflicts - Initial Draft",
    "section": "4.12.4 Computing GI statistics - Adaptive distance",
    "text": "4.12.4 Computing GI statistics - Adaptive distance\nThe code below is used to compute the Gi values for Incidents of Battles in 2023 by using an adaptive distance weight matrix (i.e knb_lw).\n\ngi.adaptive &lt;- localG(Battles_2023$Incidents, knn_lw)\nBattles_2023.gi &lt;- cbind(Battles_2023, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\ndatatable(Battles_2023.gi)\n\n\n\n\n\n\n4.12.4.1 Mapping Gi values with Adaptive distance weights\nThe code below plots the Gi values derived using adaptive distance weight matrix for event type == Battles in 2023.\n\nGimap &lt;- tm_shape(mmr_shp_mimu_2) +\n  tm_borders() +  # Draws borders for all regions\n  tm_fill(col = \"white\", alpha = 0.5, title = \"Background\") +\n  \n  tm_shape(Battles_2023.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"Adaptive Distance\\nlocal Gi\") + \n  tm_borders(alpha = 0.5)\n\nGimap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters and Output to be exposed to Shiny\n\n\n\nIn terms of enabling user inputs, I will expose it to accept user inputs:\n\nYear\nEvent type\nData Classification type, and\nNumber of clusters for the Adaptive weight matrix"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html",
    "href": "R-ex/R-Ex3/R_Ex3.html",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "",
    "text": "DataVis Makeover\n\n\n\nRemaking a peer’s original design by improving the clarity and aesthetics of charts by creating an alternative design."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#loading-r-packages",
    "href": "R-ex/R-Ex3/R_Ex3.html#loading-r-packages",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\n\npacman::p_load(tidyverse, haven, ggrepel, ggthemes, hrbrthemes, patchwork, intsvy, ggdist, ggridges,colorspace, plotly)"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#importing-the-data-set",
    "href": "R-ex/R-Ex3/R_Ex3.html#importing-the-data-set",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.2 Importing the Data set",
    "text": "2.2 Importing the Data set\n\nstu_qqq_SG &lt;- \n  read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#removing-missing-values-and-converting-data-types",
    "href": "R-ex/R-Ex3/R_Ex3.html#removing-missing-values-and-converting-data-types",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.3 Removing missing values and Converting data types",
    "text": "2.3 Removing missing values and Converting data types\nRows with missing values for ESCS were removed before further analysis.\n\n\nShow the code\nstu_qqq_SG_clean &lt;- stu_qqq_SG[complete.cases(stu_qqq_SG[, \"ESCS\"]), ]\n\n\nCNTSCHID and CNTSTUID’s data types were changed to character.\n\n\nShow the code\nstu_qqq_SG_clean$CNTSCHID &lt;- as.character(stu_qqq_SG_clean$CNTSCHID)\nstu_qqq_SG_clean$CNTSTUID &lt;- as.character(stu_qqq_SG_clean$CNTSTUID)"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#binning-variable-escs",
    "href": "R-ex/R-Ex3/R_Ex3.html#binning-variable-escs",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.4 Binning Variable ESCS",
    "text": "2.4 Binning Variable ESCS\nThe ESCS variable was binned into quantiles using the mutate() and cut() function\n\n\nShow the code\nstu_qqq_SG_clean &lt;- stu_qqq_SG_clean %&gt;%\n  mutate(ESCS_recoded = cut(ESCS,breaks=quantile(ESCS,c(0,0.25,0.5,0.75,1)),labels=c(\"Very Low\",\"Low\",\"Medium\",\"High\"),include.lowest=TRUE))"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#recoding-st004d01t-gender-variable",
    "href": "R-ex/R-Ex3/R_Ex3.html#recoding-st004d01t-gender-variable",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "2.5 Recoding ST004D01T (Gender) variable",
    "text": "2.5 Recoding ST004D01T (Gender) variable\nVariable ST004D01T, was recoded to labels to “Female” and “Male” respectively.\n\n\nShow the code\nstu_qqq_SG_clean$ST004D01T &lt;- recode(stu_qqq_SG_clean$ST004D01T, \"1\" = \"Female\", \"2\" = \"Male\")"
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#general-distribution-of-students-performance-in-math-science-and-reading",
    "href": "R-ex/R-Ex3/R_Ex3.html#general-distribution-of-students-performance-in-math-science-and-reading",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.1 General distribution of students’ performance in Math, Science and Reading",
    "text": "3.1 General distribution of students’ performance in Math, Science and Reading\nFirst, the author had created a data folder path to save any data files generated:\n\n\nShow the code\ndata_folder_path &lt;- file.path(getwd(), \"data\")\n\n\nThe code below uses the instvy package to extract the composite Means and Standard Deviations of PV values across the population of students that took the assessment.\n\n\nShow the code\npvmathgeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_general\",folder=data_folder_path)\n\npvmathgeneral$subject &lt;- \"Math\"\n\npvreadgeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_general\",folder=data_folder_path)\n\npvreadgeneral$subject &lt;- \"Reading\"\n\npvsciegeneral &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_general\",folder=data_folder_path)\n\npvsciegeneral$subject &lt;- \"Science\"\n\nmergedgeneral &lt;- rbind(pvmathgeneral,pvreadgeneral,pvsciegeneral)\nmergedgeneral %&gt;% relocate(subject)\n\n\n  subject Freq   Mean s.e.     SD  s.e\n1    Math 6559 575.27 1.26 102.68 0.93\n2 Reading 6559 543.25 1.91 105.73 1.16\n3 Science 6559 561.97 1.33  99.02 1.10\n\n\nA plot to show the general distribution of Math, Reading and Science scores was generated using the code below.\n\nThe original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the distribution of Student scores across the 3 subjects.\n\n\n\np1&lt;- ggplot(mergedgeneral, aes(x = subject, y = Mean,colour=subject)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    aes(colour=subject),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD),\n    width = 0.5,\n    size=0.8,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Scores across Math, Reading and Science across all 15-year olds who took PISA 2022 in SGP\") +\n  ylab(\"Scores\") +\n  xlab(\"Subject\")+\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 50))+\n  coord_flip() +\n  theme_minimal(base_size = 12)\np1\n\n\n\n\n\n3.1.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across subjects, I am unclear as to which score this is showing. Is it PV1 or average of the PV values?\nGraph is titled as “Scores across Math, Reading and Science”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this plot only has 3 observations and 6 variables. Upon inspecting the data set, it is actually the composite mean score for each subject for the entire student cohort.\n\n\nThe code and subset used is not suitable to plot a Distribution of student scores across subjects. I will extract a new subset with all PV1 scores at the individual student level. This will then be used to plot the distribution of student scores for the different subjects.\n\n\n\nAesthetics\n\nThere is already a legend on the right which identifies subjects by colour, thus making the Y axis labels seem redundant.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated. This contributes to “extra” space and additional non-data ink.\nUnable to decipher the actual scores even with the X-axis tick marks.\nSince the subjects are clearly labelled in the Y axis, the label “Subject” can be removed, to minimize non-data ink.\n\n\nTo remove the legend or Y-Axis labels, to reduce non-data ink.\nShorten the X-axis, or extract new data points to populate the correct range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\nTo remove the label “Subject”, since it does not provide additional information to readers.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nUpon closer examination of the code used, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD),\n    #width = 0.5,\n    #size=0.8,\n    #position = position_dodge(width = 0.5)\n\nI would conclude that this plot is actually comparing the mean subject scores across subjects.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(mergedgeneral)\n\n  Freq   Mean s.e.     SD  s.e subject\n1 6559 575.27 1.26 102.68 0.93    Math\n2 6559 543.25 1.91 105.73 1.16 Reading\n3 6559 561.97 1.33  99.02 1.10 Science\n\n\nThe mergedgeneral subset table only has 3 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe code and data subset used by the author will not be able to plot a Distribution of student scores across subjects.\nI will revert to the original large data set to extract PV1 scores which are available for all students to create new plots to visualize the distribution of scores.\n\n\n\n\n3.1.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.1.3 Remake\nThe original data sub set and plot is unable to exhibit the distribution of student scores across subjects, due to the lack of scores at an individual student level.\nFirst, I will use the code below derive some reference lines (mean and median) for our histograms.\nTo populate the distribution of scores for the student cohort, I will use PV1 scores, which are available for all students.\n\n\nShow the code\nmean_pv1math &lt;- mean(stu_qqq_SG$PV1MATH)\nmedian_pv1math &lt;- median(stu_qqq_SG$PV1MATH)\nmean_pv1read &lt;- mean(stu_qqq_SG$PV1READ)\nmedian_pv1read &lt;- median(stu_qqq_SG$PV1READ)\nmean_pv1scie &lt;- mean(stu_qqq_SG$PV1SCIE)\nmedian_pv1scie &lt;- median(stu_qqq_SG$PV1SCIE)\n\n\nNext, I will use the code below and extract data points (PV1 scores) from the original large data set to create Histograms with Density plots for each subject.\n\n\nShow the code\np_1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightblue') +\n  geom_density(color = \"purple\") + \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Math Scores\", y = \"Density\") + # Change label to 'Density'\n  geom_vline(xintercept = mean_pv1math, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1math, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1math, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1math, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1math, y = 0.0025, label = paste(\"Median:\", round(median_pv1math, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightgreen') +\n  geom_density(color = \"purple\") + \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Reading Scores\", y = \"Density\") + \n  geom_vline(xintercept = mean_pv1read, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1read, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1read, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1read, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1read, y = 0.0025, label = paste(\"Median:\", round(median_pv1read, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(aes(y = ..density..), binwidth = 20, color = \"black\", fill='lightpink') +\n  geom_density(color = \"purple\") +  \n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(x = \"Science Scores\", y = \"Density\") + \n  geom_vline(xintercept = mean_pv1scie, color = 'black', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = mean_pv1scie, y = 0.0025, label = paste(\"Mean:\", round(mean_pv1scie, 2))), # Adjust y position for text\n    color = \"black\", hjust = 1.75, size = 3\n  ) +\n  geom_vline(xintercept = median_pv1scie, color = 'red', size = 0.65, linetype = \"dashed\") +\n  geom_text(\n    aes(x = median_pv1scie, y = 0.0025, label = paste(\"Median:\", round(median_pv1scie, 2))), # Adjust y position for text\n    color = \"red\", hjust = -0.75, size = 3\n  ) +\n  theme_minimal()\n\np_4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Combined Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\nFinally, using the code below, I will create a composite graph for all the histograms and density plots.\n\n\nShow the code\npatch1 &lt;- (p_1 + p_2) / (p_3 + p_4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\",\n                subtitle = \"All subjects exhibit a slight left-skewed distribution\" )\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe revised plot consists of a patchwork of 3 histograms with density curves, and a combined density curve overlay for the 3 subjects.\n\nReference lines for the mean and median scores are added to enable readers to infer the skewness of the distribution of student scores.\n\nThe revised plot will enable readers to quickly infer the nature of the distributions of the subject scores for the student cohort."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-across-schools",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-across-schools",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.2 Distribution of students’ performance in Math, Science and Reading across schools",
    "text": "3.2 Distribution of students’ performance in Math, Science and Reading across schools\nThe author has used the pisa.mean.pv function from intsvypackage, to obtain the composite Mean and Standard Deviation values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each school.\nThe code below generates the composite PV values.\n\n\nShow the code\npvmathsch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bysch\",folder=data_folder_path)\n\npvreadsch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bysch\",folder=data_folder_path)\n\npvsciesch &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),by= \"CNTSCHID\", data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bysch\",folder=data_folder_path)\n\n\nA plot to show the distribution of students performance in Math, Reading and Science scores across schools was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the subject scores by each school id (164 schools).\n\n\n\np2&lt;- ggplot(pvmathsch, aes(x = as.factor(CNTSCHID), y = Mean)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=1.5) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    position = position_dodge(width = 0.75),\n    color = \"red\"\n  ) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD),\n    width = 0.8,\n    size=1,\n    position = position_dodge(width = 10)\n  ) +\n  labs(title = \"Math Scores by School\") +\n  ylab(\"Math Score\") +\n  xlab(\"School ID\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=100))+\n  coord_flip() +\n  theme_minimal(base_size=20)             \n\n\n\n\n\n3.2.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across schools, I am unclear as to which score this is showing. Is it showing me the range of student scores by school ID?\nGraph is titled as “Subject Scores by school”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this visualization only has 164 observations and 6 variables. Upon inspecting the data set, this subset is basically the composite mean subject scores for each school.\n\n\nThe original subset can still be used, but the code will need to be changed for the revised visualization. This new visualization will retain the original intent to show the different mean scores for each of the 164 schools to visualize the relative performance between schools.\n\n\n\nAesthetics\n\nThe use of a vertically long plot to compare the scores across schools makes it visually challenging and difficult to spot differences in scores, and identify any clusters.\nSince there are 164 schools, the use of a vertically long format makes it difficult to read the school IDs on the Y-axis. There are too many schools to make identification off the Y-axis easy.\nThe distance between the X-axis, which shows the scores is too far apart from the error bars. I am unable to decipher what are the actual scores or score range for each error bar.\nThe graph is stiched side by side, resulting in the Y-axis being repeated 3 times. The order of the School IDs are all the same, hence this creates additional non-data ink, that makes the graph too “busy”.\n\n\nTo revise the design of a vertically long format and re-make it to a horizontally compact visualization, by using bubble plots.\nTo minimize axis labels, we can drop axis labels for school id, and only identify and directly annotate the top & bottom 5 schools instead.\nTo remove the 2 other repetitive Y-axes to reduce non-data ink.\n\n\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(head(pvmathsch))\n\n  CNTSCHID Freq   Mean  s.e.    SD   s.e\n1 70200001   55 725.21  9.34 59.23  6.38\n2 70200002   37 536.19 17.09 90.27 14.50\n3 70200003   36 739.92 12.30 59.23  7.70\n4 70200004   56 509.61 12.84 86.63  7.71\n5 70200005   37 548.39 13.10 86.30  9.18\n6 70200006   36 485.30 13.90 76.47  8.86\n\n\nThe pvmathsch subset table only has 164 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe data subset can only be used to visualize the mean scores per school ID.\nThe original graph is not of a suitable design to help readers easily and quickly understand the disparity in performances between schools.\nI will use a new compact visualization to enable the reader to quickly see the differences in school performance.\n\n\n\n\n3.2.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.2.3 Remake\nThe original plot does not enable the reader to quickly capture information from the data points.\nUsing the exact same data set, I will use the code below to re-make the original graph into a Bubble plots instead. I will employ color as a means to differentiate the mean scores.\nI have also added reference lines to show the 10th and 90th percentile scores to help readers easily identify the schools belonging to these two opposite segments.\nLastly, I have also added annotations to help readers to quickly identify the top 5 and bottom 5 schools with the best and worst mean scores.\n\n\nShow the code\nlibrary(ggrepel)\n\n# Identify the top 5 and bottom 5 schools\ntop_5 &lt;- pvmathsch %&gt;% \n  arrange(desc(Mean)) %&gt;%\n  slice(1:5)\n\nbottom_5 &lt;- pvmathsch %&gt;% \n  arrange(Mean) %&gt;%\n  slice(1:5)\n\n# Base plot with points colored by the mean score\np1 &lt;- ggplot(pvmathsch, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(color = Mean), alpha = 0.5, size = 7) +\n  scale_color_gradient(low = \"red\", high = \"green\") +\n  labs(title = \"Mean Math Scores per School\",\n       subtitle = \"5 Schools with the Best & Worst mean scores\",\n       x = \"School ID\",\n       y = \"Mean Math Scores\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.grid.major = element_blank())  # Remove major grid lines\n       \n\n# Annotate the top 5 and bottom 5 schools\np1 &lt;- p1 +\n  geom_text_repel(\n    data = top_5, \n    aes(label = CNTSCHID, y = Mean), \n    color = \"blue\", \n    size = 3, \n    nudge_y = 12,   # Adjust nudge_y if necessary to move text up or down\n    direction = \"y\"\n  ) +\n  geom_text_repel(\n    data = bottom_5, \n    aes(label = CNTSCHID, y = Mean), \n    color = \"blue\", \n    size = 3, \n    nudge_y = -12,  # Adjust nudge_y if necessary to move text up or down\n    direction = \"y\"\n  )\n\n# Calculate the 10th and 90th percentiles\npercentile10 &lt;- quantile(pvmathsch$Mean, probs = 0.10, na.rm = TRUE)\npercentile90 &lt;- quantile(pvmathsch$Mean, probs = 0.90, na.rm = TRUE)\n\n# Add horizontal lines for the 10th and 90th percentiles to your plot\np1 &lt;- p1 + \n  geom_hline(yintercept = percentile10, linetype = \"dashed\", color = \"blue\", size = 0.5) +\n  geom_hline(yintercept = percentile90, linetype = \"dashed\", color = \"blue\", size = 0.5) +\n  geom_text(aes(x = Inf, y = percentile10, label = paste(\"10th Percentile:\", round(percentile10, 2))), \n            hjust = 1.05, vjust = 0, color = \"blue\", size = 3) +\n  geom_text(aes(x = Inf, y = percentile90, label = paste(\"90th Percentile:\", round(percentile90, 2))), \n            hjust = 1.05, vjust = 1, color = \"blue\", size = 3)\n\np1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe use of colour can help readers quickly differentiate the scores between schools.\n\nI have annotated school IDs for the top and bottom 5 schools for readers to quickly identify the “best” and “worst” schools.\nI have also added reference lines for the 10th and 90th Percentile. Here we can quickly see that while most of the 164 schools attain the same range of mean scores, there are a number of schools that outperform the rest.\nAnnotating all the 164 school IDs will add too much ink to the plot. Hence, we can consider having another interactive plot (below) for readers to examine the data points in more detail at their descretion.\n\n\nFor interactivity, I have used ggplotly() to convert this to an interactive graph. With the added interactivity, readers can use the tooltip function to get more information on the data points.\n\n\nShow the code\nlibrary(plotly)\n\n# Convert to an interactive plot\nggplotly(p1, tooltip = c(\"x\", \"y\"))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome of the previous annotations generated in the static plot may not appear after turning it into an interactive plot. Hence, we can use both static and interactive plots together if needed. For example, static charts to quickly summarize and communicate information to reader, and an interactive plot for readers to drill down in more detail, eg use of zoom, identifying specific data points via tooltip etc."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.3 Distribution of students’ performance in Math, Science and Reading by gender",
    "text": "3.3 Distribution of students’ performance in Math, Science and Reading by gender\nThe author had used the pisa.mean.pv function from intsvy package to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each gender.\nThe code below was used to generate the composite PV values for Math, Reading and Science by gender.\n\n\nShow the code\npvmathgenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bygenderonly\",folder=data_folder_path)\n\npvreadgenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bygenderonly\",folder=data_folder_path)\n\npvsciegenderonly &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),c(\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bygenderonly\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores by gender was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars across the 3 subjects to represent the subject scores by gender.\n\n\n\nggplot(pvmathgenderonly, aes(x = as.factor(ST004D01T), y = Mean,fill=ST004D01T)) +\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 5,\n    aes(colour=ST004D01T),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    width = 0.5,\n    size=0.8,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Math Scores by Gender\") +\n  ylab(\"Math Score\") +\n  xlab(\"Gender\")+\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, by = 100))+\n  scale_fill_discrete(name = \"Gender\") +\n  scale_color_discrete(name = \"Gender\") +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n3.3.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to see the differences in scores between the genders, I am unclear as to which score this is showing. Is it showing the range of PV1 scores or average of PV values?\nGraph is titled as “Subject Scores by Gender”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for the plot only has 2 observations and 6 variables. Upon inspecting the data set, this subset is actually the composite mean score for each gender across the subjects.\n\n\nThe code and subset used is not suitable to plot a range of scores for each gender across subjects.\nI will extract a new subset with all PV1 scores at the individual student level. This will then be used to plot the distribution and range of scores for the different genders across all subjects.\n\n\n\nAesthetics\n\nThe graphs are already well labelled in the x and y axes, and identifies the different genders clearly. The use of different colors per gender together with the legend is not necessary.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated.\nUnable to decipher the actual scores even with the granularity in X-axis tick marks.\n\n\nTo remove the legend since there are only two categories of gender and they are already differentiated by colour.\nShorten the X-axis, or extract new data points to correctly populate the range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\nThe new visualization will need to enable readers to quickly differentiate the performance between genders across subjects.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nupon closer examination of the code below, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    #width = 0.5,\n    #size=0.8,\n    #position = position_dodge(width = 0.5)\n\nThe original data set and plot is instead comparing the mean subject scores across different genders.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(pvmathgenderonly)\n\n  ST004D01T Freq   Mean s.e.     SD  s.e\n1    Female 3227 569.01 1.70  97.49 1.14\n2      Male 3332 581.30 1.74 107.09 1.36\n\n\nThe pvmathgenderonly data set only has 2 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThis data set can only be used to compare the overall mean scores for genders across the 3 subjects.\nThe visualization needs to be done differently to enable the reader to obtain more information regarding the performance between genders.\nI will extract a new subset and create new graphs to show the range and distribution of scores between the genders across subjects.\nI will use PV1 scores which are available for all students to create new plots to visualize the distribution of scores.\n\n\n\n\n3.3.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.3.3 Remake\nThe original data subset and plot is unable to exhibit the range and distribution of subject scores across genders, due to the lack of scores at an individual student level.\nTherefore, I will extract data from the original data set to create new subsets for the new plots using the code below.\nIn terms of data, I will use the PV1 scores which are available for all students.\n\n\nShow the code\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nMath_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1MATH, na.rm = TRUE)  # Calculate the mean of PV1MATH, removing NA values\n  )\n\nRead_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1READ, na.rm = TRUE)  # Calculate the mean of PV1READ, removing NA values\n  )\n\nSCIE_gender &lt;- subset_gender_PV1 %&gt;%\n  group_by(ST004D01T) %&gt;%  # Group by gender column\n  summarise(\n    Freq = n(),             # Count the frequency of each gender\n    Mean = mean(PV1SCIE, na.rm = TRUE)  # Calculate the mean of PV1SCIE, removing NA values\n  )\n\n\nNext, I will use the code below to create new box plots for each subject. This will show the range of scores for the genders in different subjects.\n\n\nShow the code\n# Create the plot using the subset_data\nbxp1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust =4, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Math Scores\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +  \n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\nbxp2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust = 4.25, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Reading Scores\") +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightgreen\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\") \n  \n\nbxp3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 1000)) +\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +\n  geom_text(data = Math_gender, \n          aes(x = ST004D01T, y = Mean, label = paste(\"Mean:\", round(Mean, 2))), \n          color = \"black\", hjust = -0.2, vjust = 4, size = 2.75) +\n  labs(x = \"Gender\",\n       y = \"Science Scores\") +\n  scale_fill_manual(values = c(\"lightpink\", \"lightpink\")) +  # Associate colors with factor levels\n  theme_minimal() +\n  theme(legend.position = \"none\") \n\n\nBesides showing the differences in scores through box plots, I will also plot ridgeline plots, segmented into 4 quantiles, to display the distribution and difference in subject scores between genders across the subjects.\n\n\nShow the code\nrp1 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1MATH, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  )  +\n  labs(title = \"Math Scores\\nacross genders\")\n\nrp2 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1READ, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  )  +\n  labs(title = \"Reading Scores \\nacross genders\")\n\nrp3 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1SCIE, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE,\n    alpha = 0.5 ) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = NULL,\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 10)  \n  ) +\n  labs(title = \"Science Scores\\nacross genders\")\n\n\nI will use the code below to create a composite graph.\n\n\nShow the code\npatch3 &lt;- (rp1+ bxp1)/(rp2+bxp2)/(rp3+bxp3) + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\",\n                subtitle = \"Female students outperform in Reading\")\n\npatch3 & theme(panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe revised plot consists of a patchwork of ridgeline plots and box plots for each subject.\nThe ridgeline plot been further segmented into 4 quantiles, differentiated by colour. This not only provides insights into the distribution of the scores but also enables readers to quickly understand the differences in performance between genders.\nThe box plots, further help readers to understand the differences in scores between genders. The range of scores can be seen along with the mean scores."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-math-reading-and-science-scores-by-socioeconomic-status",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-math-reading-and-science-scores-by-socioeconomic-status",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.4 Distribution of Math, Reading and Science scores by socioeconomic status",
    "text": "3.4 Distribution of Math, Reading and Science scores by socioeconomic status\nThe author had previously created a new variable, “ESCS_recoded”, which bins the socio-economic (ESCS) score into four quantiles - “Very Low”, “Low”, “Medium” and “High”. The author had used the pisa.mean.pv function from intsvypackage to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” and grouped by the ESCS score.\nThe code below was used to generate the composite PV values for Math, Reading and Science.\n\n\nShow the code\npvmathescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmathescs\",folder=data_folder_path)\n\npvreadescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvreadescs\",folder=data_folder_path)\n\npvscieescs &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),c(\"ESCS_recoded\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscieescs\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores across schools by socioeconomic status was generated using the code below.\n\nThe Original plotThe code\n\n\n\nThe author had created a series of error bars to represent the scores across the 4 categories of the ESCS score (High, Medium, Low, Very Low).\n\n\n\nggplot(pvscieescs, aes(x = ESCS_recoded, y = Mean,fill=ESCS_recoded))+\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.5),lwd=0.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 10,\n    size = 2,\n    aes(colour=ESCS_recoded),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ESCS_recoded),\n    width = 0.5,\n    size=1,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Science Scores by Socioeconomic Status\") +\n  ylab(\"Science Score\") +\n  xlab(\"Socioeconomic Status\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=200))+\n  coord_flip() +\n  theme_minimal(base_size=10)\n\n\n\n\n\n3.4.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores between the ESCS categories, I am unclear as to which score this is showing. Is it showing me the range of PV1 scores or average of PV values?\nGraph is titled as “Subject Scores by Socioeconomic Status”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for the plots only has 4 observations and 6 variables. Upon inspecting the data set, this is actually just the overall mean scores for each of the newly binned ESCS score category.\n\n\nThe code and subset used is not suitable to plot a range or distribution of scores for each ESCS score category.\nI will extract a new subset with all PV1 scores at the individual student level together with ESCS scores per student. I will then plot the relationship between the two continuous variables (subject scores and ESCS score) via scatter plots.\nA scatter plots will enable readers to quickly infer the relationships between subject scores and socioeconomic statuses.\n\n\n\nAesthetics\n\nThe Y-axis is already well labelled and identifies the different socioeconomic segments clearly, the use of different colors per segment together with legend is not necessary.\nThe scale for the X-axis is from 0 to 1000, in increments of 50. Because of the “shortness” of the error bars, the graph appears too elongated.\nEven with the background grid lines and X axis tick marks, I am unable to decipher the actual scores clearly.\n\n\nTo remove the legend since the ESCS segments are already differentiated by colour and identifiable through the Y-axis labels.\nShorten the X-axis, or extract new data points to correctly populate the range of scores.\nTo annotate the mean scores onto the plot, so that we can remove the background grid lines and X-axis tick marks and reduce non-data ink.\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nUpon closer examination of the code below, the author had included an error bar with Mean +/- one standard deviation.\n\n#geom_errorbar(\n    #aes(ymin = Mean - SD, ymax = Mean + SD,color=ESCS_recoded),\n    #width = 0.5,\n    #size=1,\n    #position = position_dodge(width = 0.5)\n\nHence the original plot can only be used to compare the mean subject scores across different ESCS segments.\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(pvscieescs)\n\n  ESCS_recoded Freq   Mean s.e.    SD  s.e\n1     Very Low 1640 503.73 3.14 97.91 1.97\n2          Low 1640 546.33 2.61 93.23 1.75\n3       Medium 1640 584.61 2.31 86.07 2.04\n4         High 1639 610.62 3.23 84.01 2.21\n\n\nThe data subset used for the plots only has 4 observations and 6 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThis subset and the code used is not able to plot the range of scores per ESCS segment, and can only be used to compare the composite mean scores of each of the 4 ESCS segments.\nAlthough it is useful to compare the mean scores between different ESCS segments, a different set of data and plots can be used instead to communicate more information about the strength or weakness of the relationship between subject scores and socioeconomic statuses.\n\n\nAdditionally, the binning of the ESCS score, while helpful may be limited in usefulness, for the following reasons:-\n\nBinning the ESCS scores adds in an element of subjectivity. For example, why should there be 4 segments of socioeconomic statuses instead of 3, or 6?\nCould the number of socioeconomic segments be applicable for other countries? For example, would a “Low segment” in Singapore be equivalent to a “Low segment” in another country?\n\n\n\n3.4.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.4.4 Remake\nInstead of binning the ESCS score and transforming it to a categorical type, I will keep the ESCS score as a continuous variable.\nI will use scatter plots to visualize the strength or weaknesses of the relationships between the socioeconomic statuses and subject scores.\nDrawing inspiration from Prof Kam’s Lesson 3 and well as additional useful references, I will proceed to plot Scatter plots with Marginal Histograms.\nIn terms of data, I will use the PV1 scores which at the individual student level.\nFor this visualization, I will use the ggstatplot package. (Patil 2021)\nFor more information on this package and how it can be used to create graphics from statistical tests included in the plots themselves, please refer to this link.\n\nlibrary(ggstatsplot)\n\nCreating new data subset for visualization.\n\n\nShow the code\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\n\nWe will use the code below from ggstatplot package to generate the plot.\n\n\nShow the code\nggscatterstats(\n  data = subset_ESCS_PV1,                                          \n  x = ESCS,                                                  \n  y = PV1MATH,\n  xlab = \"Socioeconomic score (ESCS)\",\n  ylab = \"Math scores\",\n  marginal = TRUE,\n  marginal.type = \"histogram\",\n  centrality.para = \"mean\",\n  margins = \"both\",\n  title = \"Relationship between Socio-economic and Math scores\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised Plot\n\n\n\nThe benefit of this plot is that it shows both the correlation between the two continous variables as well as their respective distributions, and includes important statistics like the Pearson coefficient.\nFrom the plot above, the pearson coefficient of 0.42 indicates that there is a weak positive relationship between ESCS scores and Math scores.\nThe marginal histograms for both variables also enables readers to additionally infer the distribution of these variables. For example, Math scores resemble a normal distribution, while ESCS scores resemble a left-skewed distribution."
  },
  {
    "objectID": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender-across-schools",
    "href": "R-ex/R-Ex3/R_Ex3.html#distribution-of-students-performance-in-math-science-and-reading-by-gender-across-schools",
    "title": "Take-Home Exercise 1c : DataVis Makeover of another student’s work",
    "section": "3.5 Distribution of students’ performance in Math, Science and Reading by gender across schools",
    "text": "3.5 Distribution of students’ performance in Math, Science and Reading by gender across schools\nThe author had intended to analyse whether there are differences in performances in Math, Reading and Science between genders at a more granular level - within schools.\nThe author had used the pisa.mean.pv function from intsvypackage to obtain the Mean and Standard Deviation of PV values from “PV1MATH” to “PV10MATH”, “PV1READ” to “PV10READ” and “PV1SCIE” to “PV10SCIE” for each school and by gender.\nThe below code was used to generate the composite PV values.\n\n\nShow the code\npvmathgender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"MATH\"),c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvmath_bygender\",folder=data_folder_path)\n\npvreadgender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"READ\"),c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvread_bygender\",folder=data_folder_path)\n\npvsciegender &lt;- pisa.mean.pv(pvlabel=paste0(\"PV\",1:10,\"SCIE\"),by= c(\"CNTSCHID\",\"ST004D01T\"), data=stu_qqq_SG_clean,export=FALSE,name=\"pvscie_bygender\",folder=data_folder_path)\n\n\nA plot to display the distribution of Math, Reading and Science scores across schools by gender, was generated using the code below.\n\nThe Original PlotThe code\n\n\n\nThe author had created a series of 2 error bars for each school ID across the 3 subjects to represent the subject scores for each gender for all schools (164 schools).\n\n\n\np8&lt;- ggplot(pvmathgender, aes(x = as.factor(CNTSCHID), y = Mean,fill=ST004D01T))+\n  geom_boxplot(width = 0.5, position = position_dodge(width = 0.75),lwd=1.5,show.legend=FALSE) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 18,\n    size = 10,\n    aes(colour=ST004D01T),\n    position = position_dodge(width = 0.75)) +\n  geom_errorbar(\n    aes(ymin = Mean - SD, ymax = Mean + SD,color=ST004D01T),\n    width = 1,\n    size=2,\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"Math Scores by Gender across Schools\") +\n  ylab(\"Math Score\") +\n  xlab(\"School ID\")+\n  scale_y_continuous(limits=c(0,1000),breaks = seq(0, 1000, by=200))+\n  coord_flip() +\n  theme_minimal(base_size=50)+\n  theme(legend.position=\"none\")\n\n\n\n\n\n3.5.1 Evaluation\n\n\n\n\n\n\n\n\nAssessment Criteria\nCritique/Feedback\nPossible Improvements\n\n\n\n\nClarity\n\nAlthough the original plot enables me to visualize the differences in scores across schools, I am unclear as to which score this is showing. Is it showing me the range of gender scores by school ID?\nGraph is titled as “Subject Scores by Gender across school”, however the code indicates that mean scores +/- one standard deviation were generated instead.\nThe data subset used for this visualization has 301 observations and 7 variables. Upon inspecting the data set, it is basically the overall mean subject scores for male and female students per school\n\n\nPrevious visualization had already analysed the differences per school. To further make it more granular by introducing gender, does not seem to add further value in terms of insights.\nWe can retain the original intent to show the granular difference in performance for the schools, by examining the actual disparity between the best and the worst schools instead.\n\n\n\nAesthetics\n\nThe use of a vertically long plot to compare the scores across schools makes it visually challenging and difficult to spot differences between schools, and identify any clusters.\nSince there are 164 schools, the use of a vertically long format makes it difficult to read the Y-axis labels. There are too many schools to make identification off the Y-axis easy. This is further compounded with the additional splitting of genders for every school.\nThe distance between the X-axis, which shows the scores is too far apart from the error bars. I am unable to decipher what are the actual scores or score range for each error bar.\nThe graph is patched side by side, resulting in the Y-axis being repeated 3 times. The order of the School IDs are all the same, hence this creates additional non-data ink, that makes the graph too “busy”.\n\n\nTo revise the design of a vertically long format and re-make it to a horizontally compact visualization, by using ridge line plots to show the differences in performance only between the best and worst schools.\nTo minimize axis labels, we will only identify and plot the top & bottom 5 schools instead.\n\n\n\n\nExamining the subset used for the original plot in more detail.\n\nprint(head(pvmathgender))\n\n  CNTSCHID ST004D01T Freq   Mean  s.e.    SD   s.e\n1 70200001      Male   55 725.21  9.34 59.23  6.38\n2 70200002    Female   15 537.70 27.07 80.64 15.79\n3 70200002      Male   22 535.13 21.92 96.03 20.81\n4 70200003    Female    7 739.65 22.04 39.42 14.39\n5 70200003      Male   29 739.99 14.58 62.67  8.56\n6 70200004    Female   28 505.32 19.68 91.11 11.94\n\n\nThe pvmathgender subset table has 307 observations and 7 variables.\n\n\n\n\n\n\nConclusion\n\n\n\nThe data subset can only be used to visualize the mean scores of the two genders for every school.\nThe original graph is not of a suitable design to help readers easily and quickly understand the disparity in performances between schools. By adding a further split into genders per school further complicates this visually.\nI will use a new compact visualization to enable the reader to quickly identify the disparity in school performance. Instead of analyzing all schools, we can narrow our focus just to the “best” and “worst” schools.\n\n\n\n\n3.5.2 Sketch of Proposed Design\nRough sketch of proposed design is shown below.\n\n\n\n3.5.3 Remake\nThe author had previously extracted the mean scores per school across the 3 subjects. I will use the same sub sets to extract the top and bottom 5 School IDs for each subject.\n\n\nShow the code\n# Identify the top and bottom 5 schools\ntop_bottom_math &lt;- pvmathsch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\ntop_bottom_read &lt;- pvreadsch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\ntop_bottom_scie &lt;- pvsciesch %&gt;%\n  arrange(Mean) %&gt;%\n  slice(c(1:5, (n()-4):n())) %&gt;%\n  pull(CNTSCHID)\n\n\nNext I will filter the data set and obtain PV1 student scores from the top and bottom 5 schools only.\n\n\nShow the code\n# Filter the original dataset to include only the selected schools\n\ntop_bottom_mathsch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_math)\n\ntop_bottom_readsch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_read)\n\ntop_bottom_sciesch &lt;- stu_qqq_SG %&gt;%\n  filter(CNTSCHID %in% top_bottom_scie)\n\n\ntop_bottom_mathsch &lt;- top_bottom_mathsch %&gt;%\n  mutate(CNTSCHID = factor(CNTSCHID, levels = top_bottom_math))\n\n\nLastly, I will use the code below to create Ridgeline plots for the top and bottom 5 schools.\n\n\nShow the code\nmath_5 &lt;- ggplot(top_bottom_mathsch, aes(x = PV1MATH, y = factor(CNTSCHID), fill = after_stat(x))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_gradientn(\n    name = \"Math Scores\",\n    colors = c(\"red\", \"green\"),\n    limits = c(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE)),\n    breaks = seq(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE), length.out = 5)\n  ) +\n  coord_cartesian(xlim=c(0,1000)) +\n  theme_ridges() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 15)  \n  )  +\n  labs(y = \"School ID\", x = NULL, title = \"Math Scores for the Top and Bottom 5 Schools\") +\n  theme(\n    axis.text.y = element_text(angle = 0, hjust = 0.5)  \n  )\n\n\nmath_5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevised plot\n\n\n\nBy narrowing our focus to compare only the top and bottom 5 schools, we are able to clearly infer the actual difference between the “best” and “worst” schools.\nThe new revised plot is now more visually compact, and allows readers to quickly examine the difference in performance between the top and bottom 5 schools.\n\n\nBesides this, we can also examine the differences in performance between Male and Female students in these top and bottom 5 schools, using the code below.\n\n\nShow the code\n# Convert ST004D01T to a factor with more meaningful level names\ntop_bottom_mathsch$ST004D01T &lt;- factor(top_bottom_mathsch$ST004D01T, levels = c(1, 2), labels = c(\"Girls\", \"Boys\"))\n\n# Plot\nmath_gender_ridges &lt;- ggplot(top_bottom_mathsch, \n                             aes(x = PV1MATH, \n                                 y = interaction(CNTSCHID, ST004D01T),  # Create interaction between school ID and gender\n                                 fill = after_stat(x))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE\n  ) +\n  scale_fill_gradientn(\n    name = \"Math Scores\",\n    colors = c(\"red\", \"green\"),\n    limits = c(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE)),\n    breaks = seq(min(top_bottom_mathsch$PV1MATH, na.rm = TRUE), max(top_bottom_mathsch$PV1MATH, na.rm = TRUE), length.out = 5)\n  ) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  theme_ridges() +\n  labs(y = \"School ID and Gender\", x = \"Math Scores\", title = \"Math Scores for the Top and Bottom 5 Schools by Gender\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 15),\n    axis.text.y = element_text(angle = 0, hjust = 0.5)  \n  )\n\nmath_gender_ridges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSchool IDs 70200001, 70200139 and 70200110 does not have any data on Female students and are likely a single-gender school."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#loading-r-packages",
    "href": "R-ex/R-Ex1/R_Ex1.html#loading-r-packages",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.1 Loading R packages",
    "text": "1.1 Loading R packages\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#importing-pisa-data",
    "href": "R-ex/R-Ex1/R_Ex1.html#importing-pisa-data",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.2 Importing PISA data",
    "text": "1.2 Importing PISA data\nThe code chunk below uses ‘read_sas()’ of haven to import PISA data into the R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\") \n\nUpon first import, as the student questionaire data file contains data from other countries, we will use filter() to filter the data file to only Singapore data.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%   \n  filter(CNT == \"SGP\")\n\nWe use write_rds() to save the filtered datafile to a seperate file called stu_qqq_SG\n\nwrite_rds(stu_qqq_SG,           \n          \"data/stu_qqq_SG.rds\")\n\nFor our analysis we shall read in data from stu_qqq_SG.rds using read_rds()\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#installing-and-loading-r-instvy",
    "href": "R-ex/R-Ex1/R_Ex1.html#installing-and-loading-r-instvy",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3 Installing and Loading R instvy",
    "text": "1.3 Installing and Loading R instvy\nThe R package intsvy allows R users to analyse PISA data among other international large-scale assessments. The use of PISA data via R requires data preparation, and intsvy offers a data transfer function to import data available in other formats directly into R. Intsvy also provides a merge function to merge the student, school, parent, teacher and cognitive databases.\nTo understand more about the packages available and the methodology to analyse the PISA data files, please refer to this link.\nThe analytical commands within intsvy enables users to derive mean statistics, standard deviations, frequency tables, correlation coefficients and regression estimates.\nAdditionally, intsvy deals with the calculation of point estimates and standard errors that take into account the complex PISA sample design with replicate weights, as well as the rotated test forms with plausible values.\nTo understand more about the instvy package, please refer to this link.\n\ninstall.packages(\"intsvy\",repos = \"http://cran.us.r-project.org\")\n\nWe will load the package using library()\n\nlibrary(\"intsvy\")"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#note",
    "href": "R-ex/R-Ex1/R_Ex1.html#note",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Note",
    "text": "Note\nFrom our initial checks, we noted that there are several variables with missing values.\nRather than checking and deleting all missing values, we will continue to maintain the original data file.\nThis will enable us to use several functions in the instvy package to compute and derive statistics from variables like Plausible Values. Subsequently we will prepare subsets from the data file for each EDA visualization.\n\n\n\n\n\n\nImportant\n\n\n\nThe columns in the PISA data set are named in a specific format. For more information on what each Variable means and how it is derived or calculated, please refer to the questionnaire or code book at this link.\nVariable values are both continuous and discrete."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#working-with-plausible-values",
    "href": "R-ex/R-Ex1/R_Ex1.html#working-with-plausible-values",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.2.1 Working with Plausible Values",
    "text": "1.2.1 Working with Plausible Values\nPISA reports student performance through plausible values (PVs), obtained from Item Response Theory models (for details, see Chapter 5 of the PISA Data Analysis Manual: SAS or SPSS, Second Edition or the associated guide “Scaling of Cognitive Data and Use of Students Performance Estimates”).\nAn accurate and efficient way of measuring proficiency estimates in PISA requires five steps:\n\nCompute estimates for each Plausible Values (PV)\nCompute final estimate by averaging all estimates obtained from (1)\nCompute sampling variance (unbiased estimate are providing by using only one PV)\nCompute imputation variance (measurement error variance, estimated for each PV and then average over the set of PVs)\nCompute final standard error by combining (3) and (4)\n\nFor more information, please refer to this link.\nFor example, in order to obtain single mean scores in Math, Reading and Science for the student cohort, we can use the pisa.mean.pv() function from 'instvy' package like below.\n\nMath_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", data=stu_qqq_SG)  \nRead_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", data=stu_qqq_SG)  \nSCIE_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", data=stu_qqq_SG)  \n\nBelow is an example of a single mean score for Math for the student cohort.\n\nprint(Math_mean_SG) \n\n  CNT Freq   Mean s.e.    SD  s.e\n1 SGP 6606 574.66 1.23 102.8 0.91\n\nprint(Read_mean_SG) \n\n  CNT Freq   Mean s.e.     SD  s.e\n1 SGP 6606 542.55 1.87 105.89 1.15\n\nprint(SCIE_mean_SG)\n\n  CNT Freq   Mean s.e.    SD s.e\n1 SGP 6606 561.43 1.33 99.09 1.1\n\n\nThese mean scores values can be corroborated at the following link (pages 310-315).\nFor a visual of the past performance of the mean scores for Singaporean students, please refer to this link.\nWe will use these calculated mean values from the 10 plausible values as a statistic (Mean) for some of our visualizations."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#distribution-of-students-performance-in-math-reading-and-science",
    "href": "R-ex/R-Ex1/R_Ex1.html#distribution-of-students-performance-in-math-reading-and-science",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.1 Distribution of student’s performance in Math, Reading and Science",
    "text": "1.3.1 Distribution of student’s performance in Math, Reading and Science\nThe objective of this visualization is to examine if subject scores are normally distributed in general within the student population sampled in the PISA test.\nSince there are 10 Plausible Values for the 3 subjects, we shall use the first plausible value, PV1 to visualize the distribution of scores for the subjects.\nFor an example of precedence of using only one Plausible Value, please refer to the article on “How to deal with Plausible Values from International Large-scale assessments.”\nWe will use the below code to plot our histograms to show the distribution of scores across subjects.\n\n\nShow the code\n# Create the histogram plot with an annotated mean line using Math_mean_SG \nplt1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +   \n  geom_histogram(binwidth = 20, color = \"white\", fill='lightblue') +   \n  labs(x = \"PV1 Math Score\",        \n       y = \"Frequency\") +   \n  geom_vline(xintercept = Math_mean_SG$Mean,              \n             col = 'black',              \n             size = 0.5,              \n             linetype = \"dashed\") +   \n  geom_text(aes(x = Math_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2))),             \n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()  \n\n\n# Create the histogram plot with an annotated mean line using Read_mean_SG \nplt2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +   \n  geom_histogram(binwidth = 20, color = \"white\", fill='lightgreen') +   \n  labs(x = \"PV1 Reading Score\",        \n       y = \"Frequency\") +   \n  geom_vline(xintercept = Read_mean_SG$Mean,              \n             col = 'black',              \n             size = 0.5,              \n             linetype = \"dashed\") +   \n  geom_text(aes(x = Read_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2))),             \n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position   \n  theme_minimal()   \n\n# Create the histogram plot with an annotated mean line using Science_mean_SG \n\nplt3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +   \n  geom_histogram(binwidth = 20, color = \"white\", fill='lightpink') +   \n  labs(x = \"PV1 Science Score\",        \n       y = \"Frequency\") +   \n  geom_vline(xintercept = SCIE_mean_SG$Mean,              \n             col = 'black',              \n             size = 0.5,              \n             linetype = \"dashed\") +   \n  geom_text(aes(x = SCIE_mean_SG$Mean, y = 100, \n                label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2))),             color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position   \n  theme_minimal()  \n\n\n# Create a single plot with density plots for Math, Reading, and Science scores \n\nplt4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +   \n  geom_density(alpha = 0.5) +   \n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +   \n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +   labs(x = \"Scores\",        \n            y = \"Density\") +   \n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +   \n  guides(fill = FALSE) +  # Remove the legend   \n  theme_minimal() \n\n\n\nCombined Visual of the distribution of scores in general\nWe will use patchwork to create a composite plot.\n\n\nShow the code\nlibrary(patchwork)  \n\npatch1 &lt;- (plt1+plt2) / (plt3+plt4)  +                \n  plot_annotation(                 \n    title = \"Distribution of student performance in Math, Reading and Science\")  \n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-1",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-1",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 1",
    "text": "Observation 1\nThe distribution of scores seem to resemble a normal distribution across all 3 subjects. Singaporean students seem to have a higher mean score In Mathematics relative to Reading and Science.\nFurther statistical tests like the Anderson-Darling or Shapiro-Wilk tests will need to be conducted to confirm the normality in distribution."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-schools",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-schools",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.2 Relationship between Scores and Schools",
    "text": "1.3.2 Relationship between Scores and Schools\nThe objective of this visualization is to examine the relationship between subject scores and the schools sampled in the PISA test.\nWe will use unique() and length() to obtain the number of unique schools in the data set.\n\nunique_values &lt;- unique(stu_qqq_SG$CNTSCHID)  \n\nlength(unique_values)\n\n[1] 164\n\n\nThere are 164 unique schools in this data set.\nNext, we use the code below to plot our scatter plots.\n\n\nShow the code\np1 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1MATH)) +   \n  geom_point(color = \"lightblue\", alpha = 0.5) +   \n  geom_hline(yintercept = Math_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +     \n  annotate(\"text\", x = Inf, y = Math_mean_SG$Mean, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2)),             \n           hjust = 1, vjust = -1) +     \n  labs(x = \"School ID\",        \n       y = \"PV1 Math Score\") +   \n  theme_minimal()  \n\np2 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1READ)) +   \n  geom_point(color = \"lightgreen\", alpha = 0.5) +   \n  geom_hline(yintercept = Read_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +     \n  annotate(\"text\", x = Inf, y = Read_mean_SG$Mean, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2)),             \n           hjust = 1, vjust = -1) +     \n  labs(x = \"School ID\",        \n       y = \"PV1 Reading Score\") +   \n  theme_minimal()  \n\np3 &lt;- ggplot(stu_qqq_SG, aes(x = CNTSCHID, y = PV1SCIE)) +   \n  geom_point(color = \"lightpink\", alpha = 0.5) +   \n  geom_hline(yintercept = SCIE_mean_SG$Mean, color = \"black\", linetype = \"dashed\") +     annotate(\"text\", x = Inf, y = SCIE_mean_SG$Mean, label = paste(\"Mean =\",round(SCIE_mean_SG$Mean, 2)),             \n                                                                                                  hjust = 1, vjust = -1) +     \n  labs(x = \"School ID\",        \n       y = \"PV1 Science Score\") +   \n  theme_minimal() \n\n\n\nCombined Visual of the distribution of scores across Schools\nWe will use patchwork to create a composite plot.\n\n\nShow the code\npatch2 &lt;- p1/p2/p3 +                \n  plot_annotation(                 \n    title = \"Students seem to be performing equally across Schools\")  \n\npatch2 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-2",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-2",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 2",
    "text": "Observation 2\nAcross the 164 schools, students seem to be performing equally across the 3 subjects. There are no significant clusters that are different from each other, for example, a large number of schools which only have good scores or only poor scores.\nAdditional analysis could be done to examine if the highest and lowest performing students (in terms of scores) belong to the same type of schools.\nSince this data set only contains students on students, there is no additional information on either the type of school or its resources.. Further analysis could incorporate other data sets to build a more complete analysis."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-gender",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-gender",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.3 Relationship between Scores and Gender",
    "text": "1.3.3 Relationship between Scores and Gender\nThe objective of this visualization is to examine the relationship between subject scores and gender within the students sampled in the PISA test.\nThe gender column of the data set is named as “ST004D01T” with values of 1=Female and 2=Male.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by gender.\nIn the code below, we will create separate tables for the mean scores for each subject by different genders.\n\n#| code-fold: true \n#| code-summary: \"Show the code\"  \n\nMath_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"ST004D01T\", data = stu_qqq_SG)  \n\nMath_gender$ST004D01T &lt;- factor(Math_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))  \n\nRead_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"ST004D01T\", data = stu_qqq_SG)  \n\nRead_gender$ST004D01T &lt;- factor(Read_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))  \n\nSCIE_gender &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"ST004D01T\", data = stu_qqq_SG)  \n\nSCIE_gender$ST004D01T &lt;- factor(SCIE_gender$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))   \n\nBelow is how the new tables look like for mean Math score grouped by gender. We can use the mean statistic here as an additional statistic for our box plots.\n\nprint(Math_gender) \n\n  ST004D01T Freq   Mean s.e.     SD  s.e\n1    Female 3248 568.49 1.65  97.62 1.14\n2      Male 3358 580.59 1.75 107.20 1.33\n\n\nNext, we plot the PV1 scores by different genders to examine the performance of different genders across subjects.\n\n\nShow the code\n# Create a subset of the data with gender and PV1 score columns \nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%   \n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)  \n\n# Convert the \"ST004D01T\" column to a factor  \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))  \n\n# Create the plot using the subset_data \n\nbxp1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +   \n  geom_boxplot() +   \n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +   \n  geom_text(data = Math_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)),              \n            color = \"black\", vjust = -0.5, size = 3.5) +   \n  labs(x = \"Gender\",        \n       y = \"PV1 Math Score\") +   \n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +     theme_minimal() +   \n  theme(legend.position = \"none\")  \n\n# Remove the legend   \n\nbxp2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +   \n  geom_boxplot() +   \n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +   \n  geom_text(data = Read_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)),              \n            color = \"black\", vjust = -0.5, size = 3.5) +   \n  labs(x = \"Gender\",        \n       y = \"PV1 Reading Score\") +   \n  scale_fill_manual(values = c(\"lightgreen\", \"lightgreen\")) +  # Associate colors with factor levels   \n  theme_minimal() +   \n  theme(legend.position = \"none\")  \n\n# Remove the legend  \n\nbxp3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +   \n  geom_boxplot() +   \n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"blue\", size = 1.5) +   \n  geom_text(data = SCIE_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)),              \n            color = \"black\", vjust = -0.5, size = 3.5) +   \n  labs(x = \"Gender\",        \n       y = \"PV1 Science Score\") +   \n  scale_fill_manual(values = c(\"lightpink\", \"lightpink\")) +  # Associate colors with factor levels   \n  theme_minimal() +   \n  theme(legend.position = \"none\")  # Remove the legend \n\n\n\nCombined Visual of Performance across Genders\nWe will use the code below to create a composite plot for our box plots.\n\n\nShow the code\npatch3 &lt;- bxp1 + bxp2 + bxp3 +                \n  plot_annotation(                 \n    title = \"Male students outperform in Maths and Science\")  \n\npatch3 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-3",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-3",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 3",
    "text": "Observation 3\nMale students seem to outperform Female students in both Maths and Science with mean scores of 580.59 and 564.81 respectively. Female students seem to outperform Male students in Reading with a mean score of 552.55."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-socioeconomic-status-of-students",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-socioeconomic-status-of-students",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.4 Relationship between Scores and Socioeconomic status of students",
    "text": "1.3.4 Relationship between Scores and Socioeconomic status of students\nThe socioeconomic status of students is represented by the “ESCS” score in the PISA data set. The ESCS score is a continuous variable and is calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS). A higher ESCS score translates to a “better-off” student.\nFurther break down on the 3 main components of the ESCS score is shown in the diagram below. For further information on the computation methodology, please refer to the PISA 2022 Technical report: Chapter 19.\n\n\n\nComputation of ESCS in PISA 2022\n\n\nThe objective of this visualization is to examine the relationship between subject scores and a student’s socioeconomic status.\nFirst we check for any missing values in the ESCS column using the code below.\n\n# Check for NAs in the 'ESCS' column \nhas_nas &lt;- any(is.na(stu_qqq_SG$ESCS))  \nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the ESCS column, we shall delete the rows with missing ESCS values. We will create a new subset with ESCS and the PV1 scores for this visualization.\n\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%   \n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)  \n\n#omiting NA values \nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\nUsing our new table subset_ESCS_PV1, we will create scatter plots for ESCS versus each PV1 score for each subject using the code below.\n\n\nShow the code\nc_coeff_ESCS_Math &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1MATH)  \n\nC_plt1 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1MATH)) +   \n  geom_point(color = \"lightblue\") +   \n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +   \n  geom_text(     \n    x = max(subset_ESCS_PV1$ESCS),       \n    y = max(subset_ESCS_PV1$PV1MATH),       \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Math, 2)),     \n    hjust = 1,  # Adjust horizontal justification     \n    vjust = 1   # Adjust vertical justification   \n    ) +   \n  labs(x = \"Socio-Economic Status (ESCS)\",        \n       y = \"PV1 Math Score\") +   \n  theme_minimal()  \n\nc_coeff_ESCS_Read &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1READ)  \n\nC_plt2 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1READ)) +  \n  geom_point(color = \"lightgreen\") +   \n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +   \n  geom_text(     \n    x = max(subset_ESCS_PV1$ESCS),       \n    y = max(subset_ESCS_PV1$PV1READ),       \n    label = paste(\"Corr Coeff:\", \n                  round(c_coeff_ESCS_Read, 2)),     \n    hjust = 1,  # Adjust horizontal justification     \n    vjust = 1   # Adjust vertical justification   \n    ) +   labs(x = \"Socio-Economic Status (ESCS)\",        \n               y = \"PV1 Read Score\") +   \n  theme_minimal()  \n\nc_coeff_ESCS_Scie &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1SCIE)  \n\nC_plt3 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1SCIE)) +   \n  geom_point(color = \"lightpink\") +   \n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +   \n  geom_text(     \n    x = max(subset_ESCS_PV1$ESCS),       \n    y = max(subset_ESCS_PV1$PV1SCIE),       \n    label = paste(\"Corr Coeff:\", \n                  round(c_coeff_ESCS_Scie, 2)),     \n    hjust = 1,  # Adjust horizontal justification     \n    vjust = 1   # Adjust vertical justification   \n    ) +   labs(x = \"Socio-Economic Status (ESCS)\",        \n               y = \"PV1 Science Score\") +   \n  \n  theme_minimal() \n\n\n\nCombined Scatter plots of PV1 Scores Vs ESCS scores\nWe will use patchwork to create a composite plot for our scatter plots.\n\n\nShow the code\npatch4 &lt;- C_plt1 / C_plt2 / C_plt3 +                \n  plot_annotation(                 \n    title = \"Weak positive relationship between Scores and ESCS\")  \n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-4",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-4",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 4",
    "text": "Observation 4\nThere is a weak positive relationship between subject scores and Socioeconomic statuses. The ESCS score is a composite score calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS). It could be likely that the larger number of constituents has ‘diluted’ the score, where the effect is more prominent for developed countries like Singapore.\n\nFurther analysis could be conducted on the individual components of the ESCS score to check for their individual influence on student performance."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-the-years-of-education-for-parents",
    "href": "R-ex/R-Ex1/R_Ex1.html#relationship-between-scores-and-the-years-of-education-for-parents",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.5 Relationship between Scores and the years of Education for Parents",
    "text": "1.3.5 Relationship between Scores and the years of Education for Parents\nAs highlighted in the previous analysis, there is a weak positive relationship between student scores and ESCS scores. The objective of this visualization is to examine the relationship between one of the constituents, PAREDINT and student scores.\nPAREDINT is the index of the highest education of parents in years, based on the median cumulative years of education completed. The variable values are discrete and ranges from a scale of 3 to 16 years. For more information on this variable, please refer to the PISA 2022 Technical report: Chapter 19.\nFirst we check for any missing values in the ESCS column using the code below.\n\n# Check for NAs in the 'PAREDINT' column \nhas_nas &lt;- any(is.na(stu_qqq_SG$PAREDINT))  \nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the PAREDINT column, we shall delete the rows with missing PAREDINT values. We will then create a new table with PAREDINT and the Mean scores of the 10 Plausible Values for this visualization.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the years of Parents education (PARENDINT).\nIn the code below, we will create separate tables for the mean scores for each subject by different years of Education.\n\n\nShow the code\nParents_edu_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"PAREDINT\", data = stu_qqq_SG)  \n\nParents_edu_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"PAREDINT\", data = stu_qqq_SG)  \n\nParents_edu_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"PAREDINT\", data = stu_qqq_SG) \n\n\nWe examine the one of the new tables, Parents_edu_math.\n\nprint(Parents_edu_math)\n\n  PAREDINT Freq   Mean  s.e.     SD   s.e\n1        3    8 482.25 53.59 131.99 25.91\n2        6   62 500.76 12.97 102.52  9.03\n3        9  127 540.05  8.87  98.63  6.43\n4       12 1470 530.62  3.14 100.32  1.75\n5     14.5 1213 559.85  3.00  97.10  1.95\n6       16 3669 600.47  1.68  97.13  1.37\n7     &lt;NA&gt;   57 485.92 11.61  85.05  9.59\n\n\nSince there 57 rows with missing values, we will delete the rows with missing values.\n\nParents_edu_math &lt;- na.omit(Parents_edu_math) \nParents_edu_read &lt;- na.omit(Parents_edu_read) \nParents_edu_scie &lt;- na.omit(Parents_edu_scie)\n\nNext, we use the below code to plot dot plots for each subject.\n\n\nShow the code\n# Create a dot plot with annotations \n\nDp1 &lt;- ggplot(Parents_edu_math, aes(x = as.factor(PAREDINT), y = Mean)) +   geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +   geom_line(aes(group = 1), color = \"lightblue\", size = 1, alpha = 0.5) +   \n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels   \n  labs(title = \"Math\",        \n       x = \"Education (Yrs)\")+    \n  theme_minimal()   \n\nDp2 &lt;- ggplot(Parents_edu_read, aes(x = as.factor(PAREDINT), y = Mean)) +   geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +   geom_line(aes(group = 1), color = \"lightgreen\", size = 1, alpha = 0.5) +   geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels   \n  labs(title = \"Reading\",        \n       x = \"Education (Yrs)\")+    \n  theme_minimal()  \n\nDp3 &lt;- ggplot(Parents_edu_scie, aes(x = as.factor(PAREDINT), y = Mean)) +   geom_dotplot(binaxis = \"y\", stackdir = \"center\", fill = \"black\", alpha = 0.7) +   geom_line(aes(group = 1), color = \"lightpink\", size = 1, alpha = 0.5) +   \n  geom_text(aes(label = Mean), vjust = -0.5, color = \"black\", size = 3) +  # Add text labels   \n  labs(title = \"Science\",        \n       x = \"Education (Yrs)\")+    \n  theme_minimal() \n\n\n\nCombined dot plots of Subject Mean Scores Vs Parents Education years\nWe will use the code below to create a composite plot.\n\n\nShow the code\npatch4 &lt;- (Dp1 + Dp2 + Dp3                \n           ) +                \n  plot_annotation(                 \n    title = \"Parents with more education years seem to have children with higher scores\")  \n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-5",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-5",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 5",
    "text": "Observation 5\nIn general, students seem to have performed better across all subjects the more their parents have been educated. However this factor alone is likely insufficient to cause a better performance.\nAlso, there is a slight drop off in mean scores from 9-12 years of Parents Education\nAdditional analyses taking into account the state of the study environment, both at home and in school, as well as the emotional aspects and motivation of students could be further analysed to derive more complete insights on the factors that could influence performance."
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#additional-eda",
    "href": "R-ex/R-Ex1/R_Ex1.html#additional-eda",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "1.3.6 Additional EDA",
    "text": "1.3.6 Additional EDA\n\n1) Examining closer into Mean scores per School\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the School ID (CNTSCHID).\nIn the code below, we will create separate tables for the mean scores for each subject by different School Ids.\n\n\nShow the code\nSchoolid_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), \n                  by = \"CNTSCHID\", data = stu_qqq_SG)  \n\nSchoolid_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), \n                  by = \"CNTSCHID\", data = stu_qqq_SG)  \n\nSchoolid_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), \n                  by = \"CNTSCHID\", data = stu_qqq_SG)\n\n\nWe examine the new tables created. In these new tables we are able to extract the number of students and the mean scores per School.\n\nhead(Schoolid_math) \n\n  CNTSCHID Freq   Mean  s.e.    SD   s.e\n1 70200001   55 725.21  9.34 59.23  6.38\n2 70200002   38 534.22 16.75 89.96 13.84\n3 70200003   36 739.92 12.30 59.23  7.70\n4 70200004   56 509.61 12.84 86.63  7.71\n5 70200005   38 546.52 12.95 86.04  8.86\n6 70200006   36 485.30 13.90 76.47  8.86\n\n\nNext, we use the below code to plot bubble plots to examine the number of students and their mean scores for each school. We will also use the plotly package for added interactivity.\n\n\nShow the code\nlibrary(ggplot2) \nlibrary(plotly)  \n\np_1 &lt;- ggplot(Schoolid_math, aes(x = CNTSCHID, y = Mean)) +   \n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +   \n  scale_size_area(max_size = 10) +   \n  scale_color_gradient(low = \"skyblue\", high = \"darkblue\") +   \n  labs(title = \"Mean Math Scores by School ID\",     \n       y = \"Mean Math Scores\",      \n       size = \"Number of Students\",      \n       color = \"Number of Students\") +   \n  theme_minimal() +   \n  theme(axis.text.x = element_blank(),         \n        axis.ticks.x = element_blank(),         \n        axis.title.x = element_blank(),         \n        panel.grid.major = element_blank(),  # Remove major grid lines         \n        panel.grid.minor = element_blank())  # Remove minor grid lines  \n\n# Convert to an interactive plot \n\nggplotly(p_1, tooltip = c(\"x\", \"y\", \"size\", \"color\"))  \n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_2 &lt;- ggplot(Schoolid_read, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"yellow\", high = \"darkorchid\") +\n  labs(title = \"Mean Reading Scores by School ID\",\n    y = \"Mean Reading Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_2, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(plotly)\n\np_3 &lt;- ggplot(Schoolid_scie, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"lightpink\", high = \"darkred\") +\n  labs(title = \"Mean Science Scores by School ID\",\n    y = \"Mean Science Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Convert to an interactive plot\nggplotly(p_3, tooltip = c(\"x\", \"y\", \"size\", \"color\"))"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-6",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-6",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 6",
    "text": "Observation 6\nThe ability to extract and assign Mean scores to individual schools enables us to further explore and examine the disparity in performance between schools. For example, looking at the two extremes of score results, we note that Schools (70200001 & 70200003) out perform other schools in all subjects. On the other hand, Schools (7020115 & 70200149) under perform other schools in all subjects.\n\nThis shows that there are still marked differences between the ‘’best” schools and the’‘worst’’ schools. Additional analysis should be done to identify the differences between these two sets of schools in terms of resources, teaching quality, and students attitudes or motivation to fully understand the differences between the scores.\n\n2) Examining the Breakdown of scores per Subject\nPreviously we had only examined the distribution of marks for the student population across the 3 subjects (whether resembles normal distribution).\nWe can further examine the percentage of students per score range for each subject. This might help us examine whether there are specific strengths or weaknesses in the student cohort.\nFirst, we use the pisa.ben.pv() function from the instvy package which calculates student scores from the 10 plausible values and calculates the percentage of students at each proficiency level (Score range) as defined by PISA.\nIn the code below, we will create separate tables for the percentage breakdown of scores for each subject.\n\n\nShow the code\nMath_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nRead_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nScie_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\n\nWe examine the new tables created. In these new tables we are able to obtain the percentage breakdown of students per score range for our next visualization.\n\nprint(Math_Breakdown)\n\n  CNT       Benchmarks Percentage Std. err.\n1 SGP        &lt;= 357.77       2.17      0.22\n2 SGP (357.77, 420.07]       5.85      0.38\n3 SGP (420.07, 482.38]      11.25      0.59\n4 SGP (482.38, 544.68]      17.59      0.61\n5 SGP (544.68, 606.99]      22.62      0.69\n6 SGP  (606.99, 669.3]      21.96      0.69\n7 SGP          &gt; 669.3      18.56      0.52\n\n\nWe will plot bar charts for the percentage of students per Score range for each subject.\n\nMath Scores breakdownReading Scores breakdownScience Scores breakdown\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Math_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Math\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Read_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Reading\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(forcats)\n\nggplot(Scie_Breakdown, aes(x = Percentage, y = fct_reorder(Benchmarks, Percentage))) +\n  geom_bar(stat = \"identity\", fill = \"lightpink\", color = \"black\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", Percentage)), # This adds the labels\n            position = position_stack(vjust = 0.5), # Adjust vertical position\n            color = \"black\", # Text color\n            size = 3.5) + # Text size, adjust as needed\n  labs(\n    title = \"Distribution of Scores for Science\",\n    x = \"Percentage of students\",\n    y = \"Score ranges\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we can combine these tables and plots into one plot to show the percentage of students per Score Range for all subjects.\n\n\nShow the code\n# Creating a new combined table\n\nMath_Breakdown$Subject &lt;- 'Math'\nRead_Breakdown$Subject &lt;- 'Reading'\nScie_Breakdown$Subject &lt;- 'Science'\n\nCombined_Breakdown &lt;- bind_rows(Math_Breakdown, Read_Breakdown, Scie_Breakdown)\n\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_Breakdown &lt;- Combined_Breakdown %&gt;%\n  mutate(Benchmarks = fct_inorder(Benchmarks))\n\n# Now plot using ggplot\np &lt;- ggplot(Combined_Breakdown, aes(x = Benchmarks, y = Percentage, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Dodge position for the bars\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percentage)),  # This will format the label to have 1 decimal place and a percentage sign\n    position = position_dodge(width = 0.9),  # Match the position of the text with the dodged bars\n    vjust = -0.25,   \n    size = 2  \n  ) +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"For lower Score ranges, students seem to do better in Reading\",\n       x = \"Score Range\",\n       y = \"Percentage of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to interactive plotly object\np_interactive &lt;- ggplotly(p, tooltip = c(\"x\", \"y\", \"fill\", \"text\"))\n\n# Print the interactive plot\np_interactive"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-7",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-7",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 7",
    "text": "Observation 7\n1) 63.14 % of students scored 544.68 and above for Math\n2) 52.45% of students scored 544.68 and above for Reading\n3) 59.65 % of students scored 544.68 and above for Science\n\nIf we were to select a baseline score as 544.68, we can see that students generally do better in Math relative to Science and Reading.\nWhile individual plots are useful to visualize the break down of students across scores, we can also combine them into a single bar plot to obtain insights on relative performance. From the combined plot, we can see that at lower score ranges, students seem to do better in Reading relative to Math and Science. However, at higher score ranges, students do worse in Reading relative to Math and Science.\n\n3) Violin plots for Gender performance across subjects\nPreviously we had used box plots to visualize the difference in performance between the genders across subjects. The code below ‘switches’ to violin plots instead.\n\n\nShow the code\n# Create the plot using the subset_data\nvx_plot1 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1MATH, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = Math_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = Math_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Math Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightblue\", \"Male\" = \"lightblue\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\nvx_plot2 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1READ, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = Read_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = Read_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Reading Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightgreen\", \"Male\" = \"lightgreen\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\nvx_plot3 &lt;- ggplot(subset_gender_PV1, aes(x = ST004D01T, y = PV1SCIE, fill = ST004D01T)) +\n  geom_violin(trim = FALSE) + # Use geom_violin instead of geom_boxplot\n  geom_point(data = SCIE_gender, aes(x = ST004D01T, y = Mean), color = \"black\", size = 1.5) +\n  geom_text(data = SCIE_gender, aes(x = ST004D01T, y = Mean, label = round(Mean, 2)), \n            color = \"black\", vjust = -0.5, size = 3.5) +\n  labs(x = \"Gender\", y = \"PV1 Science Score\") +\n  scale_fill_manual(values = c(\"Female\" = \"lightpink\", \"Male\" = \"lightpink\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") # Remove the legend\n\n\nWe will use the code below to do a composite plot for all 3 subjects\n\n\nShow the code\npatch5 &lt;- vx_plot1 + vx_plot2 + vx_plot3 + \n              plot_annotation(\n                title = \"Male students outperform in Maths and Science\")\n\npatch5 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#note-1",
    "href": "R-ex/R-Ex1/R_Ex1.html#note-1",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Note",
    "text": "Note\nViolin plots may provide a fuller picture of the data distribution, revealing density and multimodality that box plots may obscure. This could be useful for detecting data patterns and clusters.\nViolin plots could also emphasize the concentration of data points and potential outliers, showcasing the entire range of the data including peaks, valleys, and tails that may not be evident from box plots.\n\n4) Relationship between Scores and the times spent on studying or homework before or after school\nThe objective of this visualization is to examine the relationship between student scores and the time spent on studying or on homework (STUDYHMW).\nThe variable “STUDYHMW” measures how many times during a typical school week students studied for school or homework before going to school and/or after leaving school. Values on this index range from 0 (no studying) to 10 (10 or more times of studying per week).\nFirst we check for any missing values in the STUDYHMW column using the code below.\n\n# Check for NAs in the 'STUDYHMW' column\nhas_nas &lt;- any(is.na(stu_qqq_SG$STUDYHMW))\n\nprint(has_nas)\n\n[1] TRUE\n\n\nSince there are missing values in the STUDYHMW column, we shall delete the rows with missing values. We will then create a new table with STUDYHMW and the Mean scores of the 10 Plausible Values for this visualization.\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the time spent on studying (STUDYHMW).\nIn the code below, we will create separate tables for the mean scores for each subject.\n\n\nShow the code\nhomework_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\nhomework_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\nhomework_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"STUDYHMW\", data = stu_qqq_SG)\n\n\nWe examine the new tables created.\n\nprint(homework_math)\n\n   STUDYHMW Freq   Mean  s.e.     SD  s.e\n1         0  274 514.96  7.21 111.29 4.17\n2         1  154 537.23  9.42 112.48 6.16\n3         2  422 540.72  5.89 110.16 3.82\n4         3  500 566.21  4.89 105.07 3.27\n5         4  602 569.18  4.54 104.79 3.12\n6         5 1750 616.13  2.13  90.56 1.64\n7         6  568 551.80  4.29 103.99 3.02\n8         7  415 577.30  4.96  97.93 3.69\n9         8  416 561.92  4.83  94.31 3.86\n10        9  163 554.53  8.07  91.50 5.98\n11       10 1296 570.87  2.45  95.76 2.11\n12     &lt;NA&gt;   46 486.26 11.65  77.81 8.28\n\n\nSince there 46 rows with missing values, we will delete the rows with missing values.\n\nhomework_math &lt;- na.omit(homework_math) \nhomework_read &lt;- na.omit(homework_read) \nhomework_scie &lt;- na.omit(homework_scie)\n\nNext, we can combine these tables to be able to plot one graph to show the mean scores across the time spent on homework and studying, for all subjects.\n\nhomework_math$Subject &lt;- 'Math'\nhomework_read$Subject &lt;- 'Reading'\nhomework_scie$Subject &lt;- 'Science'\n\nCombined_homework &lt;- bind_rows(homework_math, homework_read, homework_scie)\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_homework &lt;- Combined_homework %&gt;%\n  mutate(STUDYHMW = fct_inorder(STUDYHMW))\n\n# Now plot using ggplot\np2 &lt;- ggplot(Combined_homework, aes(x = STUDYHMW, y = Mean, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"Students who study more may not necessarily score higher\",\n       x = \"Number of times in a week\",\n       y = \"Mean Scores\") +\n  theme_minimal() \n\n# Convert to interactive plotly object\np_interactive2 &lt;- ggplotly(p2, tooltip = c(\"x\", \"y\", \"fill\", \"text\"))\n\n# Print the interactive plot\np_interactive2"
  },
  {
    "objectID": "R-ex/R-Ex1/R_Ex1.html#observation-8",
    "href": "R-ex/R-Ex1/R_Ex1.html#observation-8",
    "title": "Take Home Exercise 1 - Pisa’s global education survey data",
    "section": "Observation 8",
    "text": "Observation 8\nFrom the combined plot, it appears that the optimal number of times to study in a week is 5 times for all subjects. More times spent on homework and/or studying does not seem to translate to higher scores for this PISA test. However this factor alone is insufficient to conclude causality, and we should do additional analysis on other factors that may impact a student’s ability to prepare for this test."
  },
  {
    "objectID": "Python-ex/Python-Ex2/Python_Ex2.html",
    "href": "Python-ex/Python-Ex2/Python_Ex2.html",
    "title": "Designing a 3 -stock portfolio for a robo-advisor",
    "section": "",
    "text": "Overview\n\n\n\n\nImporting Modules\n\n\n\nWorking out the required monthly return rate to achieve target wealth\n\n\n\nRead and format the data\n\n\n\nGetting month-end dates and prices from data file\n\n\n\nGetting month-end dates and prices of analysis period\n\n\n\nGetting stock universe\n\n\n\nIdentify 3 stocks for portfolio construction\n\n\n\n\nAllocation of optimised portfolio\nDesign consideration for only 3 stocks chosen to form the portfolio. Based on 3 stock design and the min_weight variable that is assigned to each of the 3 stocks, the procedure to find the optimised portfolio will be as follows:\n\nMaximum weight to be distributed will be determined as 100% - (min weight * no. of stocks)\nThe weight will be randomly distributed over the 3 stocks, and the random distribution will be iterated over 10% of the total possible combinations possible based on max weight to be distributed and 3 stocks in portfolio\nAssumption that the random sample will be enough to get the optimal distribution and decision for the random sample is to alleviate the computational power required to calculate all possible combinations\nEach random distribution of weight will then be test by calculating the Sharpe ratio of the portfolio with that weight distribution\nThe results of the random test will be plotted on a graph to find the distribution with the highest Sharpe Ratio\n\n\n\n\n\n\n\n\n\n\nReporting the required results\n\n\n\nQuery on the portfolio value\n\n\n\n\nAppendix\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Imran’s Postgrad repository",
    "section": "",
    "text": "Welcome to my website.\nHere, you will find some of my post-graduate research, assignments and projects on Financial Technology and Data Analytics.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Article-ex/Article-Ex3/Article_Ex3.html",
    "href": "Article-ex/Article-Ex3/Article_Ex3.html",
    "title": "Assess and discuss how the role of financial regulators has evolved",
    "section": "",
    "text": "Assess and discuss how the role of financial regulators has evolved, as we now have both incumbent banks and fintech businesses providing the same financial services to the masses.\nMake good use of examples, statistics and events to augment and develop your discussion.\n\n\nUtilizing their expertise in new technologies, adopting innovative business models, and unencumbered with the same magnitude of regulation as banks; fintech businesses have flourished globally and introduced important solutions in payments, P2P lending, crowdfunding, consumer credit, insurance, online trading, and cryptocurrencies services.\nAs regulators adapt to these new types of financial firms and services, new concerns emerge including consumer protection, clarity and consistency of regulations, regulatory arbitrage, concentration risk and contagion, adequacy of existing financial safeguards, and threats to financial stability.\n\n\n\nFinancial regulators have had to balance between promoting innovation and fair competition, safeguard their financial systems, protect consumers; while also monitoring for illicit activities, and non- compliance to rules. In some economies, fintech growth was driven by an unmet demand for financial services, while in others, growth was driven by the high cost of financial services and the lack of competition. Although fintech has helped make the financial system become more inclusive and efficient, market failures and systematic risks remain relevant, and may substantially become worse if left unchecked.\nFinancial regulators have had to keep up with technology trends and anticipate their impact on financial stability. With tech companies becoming more involved in financial services, and with banks deploying new technologies and business models to stay competitive, regulators have had to engage both sides of the divide to understand new technologies and their impact to the financial system. Regulators must now additionally ensure that their policies and regulations can be applied fairly between these two segments to prevent any side from having an advantage over the other.\nThe role of financial regulators has evolved from merely being a supervisory body to being a progressive entity, actively initiating programs to engage innovative businesses and encourage financial innovation. Regulators like MAS and FCA provide regulatory advice and support, and construct testbeds or environments to actively test new financial products and services. By implementing these initiatives and becoming driving forces in these projects, they enable both banks and fintech firms to grow organically within the stipulated rules, bringing benefit to both their financial system and consumers.\nFor example, in Singapore, the MAS has initiatives in place to foster fintech collaboration such as fintech offices, accelerator programs and sandbox environments where fintech firms and financial institutions can come together to experiment on innovative products and solutions within a live environment [1]. The sandbox environment enables fintech firms and start-ups to test their concepts and technology in a live environment with limited impact to the financial system, thus enabling a viable assessment of their products before launch.\nFintech sandboxes aim to:\n1) Target new business models or technologies\n2) Provide an environment for testing and understanding new technology and methodologies\n3) Encompass specific objectives to resolve existing market frictions\n4) Provide a safe production environment to evaluate real-world results\nWith the entry of new financial products and fintech firms, regulators have also had to step up their capabilities in monitoring firms and their activities; and to understand the impact of these new products on the financial system and on consumers. To this end, regulators have had to bring in suitable talent, upgrade their technologies and methodologies to keep pace with new financial trends.\nRegTech provides technology to help manage regulatory processes within the financial industry. It refers to technology companies that provide technological solutions to help financial firms and regulators. Regulators globally have begun to engage RegTech companies to revamp their IT systems and processes. Functionalities include tools to match large volumes of data to decision rules, automating compliance or oversight processes and real-time monitoring of credit flows in the financial system.  \nIn a survey of global regulators by the World bank in 2019 [2], the top technologies employed by regulators include Machine Learning, Blockchain/DLT, Natural Language Processing, APIs, Cloud computing, Robotic Process Automation and Biometrics. It is interesting to note that regulators have evolved from being mere users of technology and legacy systems to actively deploying new fields of technology to find new applications and usage and keep up with new trends.  In the process, they fortify their capabilities to detect and anticipate any potential adverse impact to their financial systems.\nWhile the objectives of regulators are clear, the implementation of policies in the face of new technologies and financial firms can be challenging. According a 2019 report from the World Bank [2] regulators from 111 jurisdictions cited their top four impediments to effective supervision as:\n1) limited technical expertise\n2) limited funding and resources,\n3) unclear or limited jurisdiction over the activity; and\n4) coordinating multiple regulators to enforce regulations.\n\n\n\nMany Fintech firms operate borderless businesses which blur international jurisdictions. These firms extend their services globally, causing complex transaction monitoring for local regulators. For example, until recently, Binance, the world’s largest crypto exchange did not have an official headquarters. Similarly, big tech firms like Google, Apple and Amazon extend their services globally across multiple jurisdictions. The issue is exacerbated as some of these companies are outside the regulator’s jurisdiction. Regulation may not be uniformed across borders, hence making it difficult to formulate effective policies with other regulators.\nAnother challenge is the trend of bypassing traditional intermediaries. Different industry players like technology and telecommunications firms have emerged on the scene, providing financial services direct to consumers without collaborating with incumbent financial institutions. These companies may fall under the mandate of different regulators and this calls into question on whether there is a sufficient level of coordination and communication between regulatory bodies to be able to implement effective regulation. For example, in Singapore, Singtel, a telecommunications firm and Grab, a ride-hailing provider both provide payment services by means of a mobile e-wallet.\nIn Singapore, MAS has issued licenses to 4 digital banks[3] which include non-bank entities like Grab, Singtel, Sea Ltd and Ant Group. These businesses were assessed based on their value proposition, use of technology to improve financial inclusion and growth prospects. In this example, MAS has clearly adapted to changing trends and discarded notions of financial services only belonging to financial institutions. By assimilating these non-banks into their supervision umbrella, MAS also ensures that these new firms are subject to the same regulations as other banks, ensuring a level playing field in terms of scrutiny and compliance to rules.\n\n\n\nThe rate of adoption of Fintech and the potential for players to scale rapidly has also put pressure on regulators to prevent concentration and systematic risk. However, regulators may not necessarily have the full picture or may lack reliable information on the ground about the structure and operations of Fintech firms and their markets, imposing stringent rules that curb financial innovation instead of encouraging them.\nFor example, regulators in China introduced several measures aimed at fintech firms to curb perceived monopolistic behavior, risky lending practices and lack of protection for customer data.\nAs a result, fintech giant, Ant Group has been directed to restructure itself into a financial holding company where all its subsidiaries engaged in financial activities must be subjected to financial regulations under the PBOC. This will also require them to have paid-in capital of at least 50% of the total registered capital of all the financial institutions they own. The stricter capital requirements are expected to constraint Ant’s huge loans business [4].\nAnt Group has also been directed to remove its financial products Huabei and Jiebei from the Alipay platform and delinking them from the Alipay payments infrastructure. The delinking of Ant’s consumer credit and microloan business is expected to further weaken its value proposition of providing competitive financial services through its Alipay platform. Controls on minimum deposit amounts and limits on the investments allowed for users in Yu’e Bao, Ant’s money-market fund platform, has also pushed investors to move their funds away from the Alipay platform.\nThe recent regulation changes in China are expected to significantly increase compliance costs for fintech companies and lead to moderate fintech growth and investment in the sector. It is well worth noting that regulators in China have evolved from adopting a lenient and light-handed approach to a stricter and more stringent regime. However it is not surprising given its unfortunate experience with P2P lenders where lenient, wait-and-see approaches led to unsustainable growth, massive fraud and subsequent crash of the industry with unfortunate consequences to millions of retail customers[5].\nIn conclusion, the rapid pace of change and innovation have required regulators to be agile and adaptive to changing trends. Regulators have also had to balance supporting Fintech and disruptive technologies while also mitigating risks to financial integrity and stability. While some regulators have adopted a one-size-fits-all approach to regulate these firms, many more have adopted new frameworks and guidelines to regulate fintech firms both as a whole and individually. Regulators have also had to look outwards in their approaches by increasing collaboration internationally. They have also had to re-think their use of technology and re-design themselves to become more progressive in their approach of regulation.\n\n\n\n[1] Overview of Regulatory Sandbox. (n.d.). Retrieved September 27, 2022, from https://www.mas.gov.sg/development/fintech/regulatory-sandbox\n[2] World Bank and Cambridge Center for Alternative Finance. (2019) “Regulating Alternative Finance: Results from a Global Regulator Survey”. 2019 https://openknowledge.worldbank.org/handle/10986/32592\n[3] MAS Announces Successful Applicants of Licences to Operate New Digital Banks in Singapore. (n.d.). Retrieved September 30, 2022, from https://www.mas.gov.sg/news/media-releases/2020/mas-announces-successful-applicants-of-licences-to-operate-new-digital-banks-in-singapore\n[4] Writer, S. (2021, March 3). Ant Group thrown into disarray by China’s new financial rules. Nikkei Asia. Retrieved September 29, 2022, from https://asia.nikkei.com/Business/Finance/Ant-Group-thrown-into-disarray-by-China-s-new-financial-rules2\n[5] PYMNTS. Protests Mark China’s Ruptured P2P Lending Landscape. August, 2018. https://www.pymnts.com/news/international/2018/china-protestors-p2p-lending-regulationfraud-debt/\nWorld Bank. (2020). How regulators respond to Fintech: Evaluating the different approaches–sandboxes and beyond. World Bank. https://openknowledge.worldbank.org/handle/10986/33698\nde Koker, Morris, N., & Jaffer, S. (2019). Regulating financial services in an era of technological disruption. Law in Context (Bundoora, Vic.), 36(2), 90–112. https://doi.org/10.26826/law-in-context.v36i2.98\nFintech : The Experience so Far - Executive Summary (English). Washington, D.C. : World Bank Group. http://documents.worldbank.org/curated/en/130201561082549144/Fintech-The-Experience-so-Far-Executive-Summary"
  },
  {
    "objectID": "Article-ex/Article-Ex3/Article_Ex3.html#introduction",
    "href": "Article-ex/Article-Ex3/Article_Ex3.html#introduction",
    "title": "Assess and discuss how the role of financial regulators has evolved",
    "section": "",
    "text": "Utilizing their expertise in new technologies, adopting innovative business models, and unencumbered with the same magnitude of regulation as banks; fintech businesses have flourished globally and introduced important solutions in payments, P2P lending, crowdfunding, consumer credit, insurance, online trading, and cryptocurrencies services.\nAs regulators adapt to these new types of financial firms and services, new concerns emerge including consumer protection, clarity and consistency of regulations, regulatory arbitrage, concentration risk and contagion, adequacy of existing financial safeguards, and threats to financial stability."
  },
  {
    "objectID": "Article-ex/Article-Ex3/Article_Ex3.html#the-changing-role-of-financial-regulators",
    "href": "Article-ex/Article-Ex3/Article_Ex3.html#the-changing-role-of-financial-regulators",
    "title": "Assess and discuss how the role of financial regulators has evolved",
    "section": "",
    "text": "Financial regulators have had to balance between promoting innovation and fair competition, safeguard their financial systems, protect consumers; while also monitoring for illicit activities, and non- compliance to rules. In some economies, fintech growth was driven by an unmet demand for financial services, while in others, growth was driven by the high cost of financial services and the lack of competition. Although fintech has helped make the financial system become more inclusive and efficient, market failures and systematic risks remain relevant, and may substantially become worse if left unchecked.\nFinancial regulators have had to keep up with technology trends and anticipate their impact on financial stability. With tech companies becoming more involved in financial services, and with banks deploying new technologies and business models to stay competitive, regulators have had to engage both sides of the divide to understand new technologies and their impact to the financial system. Regulators must now additionally ensure that their policies and regulations can be applied fairly between these two segments to prevent any side from having an advantage over the other.\nThe role of financial regulators has evolved from merely being a supervisory body to being a progressive entity, actively initiating programs to engage innovative businesses and encourage financial innovation. Regulators like MAS and FCA provide regulatory advice and support, and construct testbeds or environments to actively test new financial products and services. By implementing these initiatives and becoming driving forces in these projects, they enable both banks and fintech firms to grow organically within the stipulated rules, bringing benefit to both their financial system and consumers.\nFor example, in Singapore, the MAS has initiatives in place to foster fintech collaboration such as fintech offices, accelerator programs and sandbox environments where fintech firms and financial institutions can come together to experiment on innovative products and solutions within a live environment [1]. The sandbox environment enables fintech firms and start-ups to test their concepts and technology in a live environment with limited impact to the financial system, thus enabling a viable assessment of their products before launch.\nFintech sandboxes aim to:\n1) Target new business models or technologies\n2) Provide an environment for testing and understanding new technology and methodologies\n3) Encompass specific objectives to resolve existing market frictions\n4) Provide a safe production environment to evaluate real-world results\nWith the entry of new financial products and fintech firms, regulators have also had to step up their capabilities in monitoring firms and their activities; and to understand the impact of these new products on the financial system and on consumers. To this end, regulators have had to bring in suitable talent, upgrade their technologies and methodologies to keep pace with new financial trends.\nRegTech provides technology to help manage regulatory processes within the financial industry. It refers to technology companies that provide technological solutions to help financial firms and regulators. Regulators globally have begun to engage RegTech companies to revamp their IT systems and processes. Functionalities include tools to match large volumes of data to decision rules, automating compliance or oversight processes and real-time monitoring of credit flows in the financial system.  \nIn a survey of global regulators by the World bank in 2019 [2], the top technologies employed by regulators include Machine Learning, Blockchain/DLT, Natural Language Processing, APIs, Cloud computing, Robotic Process Automation and Biometrics. It is interesting to note that regulators have evolved from being mere users of technology and legacy systems to actively deploying new fields of technology to find new applications and usage and keep up with new trends.  In the process, they fortify their capabilities to detect and anticipate any potential adverse impact to their financial systems.\nWhile the objectives of regulators are clear, the implementation of policies in the face of new technologies and financial firms can be challenging. According a 2019 report from the World Bank [2] regulators from 111 jurisdictions cited their top four impediments to effective supervision as:\n1) limited technical expertise\n2) limited funding and resources,\n3) unclear or limited jurisdiction over the activity; and\n4) coordinating multiple regulators to enforce regulations."
  },
  {
    "objectID": "Article-ex/Article-Ex3/Article_Ex3.html#challenges-in-implementation-and-adapting-to-trends",
    "href": "Article-ex/Article-Ex3/Article_Ex3.html#challenges-in-implementation-and-adapting-to-trends",
    "title": "Assess and discuss how the role of financial regulators has evolved",
    "section": "",
    "text": "Many Fintech firms operate borderless businesses which blur international jurisdictions. These firms extend their services globally, causing complex transaction monitoring for local regulators. For example, until recently, Binance, the world’s largest crypto exchange did not have an official headquarters. Similarly, big tech firms like Google, Apple and Amazon extend their services globally across multiple jurisdictions. The issue is exacerbated as some of these companies are outside the regulator’s jurisdiction. Regulation may not be uniformed across borders, hence making it difficult to formulate effective policies with other regulators.\nAnother challenge is the trend of bypassing traditional intermediaries. Different industry players like technology and telecommunications firms have emerged on the scene, providing financial services direct to consumers without collaborating with incumbent financial institutions. These companies may fall under the mandate of different regulators and this calls into question on whether there is a sufficient level of coordination and communication between regulatory bodies to be able to implement effective regulation. For example, in Singapore, Singtel, a telecommunications firm and Grab, a ride-hailing provider both provide payment services by means of a mobile e-wallet.\nIn Singapore, MAS has issued licenses to 4 digital banks[3] which include non-bank entities like Grab, Singtel, Sea Ltd and Ant Group. These businesses were assessed based on their value proposition, use of technology to improve financial inclusion and growth prospects. In this example, MAS has clearly adapted to changing trends and discarded notions of financial services only belonging to financial institutions. By assimilating these non-banks into their supervision umbrella, MAS also ensures that these new firms are subject to the same regulations as other banks, ensuring a level playing field in terms of scrutiny and compliance to rules."
  },
  {
    "objectID": "Article-ex/Article-Ex3/Article_Ex3.html#over-regulation",
    "href": "Article-ex/Article-Ex3/Article_Ex3.html#over-regulation",
    "title": "Assess and discuss how the role of financial regulators has evolved",
    "section": "",
    "text": "The rate of adoption of Fintech and the potential for players to scale rapidly has also put pressure on regulators to prevent concentration and systematic risk. However, regulators may not necessarily have the full picture or may lack reliable information on the ground about the structure and operations of Fintech firms and their markets, imposing stringent rules that curb financial innovation instead of encouraging them.\nFor example, regulators in China introduced several measures aimed at fintech firms to curb perceived monopolistic behavior, risky lending practices and lack of protection for customer data.\nAs a result, fintech giant, Ant Group has been directed to restructure itself into a financial holding company where all its subsidiaries engaged in financial activities must be subjected to financial regulations under the PBOC. This will also require them to have paid-in capital of at least 50% of the total registered capital of all the financial institutions they own. The stricter capital requirements are expected to constraint Ant’s huge loans business [4].\nAnt Group has also been directed to remove its financial products Huabei and Jiebei from the Alipay platform and delinking them from the Alipay payments infrastructure. The delinking of Ant’s consumer credit and microloan business is expected to further weaken its value proposition of providing competitive financial services through its Alipay platform. Controls on minimum deposit amounts and limits on the investments allowed for users in Yu’e Bao, Ant’s money-market fund platform, has also pushed investors to move their funds away from the Alipay platform.\nThe recent regulation changes in China are expected to significantly increase compliance costs for fintech companies and lead to moderate fintech growth and investment in the sector. It is well worth noting that regulators in China have evolved from adopting a lenient and light-handed approach to a stricter and more stringent regime. However it is not surprising given its unfortunate experience with P2P lenders where lenient, wait-and-see approaches led to unsustainable growth, massive fraud and subsequent crash of the industry with unfortunate consequences to millions of retail customers[5].\nIn conclusion, the rapid pace of change and innovation have required regulators to be agile and adaptive to changing trends. Regulators have also had to balance supporting Fintech and disruptive technologies while also mitigating risks to financial integrity and stability. While some regulators have adopted a one-size-fits-all approach to regulate these firms, many more have adopted new frameworks and guidelines to regulate fintech firms both as a whole and individually. Regulators have also had to look outwards in their approaches by increasing collaboration internationally. They have also had to re-think their use of technology and re-design themselves to become more progressive in their approach of regulation."
  },
  {
    "objectID": "Article-ex/Article-Ex3/Article_Ex3.html#references",
    "href": "Article-ex/Article-Ex3/Article_Ex3.html#references",
    "title": "Assess and discuss how the role of financial regulators has evolved",
    "section": "",
    "text": "[1] Overview of Regulatory Sandbox. (n.d.). Retrieved September 27, 2022, from https://www.mas.gov.sg/development/fintech/regulatory-sandbox\n[2] World Bank and Cambridge Center for Alternative Finance. (2019) “Regulating Alternative Finance: Results from a Global Regulator Survey”. 2019 https://openknowledge.worldbank.org/handle/10986/32592\n[3] MAS Announces Successful Applicants of Licences to Operate New Digital Banks in Singapore. (n.d.). Retrieved September 30, 2022, from https://www.mas.gov.sg/news/media-releases/2020/mas-announces-successful-applicants-of-licences-to-operate-new-digital-banks-in-singapore\n[4] Writer, S. (2021, March 3). Ant Group thrown into disarray by China’s new financial rules. Nikkei Asia. Retrieved September 29, 2022, from https://asia.nikkei.com/Business/Finance/Ant-Group-thrown-into-disarray-by-China-s-new-financial-rules2\n[5] PYMNTS. Protests Mark China’s Ruptured P2P Lending Landscape. August, 2018. https://www.pymnts.com/news/international/2018/china-protestors-p2p-lending-regulationfraud-debt/\nWorld Bank. (2020). How regulators respond to Fintech: Evaluating the different approaches–sandboxes and beyond. World Bank. https://openknowledge.worldbank.org/handle/10986/33698\nde Koker, Morris, N., & Jaffer, S. (2019). Regulating financial services in an era of technological disruption. Law in Context (Bundoora, Vic.), 36(2), 90–112. https://doi.org/10.26826/law-in-context.v36i2.98\nFintech : The Experience so Far - Executive Summary (English). Washington, D.C. : World Bank Group. http://documents.worldbank.org/curated/en/130201561082549144/Fintech-The-Experience-so-Far-Executive-Summary"
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "2021 was an exceptional year for fintech. In the year, we witnessed once-nascent businesses scaled up to become the potential next giants in the economy. For example, Coinbase and Robinhood were listed respectively in May and July. Coinbase was listed with a market capitalisation of $86 billion and Robinhood was listed at $32 billion, surpassing the market capitalisation of some decades-old reputable companies such as Nasdaq ($26B). These companies have taken about a decade to dethrone and challenge the market share of big financial services companies.\nIn this assignment, we seek to explore and understand the chain of events culminating to these major developments and how these developments will further accelerate the adoption of technology to deliver affordable financial services to the masses.\nSelect and examine four (4) factors that contribute to the meteoric rise of fintech companies.\n\n\nThe Financial Stability Board [1] describes Fintech as “as technologically enabled innovation in financial services that could result in new business models, applications, processes or products with an associated material effect on financial markets and institutions and the provision of financial services”. From the user perspective, this can also be described as the innovation in financial services facilitated by technology and impacting user experience.\nMany factors can be attributed to the rise of Fintech, from which I will seek to examine four of these factors below:   \n1) Changing Consumer demographics: The rise of the underbanked and underserved\n2) Changes in customer preferences: Issues of trust, preferences, and expectations\n3) Improvements in Technology: Mobile phone penetration, digital connectivity, and new technologies applicable to finance\n4) The Great Financial crisis and subsequent policies and regulatory environment\n\n\n\nIn a study done by the World Bank in 2017 [2], it was estimated that 1.7 billion adults were still unbanked and lacked access to basic banking services. In developing countries, households still primarily transacted in cash for daily transactions. Further, for those with bank accounts, additional financial services like consumer credit, insurance and wealth management were not available in these geographies. Closer to home, it is also estimated that 66% of Indonesia’s 275 million population remain unbanked [3]. Many of these Indonesians live in rural areas and do not have access to basic services such as payments, transactions, credit, and insurance. It is likely that they remain largely unserved because of their remote locations and distance from the nearest bank branch. Also, many of them would likely not meet the credit requirements needed to access credit or other financial services from banks.\nIt is more than likely that this huge amount of unmet demand has directly been a driver for the tick up in the rate of fintech adoption in these geographies. Unencumbered by strict onboarding requirements, stringent regulatory and credit requirements, physical presence, and location of customers, fintech companies have made use of their existing ecosystems and technologies to expand and extend their reach by means of mobile devices and online platforms.\nFor example, M-Pesa, the mobile phone-based money transfer service by Kenya’s Safaricom initially started in 2007 in Kenya and is now operating in multiple countries in Africa. M-Pesa users credit funds into their mobile phone account and use their mobile phones to transfer funds to other M-Pesa users. This service has been successfully adopted with more than 30 million users[4], contributing to transactions worth more than 40% of Kenya’s GDP. In 2016, It is also estimated that the provision of these services has lifted 2% of Kenyans out of poverty and helped an estimated 185,000 women to move from farming to business occupations [5].\nIn Indonesia, Grab [3] extends its financial services by means of a grassroots approach to empower small businesses across the country to transact for goods and services and to provide their local communities with financial services like fund transfers, remittances, bill payments, gold saving accounts and insurance. Using the same Grab application and ecosystem, these local agents help to propagate Grab’s suite of services to local communities which are otherwise underserved by traditional banks. This not only enables local small businesses to stay relevant to their community needs but also enables more people in remote areas to participant in the financial system.\n\n\n\nAfter the Global Financial Crisis in 2008, customers grew sceptical of traditional financial institutions and instruments. With the multiple system and market failures resulting in many established firms going under, and many more suffering impactful losses; customers began to question whether the banking sector was indeed resilient and trustworthy. While banks were busy ‘fighting fire’ and adjusting business models and strategies, compliance to new regulations and re-aligning markets, fintech firms stepped up to fill the void left by them.\nAt the same time a new generation of consumers (millennials) were participating in the economy for the first time and brought with them changing shifts in preference and expectations.\nBased on a study by SDL plc in 2014 [6], the following observations regarding millennials were noted:\n1) They are more likely to engage with brands that they already know and trust\n2) They are digitally active and use multiple devices daily\n3) They connect with companies thru social media to get discounts and other benefits\n4) They discover content through social media and social networks\n5) They expect a fast response and instant gratification\n6) They prefer tailored and customized services that they can choose\nSpecifically, these group of users are ‘digital natives’ who prefer a digital approach of accessing financial services rather than traditional means.\nIt is not surprising then that tech firms who were already engaging this group of users in their e-commerce ecosystem would benefit from providing additional financial services to them. For example, users of Apple, Google, Samsung, Ant Group, We Chat, and Amazon would have no difficulty making the transition to use their respective payments applications. Similarly, new fintech firms do not need to invest in physical spaces, IT infrastructure to market to new users over the internet and social media.\nWith lower barriers of entry, Fintech firms and start-ups specifically concentrate on a few selected financial services, they tend to focus on building capabilities on services where traditional banks are unable to provide reasonable customer experience. This use of technology and data enable Fintech firms to change how customers are served and extend cost savings to the consumer. The speed, reach, availability, and customized nature of services delivered further improve the customer experience. These financial services include cryptocurrencies (CoinBase), online trading (RobinHood), payments (Paypal), P2P lending (Lending club), robo advisory ( WealthFront) among many others.\n\n\n\nSmartphone and internet penetration have revolutionised connectivity, rapidly increasing the ability to transfer information and interact remotely, between businesses and consumers. This has improved access to services and enabled the delivery of readily available, lower-cost and tailored financial services to users irrespective of location and time-zone.\nIn a 2019 report by the World Advertising Research Centre [7], an estimated 51% of global smartphone users use only their smartphones to access the internet. This is estimated to grow to 72.6% of users by 2025, with most of the growth coming from China, India, and Indonesia. In many countries it is also likely that users will only ever use a smartphone instead of a computer. The small form factor makes smartphones a convenient device to interact with banks and other fintechs, facilitating seamless online transactions, payments, investments, and other financial services. For example, in China, the two dominant payment services app is Ant Financial’s Alipay and TenCent’s WeChat Pay.\nAlipay was initially launched in 2004[8] and started off as a online e-wallet to facilitate payments and provide escrow services for users of TaoBao, its e-commerce website. As a pioneer of e-commerce, TaoBao benefited from having a large customer base and payments ecosystem. As users of smartphones in China began to proliferate from the mid 2000s, Ant launched its smartphone application in 2009 and subsequently launched Quickpay in 2010 to allow users to link their credit cards to their Alipay accounts. In 2011, QR code payments were introduced which allowed users to transact with each other by means of scanning their QR codes. As user requirements grew, so did the services offered by the Alipay app. Now, users can use their smartphones to pay via QR codes, In-app funds transfer, web-payment and point of sales terminals in-store via NFC.\nAs users of its ecosystem grew, Ant Group was also able to use new technologies like AI, ML and Big Data analysis to leverage on its wealth of consumer data and insights to develop accurate customer profiles through the analysis of customer payment history and transaction activity\nWith customers already entrenched via its digital payments service Alipay, Ant has been able to further offer complementary financial services like investments, lending and credit, credit scores, and even insurance.\nAnt’s complete suite of services:\n\n\n\nSource: (Ahern, 2020) [9]\n\n\nIt is useful to note that as China made the transition to a smartphone base economy, both consumers and businesses were seeking for a more convenient, cashless, fast, and secure payment mechanism. It is no wonder then that Ant was able to capture this huge shift in requirements and exponentially grow as the number of smartphone users grew.\n\n\n\nPost-Global Financial crisis of 2008, banks and traditional financial institutions were grappling with the fall out from the crisis and the subsequent new regulation regimes implemented to reassess risk policies, exposure to markets and capital adequacy requirements. It is noted that Fintech firms and other Tech fins were not subjected to the same regulatory burdens and levels of inquiry from regulators. This enabled Fintech firms to come into the market or even expand their suite of services to meet burgeoning demand from a customer base seeking a more digital and tailored suite of financial services.\nAt the same time, Governments and regulators recognized that many consumers were still underserved and wanted to provide a conducive environment for Fintechs to come in to improve financial inclusion and financial participation. This can be seen by the success of M-Pesa in Kenya, WeChat pay and Alipay in China and TCASH in Indonesia in mobile payments and consumer credit.\nFor example, as mobile payments in China were largely unregulated due to its relatively small size at inception, the PBOC did not impose transaction size limits nor require payment services firms to report customer transaction details to their trust banks. Similarly, in Kenya, when Safaricom approached the Central Bank of Kenya to launch M-Pesa, while there was no precedence for a tele communications company to offer financial services, the central bank developed Trust account requirements for Safaricom to comply with before enabling the launch of their services [10].\nIn India, the government initiated the Start-up India [11] program, which provides for simplified regulatory processes, tax exemptions, patent reforms, mentorship opportunities and funding for fintech start-ups.\nOther jurisdictions like the UK, Hong Kong and Singapore see Fintech as the new paradigm to maintain their competitiveness as global financial hubs.\nFor example, the MAS has initiatives in place to foster FinTech collaboration such as fintech offices, accelerator programs and sandbox environments where FinTech and financial institutions can experiment with innovative products in a live environment [12]. The sandbox environment empowers fintech firms and start-ups to test their products in a live environment with limited impact to the financial system, enabling a viable assessment of their products before launch.  \nIn conclusion, it is not surprising how the four factors above have provided the perfect stage for new entrants to disrupt traditional banking and finance. It is rare that a such a confluence of technology, financial crisis and demographics emerge at the same time to spur innovation and the entry of new players. Moving forward, changes in technology and consumer preferences will surely further spur new innovations and will likely encourage incumbent banks to step up their game to stay competitive and relevant.\n\n\n\n[1] FinTech. (n.d.). Financial Stability Board. Retrieved September 26, 2022, from https://www.fsb.org/work-of-the-fsb/financial-innovation-and-structural-change/fintech/#:%7E:text=The%20FSB%20defines%20FinTech%20as,the%20provision%20of%20financial%20services\n[2] Overview. (n.d.). World Bank. Retrieved September 26, 2022, from https://www.worldbank.org/en/topic/financialinclusion/overview\n[3] Post, T. J. (2021, April 15). Grassroot strategy to realize financial inclusion in Indonesia. The Jakarta Post. Retrieved September 26, 2022, from https://www.thejakartapost.com/life/2021/04/15/grassroot-strategy-to-realize-financial-inclusion-in-indonesia.html\n[4] Guguyu, O. (2022, March 10). Safaricom’s M-Pesa crosses 30 million active users in Kenya. The East African. Retrieved September 26, 2022, from https://www.theeastafrican.co.ke/tea/business/safaricom-m-pesa-crosses-30-million-active-users-in-kenya-3743258#:%7E:text=Mobile%20money%20transfer%20platform%20M,digital%20transactions%20in%20the%20country\n[5] Study: Mobile-money services lift Kenyans out of poverty. (2016, December 8). MIT News | Massachusetts Institute of Technology. Retrieved September 26, 2022, from https://news.mit.edu/2016/mobile-money-kenyans-out-poverty-1208\n[6] Content Finds the Customer: SDL Global Study Finds Social Media Drives Content Discovery with Millennials. (2014, June 4). Business Wire. Retrieved September 26, 2022, from https://www.businesswire.com/news/home/20140604005209/en/Content-Finds-the-Customer-SDL-Global-Study-Finds-Social-Media-Drives-Content-Discovery-with-Millennials\n[7] Handley, L. (2019, January 24). Nearly three quarters of the world will use just their smartphones to access the internet by 2025. CNBC. Retrieved September 26, 2022, from https://www.cnbc.com/2019/01/24/smartphones-72percent-of-people-will-use-only-mobile-for-internet-by-2025.html\n[8] Ant Group Official Website. (n.d.). Retrieved September 27, 2022, from https://www.antgroup.com/en/about/history\n[9] Ahern, B. (2020, October 15). Ant Group 101. SeekingAlpha. Retrieved September 27, 2022, from https://seekingalpha.com/article/4379258-ant-group-101\n[10] World Bank and Cambridge Center for Alternative Finance. (2019) “Regulating Alternative Finance: Results from a Global Regulator Survey”. 2019 https://openknowledge.worldbank.org/handle/10986/32592\n[11] Government Initiatives. (n.d.). Retrieved September 27, 2022, from https://www.startupindia.gov.in/content/sih/en/international/go-to-market-guide/government-initiatives.html\n[12] Overview of Regulatory Sandbox. (n.d.). Retrieved September 27, 2022, from https://www.mas.gov.sg/development/fintech/regulatory-sandbox\nFeyen, E. (2021, July 13). Fintech and the digital transformation of financial services: implications for market structure and public policy. Retrieved September 27, 2022, from https://www.bis.org/publ/bppdf/bispap117.htm\nFrost, J. (2020, February 4). The economic forces driving fintech adoption across countries. Retrieved September 27, 2022, from https://www.bis.org/publ/work838.htm"
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html#introduction",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html#introduction",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "The Financial Stability Board [1] describes Fintech as “as technologically enabled innovation in financial services that could result in new business models, applications, processes or products with an associated material effect on financial markets and institutions and the provision of financial services”. From the user perspective, this can also be described as the innovation in financial services facilitated by technology and impacting user experience.\nMany factors can be attributed to the rise of Fintech, from which I will seek to examine four of these factors below:   \n1) Changing Consumer demographics: The rise of the underbanked and underserved\n2) Changes in customer preferences: Issues of trust, preferences, and expectations\n3) Improvements in Technology: Mobile phone penetration, digital connectivity, and new technologies applicable to finance\n4) The Great Financial crisis and subsequent policies and regulatory environment"
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html#the-rise-of-the-under-banked-and-under-served",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html#the-rise-of-the-under-banked-and-under-served",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "In a study done by the World Bank in 2017 [2], it was estimated that 1.7 billion adults were still unbanked and lacked access to basic banking services. In developing countries, households still primarily transacted in cash for daily transactions. Further, for those with bank accounts, additional financial services like consumer credit, insurance and wealth management were not available in these geographies. Closer to home, it is also estimated that 66% of Indonesia’s 275 million population remain unbanked [3]. Many of these Indonesians live in rural areas and do not have access to basic services such as payments, transactions, credit, and insurance. It is likely that they remain largely unserved because of their remote locations and distance from the nearest bank branch. Also, many of them would likely not meet the credit requirements needed to access credit or other financial services from banks.\nIt is more than likely that this huge amount of unmet demand has directly been a driver for the tick up in the rate of fintech adoption in these geographies. Unencumbered by strict onboarding requirements, stringent regulatory and credit requirements, physical presence, and location of customers, fintech companies have made use of their existing ecosystems and technologies to expand and extend their reach by means of mobile devices and online platforms.\nFor example, M-Pesa, the mobile phone-based money transfer service by Kenya’s Safaricom initially started in 2007 in Kenya and is now operating in multiple countries in Africa. M-Pesa users credit funds into their mobile phone account and use their mobile phones to transfer funds to other M-Pesa users. This service has been successfully adopted with more than 30 million users[4], contributing to transactions worth more than 40% of Kenya’s GDP. In 2016, It is also estimated that the provision of these services has lifted 2% of Kenyans out of poverty and helped an estimated 185,000 women to move from farming to business occupations [5].\nIn Indonesia, Grab [3] extends its financial services by means of a grassroots approach to empower small businesses across the country to transact for goods and services and to provide their local communities with financial services like fund transfers, remittances, bill payments, gold saving accounts and insurance. Using the same Grab application and ecosystem, these local agents help to propagate Grab’s suite of services to local communities which are otherwise underserved by traditional banks. This not only enables local small businesses to stay relevant to their community needs but also enables more people in remote areas to participant in the financial system."
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html#issues-of-trust-preferences-and-expectations",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html#issues-of-trust-preferences-and-expectations",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "After the Global Financial Crisis in 2008, customers grew sceptical of traditional financial institutions and instruments. With the multiple system and market failures resulting in many established firms going under, and many more suffering impactful losses; customers began to question whether the banking sector was indeed resilient and trustworthy. While banks were busy ‘fighting fire’ and adjusting business models and strategies, compliance to new regulations and re-aligning markets, fintech firms stepped up to fill the void left by them.\nAt the same time a new generation of consumers (millennials) were participating in the economy for the first time and brought with them changing shifts in preference and expectations.\nBased on a study by SDL plc in 2014 [6], the following observations regarding millennials were noted:\n1) They are more likely to engage with brands that they already know and trust\n2) They are digitally active and use multiple devices daily\n3) They connect with companies thru social media to get discounts and other benefits\n4) They discover content through social media and social networks\n5) They expect a fast response and instant gratification\n6) They prefer tailored and customized services that they can choose\nSpecifically, these group of users are ‘digital natives’ who prefer a digital approach of accessing financial services rather than traditional means.\nIt is not surprising then that tech firms who were already engaging this group of users in their e-commerce ecosystem would benefit from providing additional financial services to them. For example, users of Apple, Google, Samsung, Ant Group, We Chat, and Amazon would have no difficulty making the transition to use their respective payments applications. Similarly, new fintech firms do not need to invest in physical spaces, IT infrastructure to market to new users over the internet and social media.\nWith lower barriers of entry, Fintech firms and start-ups specifically concentrate on a few selected financial services, they tend to focus on building capabilities on services where traditional banks are unable to provide reasonable customer experience. This use of technology and data enable Fintech firms to change how customers are served and extend cost savings to the consumer. The speed, reach, availability, and customized nature of services delivered further improve the customer experience. These financial services include cryptocurrencies (CoinBase), online trading (RobinHood), payments (Paypal), P2P lending (Lending club), robo advisory ( WealthFront) among many others."
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html#improvements-in-technology",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html#improvements-in-technology",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "Smartphone and internet penetration have revolutionised connectivity, rapidly increasing the ability to transfer information and interact remotely, between businesses and consumers. This has improved access to services and enabled the delivery of readily available, lower-cost and tailored financial services to users irrespective of location and time-zone.\nIn a 2019 report by the World Advertising Research Centre [7], an estimated 51% of global smartphone users use only their smartphones to access the internet. This is estimated to grow to 72.6% of users by 2025, with most of the growth coming from China, India, and Indonesia. In many countries it is also likely that users will only ever use a smartphone instead of a computer. The small form factor makes smartphones a convenient device to interact with banks and other fintechs, facilitating seamless online transactions, payments, investments, and other financial services. For example, in China, the two dominant payment services app is Ant Financial’s Alipay and TenCent’s WeChat Pay.\nAlipay was initially launched in 2004[8] and started off as a online e-wallet to facilitate payments and provide escrow services for users of TaoBao, its e-commerce website. As a pioneer of e-commerce, TaoBao benefited from having a large customer base and payments ecosystem. As users of smartphones in China began to proliferate from the mid 2000s, Ant launched its smartphone application in 2009 and subsequently launched Quickpay in 2010 to allow users to link their credit cards to their Alipay accounts. In 2011, QR code payments were introduced which allowed users to transact with each other by means of scanning their QR codes. As user requirements grew, so did the services offered by the Alipay app. Now, users can use their smartphones to pay via QR codes, In-app funds transfer, web-payment and point of sales terminals in-store via NFC.\nAs users of its ecosystem grew, Ant Group was also able to use new technologies like AI, ML and Big Data analysis to leverage on its wealth of consumer data and insights to develop accurate customer profiles through the analysis of customer payment history and transaction activity\nWith customers already entrenched via its digital payments service Alipay, Ant has been able to further offer complementary financial services like investments, lending and credit, credit scores, and even insurance.\nAnt’s complete suite of services:\n\n\n\nSource: (Ahern, 2020) [9]\n\n\nIt is useful to note that as China made the transition to a smartphone base economy, both consumers and businesses were seeking for a more convenient, cashless, fast, and secure payment mechanism. It is no wonder then that Ant was able to capture this huge shift in requirements and exponentially grow as the number of smartphone users grew."
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html#policies-and-regulatory-environment",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html#policies-and-regulatory-environment",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "Post-Global Financial crisis of 2008, banks and traditional financial institutions were grappling with the fall out from the crisis and the subsequent new regulation regimes implemented to reassess risk policies, exposure to markets and capital adequacy requirements. It is noted that Fintech firms and other Tech fins were not subjected to the same regulatory burdens and levels of inquiry from regulators. This enabled Fintech firms to come into the market or even expand their suite of services to meet burgeoning demand from a customer base seeking a more digital and tailored suite of financial services.\nAt the same time, Governments and regulators recognized that many consumers were still underserved and wanted to provide a conducive environment for Fintechs to come in to improve financial inclusion and financial participation. This can be seen by the success of M-Pesa in Kenya, WeChat pay and Alipay in China and TCASH in Indonesia in mobile payments and consumer credit.\nFor example, as mobile payments in China were largely unregulated due to its relatively small size at inception, the PBOC did not impose transaction size limits nor require payment services firms to report customer transaction details to their trust banks. Similarly, in Kenya, when Safaricom approached the Central Bank of Kenya to launch M-Pesa, while there was no precedence for a tele communications company to offer financial services, the central bank developed Trust account requirements for Safaricom to comply with before enabling the launch of their services [10].\nIn India, the government initiated the Start-up India [11] program, which provides for simplified regulatory processes, tax exemptions, patent reforms, mentorship opportunities and funding for fintech start-ups.\nOther jurisdictions like the UK, Hong Kong and Singapore see Fintech as the new paradigm to maintain their competitiveness as global financial hubs.\nFor example, the MAS has initiatives in place to foster FinTech collaboration such as fintech offices, accelerator programs and sandbox environments where FinTech and financial institutions can experiment with innovative products in a live environment [12]. The sandbox environment empowers fintech firms and start-ups to test their products in a live environment with limited impact to the financial system, enabling a viable assessment of their products before launch.  \nIn conclusion, it is not surprising how the four factors above have provided the perfect stage for new entrants to disrupt traditional banking and finance. It is rare that a such a confluence of technology, financial crisis and demographics emerge at the same time to spur innovation and the entry of new players. Moving forward, changes in technology and consumer preferences will surely further spur new innovations and will likely encourage incumbent banks to step up their game to stay competitive and relevant."
  },
  {
    "objectID": "Article-ex/Article-Ex1/Article_Ex1.html#references",
    "href": "Article-ex/Article-Ex1/Article_Ex1.html#references",
    "title": "Examine factors that contributed to the rise of Fintech companies",
    "section": "",
    "text": "[1] FinTech. (n.d.). Financial Stability Board. Retrieved September 26, 2022, from https://www.fsb.org/work-of-the-fsb/financial-innovation-and-structural-change/fintech/#:%7E:text=The%20FSB%20defines%20FinTech%20as,the%20provision%20of%20financial%20services\n[2] Overview. (n.d.). World Bank. Retrieved September 26, 2022, from https://www.worldbank.org/en/topic/financialinclusion/overview\n[3] Post, T. J. (2021, April 15). Grassroot strategy to realize financial inclusion in Indonesia. The Jakarta Post. Retrieved September 26, 2022, from https://www.thejakartapost.com/life/2021/04/15/grassroot-strategy-to-realize-financial-inclusion-in-indonesia.html\n[4] Guguyu, O. (2022, March 10). Safaricom’s M-Pesa crosses 30 million active users in Kenya. The East African. Retrieved September 26, 2022, from https://www.theeastafrican.co.ke/tea/business/safaricom-m-pesa-crosses-30-million-active-users-in-kenya-3743258#:%7E:text=Mobile%20money%20transfer%20platform%20M,digital%20transactions%20in%20the%20country\n[5] Study: Mobile-money services lift Kenyans out of poverty. (2016, December 8). MIT News | Massachusetts Institute of Technology. Retrieved September 26, 2022, from https://news.mit.edu/2016/mobile-money-kenyans-out-poverty-1208\n[6] Content Finds the Customer: SDL Global Study Finds Social Media Drives Content Discovery with Millennials. (2014, June 4). Business Wire. Retrieved September 26, 2022, from https://www.businesswire.com/news/home/20140604005209/en/Content-Finds-the-Customer-SDL-Global-Study-Finds-Social-Media-Drives-Content-Discovery-with-Millennials\n[7] Handley, L. (2019, January 24). Nearly three quarters of the world will use just their smartphones to access the internet by 2025. CNBC. Retrieved September 26, 2022, from https://www.cnbc.com/2019/01/24/smartphones-72percent-of-people-will-use-only-mobile-for-internet-by-2025.html\n[8] Ant Group Official Website. (n.d.). Retrieved September 27, 2022, from https://www.antgroup.com/en/about/history\n[9] Ahern, B. (2020, October 15). Ant Group 101. SeekingAlpha. Retrieved September 27, 2022, from https://seekingalpha.com/article/4379258-ant-group-101\n[10] World Bank and Cambridge Center for Alternative Finance. (2019) “Regulating Alternative Finance: Results from a Global Regulator Survey”. 2019 https://openknowledge.worldbank.org/handle/10986/32592\n[11] Government Initiatives. (n.d.). Retrieved September 27, 2022, from https://www.startupindia.gov.in/content/sih/en/international/go-to-market-guide/government-initiatives.html\n[12] Overview of Regulatory Sandbox. (n.d.). Retrieved September 27, 2022, from https://www.mas.gov.sg/development/fintech/regulatory-sandbox\nFeyen, E. (2021, July 13). Fintech and the digital transformation of financial services: implications for market structure and public policy. Retrieved September 27, 2022, from https://www.bis.org/publ/bppdf/bispap117.htm\nFrost, J. (2020, February 4). The economic forces driving fintech adoption across countries. Retrieved September 27, 2022, from https://www.bis.org/publ/work838.htm"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Imran",
    "section": "",
    "text": "Having spent many years lost in a traditional finance career, I took the leap of faith to embark on a learning journey to deepen my understanding on Financial technology and Data Analytics.\nNever settle, and always strive to be a better version of yourself every day. It’s never to late to start something or learn something new. Your ability is not determined by your age.\n\n“Never say never, because limits, like fears, are often just an illusion” - Michael Jordan.\n\n\n\n Back to top"
  },
  {
    "objectID": "Article-ex/Article-Ex2/Article_Ex2.html",
    "href": "Article-ex/Article-Ex2/Article_Ex2.html",
    "title": "Banks have changed the way they deploy technology",
    "section": "",
    "text": "In 2018, the CEO of DBS Piyush Gupta has said that At DBS, we act less like a bank and more like a tech company and this sufficiently reflects how banks have changed the way they view and use technology.\nAppraise and explain banks have changed the way they deploy technology in their banking business.\n\n\nAfter the Global Financial crisis of 2008, as banks were grappling with the fallout from the crisis and new regulation regimes, they were occupied with changing their risk policies, re-thinking exposure to markets and increasing their capital adequacy requirements. At the same time, Fintech firms and other Tech fins, who were not subjected to the same levels of pressure from regulators and governments, came forward to expand their suite of services, meeting an increasing demand from a customer base seeking a new suite of digital and customized services.\nFintech firms had the advantage of complementary technologies, talent with knowledge of banking and their businesses, freedom from legacy IT systems, a focused product line and an existing customer base already entrenched in their ecosystems.\nFor example, it was said [1] that a key impetus for DBS’s digital transformation was the increasing competition from fintech firms in the region. Alibaba had expanded aggressively in Southeast Asia through its controlling stake in e-commerce player Lazada. Alibaba’s Ant Financial then merged Lazada’s payments system with its flagship Alipay wallet. Similarly, Grab also introduced Grab Pay, a mobile e-wallet that could be linked to customers’ credit and debit cards, along with other financial services such as loans and insurance offered in its wholly digital proposition.\nIn traditional banks, technology had only been used to address the operational, reliability and security aspects of banking services. In comparison, fintech firms brought with them the use of disruptive technologies consisting of cloud computing, artificial intelligence, machine learning, big data, blockchain etc to bring about new novel solutions and ways to service customers. With the entrants of new non-bank entities providing basic financial services and utilizing technology to entice customers, banks like DBS began to think more seriously about how to compete and how to deploy technology in their banking business.\n\n\n\nIn 2017, DBS introduced the first Video Teller Machines (VTMs)[2] across Singapore. These teller machines provide 24 hour banking services to customers, with the option of interacting with bank officers through live video streaming. The combination of redesigning the function and utility of an ATM to include more self-serve functions, as well as the ability of having ‘face to face’ help through live video enabled the bank to present a new way of delivering services in physical branches. This improved customer experience and helped reduce waiting times in branches.   \nSimilarly, in 2018, OCBC announced that half of its bank tellers would be redeployed to other roles with the introduction of new ATMs and digital service kiosks in branches [3]. These new machines took 2 years to design and incorporated 15 of the most frequent counter services, effectively functioning as mini bank branches. The new machines also include new capabilities like biometric authentication and signature pads for signed transactions. OCBC estimates that these new machines would save customers an average of 15 mins compared to previous.\nIn comparison to fintech firms which offer solely online, digital, and cashless services; banks like DBS and OCBC recognize that they are unable to repackage their legacy services to compete with these digital offerings. However, using technology, they could improve the delivery of existing services and in the process, improve customer journeys and experiences, keeping them happy and satisfied.\n\n\n\nUnencumbered by legacy IT systems that serve multiple businesses and product lines, fintech firms which offered a smaller suite of services were able to leverage on their existing technologies to lower cost of entry, increase speed to market, and directly pass on benefits to customers in the form of lower priced services and products. On the other hand, traditional banks were managing multiple IT systems, most of which were not interoperable with each other, supporting needs for multiple business units and product lines.\nIn the case of DBS, they made the decision to convert their core IT systems to a component-based infrastructure by means of scalable cloud-based deployment. Legacy banking applications and systems were gradually moved into modern hardware or implemented to the cloud, empowering the delivery of efficient and scalable solutions.\nCloud computing can be defined as the on-demand delivery of IT resources, especially computing power, and data storage, through a cloud services platform via the internet. As they decommissioned servers and began to shrink data centres, DBS instituted a policy of “everything goes cloud” [1]. Its IT systems changed from being cloud-ready to cloud-optimized and, eventually, to cloud-native. DBS also partnered with Amazon Web Services (AWS) to develop competence in cloud engineering.\nWhen developing new applications, DBS had focused on usability and re-usability. Hence, they made a conscious decision to develop applications which could function in the cloud, work as micro-services, and be API-ready. This ensured that they did not invest in technologies that were not scalable and flexible for future disruptions. This also accorded speed, flexibility, and experimentation when developing new digital solutions.\nSimilarly, for managing customer data, multiple Customer Relationship Management (CRM) systems were revamped, and data lakes implemented. Data lakes provide a single place where data can be collected across the company and be effectively analysed. This served to instil some level of interoperability between legacy systems, with the ability to draw insightful data points for the same customer. Previously, different business units had collaborated by means of bulk csv files or Manage Information systems (MIS); bankers could now leverage on real time updates directly uploaded to their respective CRM systems to gain more information on customer activities and transactions. This has enabled better data analytics of customer behaviour and preferences, improving cross-sell and ability to deliver more customised solutions.\n\n\n\nDBS CEO Piyush Gupta [4] describes invisible banking as “hiding banking services inside whatever it is that the customer really wants to do with his or her life, so that we can serve the customer at their convenience, in their domain, and on their time”.  Invisible banking enables customers to make financial transactions and decisions without consciously associating it as something separate from their other daily tasks. Banking services are delivered seamlessly such that it becomes embedded in their everyday activities. For example, paying your bills or sending a funds transfer should be as simple as sending a message to a friend or uploading a post to social media.\nIn the case of banks like DBS and OCBC, this starts from removing physical touchpoints, improving current frictions and manual processes, and delivering services in ways and medium that users can conveniently incorporate into their daily lives. For many banks, this is consistently embodied in the form of their mobile banking applications.\nIn 2016, DBS launched its rebranded and redesigned mobile banking application, Digibank [5] which delivered a significant number of self-service options and improvements over previous versions. Customers could open accounts through the app without the need to visit a branch, and other industry firsts were also introduced, which include:\n1) Viewing account balances with a single swipe on their mobile devices or Apple Watch without logging in\n2) Customising profiles and creating shortcuts for frequently used services\n3) Using a list of recent and favourite payees for faster transactions\n4) Using existing smartphone biometrics (eg Touch ID) to do faster log-ins\nOften described as a ‘super-app’ for banking services, Digibank allows customers to perform basic banking, apply for new accounts and cards, trade stocks, invest, convert currencies, send overseas remittances, analyse net worth, plan for retirement, and even track their carbon footprint; providing integrated and instantaneous services by means of a single application and touchpoint [6].\nIt is worth noting that newer functions like Net Asset Value planner, leverages on AI and data analytics to provide useful insights for customers to track their wealth accumulation journey, empowering them to make better decisions to meet their retirement goals. Through predictively analysis, the app also provides useful insights on investment strategies and suggests investments aligned to customer risk profiles. \nThe success of DigiBank has allowed DBS to launch a fully digital and branch-less service in India (2016) [7] and Indonesia (2017) [8]; where customer service is powered by a 24-hour AI-driven virtual assistant. Making use of Natural Language Processing, the virtual assistant is able to understand customer questions and respond real-time to urgent enquiries. With seamless non face to face onboarding, clients are also able to open their accounts faster and more conveniently. It is not surprising then that DBS shares the same advantage as online fintech firms in these countries and are able to provide clients with lower cost fees and competitive interest rates due to the benefits derived from technologies utilized.\nIn conclusion, banks like DBS exemplify how banks globally are using technology to stay competitive and penetrate new markets. Rather than inventing new novel solutions, they are concentrating on removing frictions, improving pain points and manual processes, revamping legacy systems and investing in new scalable technologies to make their services more accessible and to improve user experiences and journeys.\nDBS CFO Chng Sok Hui [9] mentioned that digital segment customers had a cost-to-income ratio of 34% Versus traditional segment clients of 54%. It is not surprising then, why banks like DBS have changed the way they deploy technology to stay relevant and competitive.\n\n\n\n[1] Sia, Weill, P., & Zhang, N. (2021). Designing a Future-Ready Enterprise: The Digital Transformation of DBS Bank. California Management Review, 63(3), 35–57. https://doi.org/10.1177/0008125621992583\n[2] DBS/POSB launches Singapore’s first video teller machines across nine locations. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/newsroom/DBS_POSB_launches_Singapores_first_video_teller_machines_across_nine_locations\n[3] Ming, T. E. (2018, July 23). OCBC to cut half of bank teller jobs in next two years; affected staff to be reassigned. TODAY. Retrieved September 28, 2022, from https://www.todayonline.com/singapore/ocbc-cut-half-bank-teller-jobs-next-two-years-affected-staff-be-reassigned\n[4] What will (and won’t) change in the future of Asia: An interview with the CEO of DBS. (2020, September 29). McKinsey & Company. Retrieved September 28, 2022, from https://www.mckinsey.com/featured-insights/asia-pacific/what-will-and-wont-change-in-the-future-of-asia-an-interview-with-the-ceo-of-dbs\n[5] DBS breaks new ground in digital banking. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/NewsPrinter.page?newsId=jy1w61p0&locale=en\n[6] Services available on digibank | DBS Singapore. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com.sg/personal/deposits/bank-with-ease/manage-account-and-more.page\n[7] DBS launches India’s first mobile-only bank, heralds ‘WhatsApp moment in banking.’ (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/newsroom/DBS_launches_Indias_first_mobile_only_bank_heralds_WhatsApp_moment_in_banking\n[8] DBS launches digibank, an entire bank in the phone, in Indonesia. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/newsroom/DBS_launches_digibank_an_entire_bank_in_the_phone_in_Indonesia\n[9] Becoming more than a bank: Digital transformation at DBS. (n.d.). McKinsey & Company. Retrieved September 28, 2022, from https://www.mckinsey.com/industries/financial-services/our-insights/banking-matters/becoming-more-than-a-bank-digital-transformation-at-dbs"
  },
  {
    "objectID": "Article-ex/Article-Ex2/Article_Ex2.html#introduction",
    "href": "Article-ex/Article-Ex2/Article_Ex2.html#introduction",
    "title": "Banks have changed the way they deploy technology",
    "section": "",
    "text": "After the Global Financial crisis of 2008, as banks were grappling with the fallout from the crisis and new regulation regimes, they were occupied with changing their risk policies, re-thinking exposure to markets and increasing their capital adequacy requirements. At the same time, Fintech firms and other Tech fins, who were not subjected to the same levels of pressure from regulators and governments, came forward to expand their suite of services, meeting an increasing demand from a customer base seeking a new suite of digital and customized services.\nFintech firms had the advantage of complementary technologies, talent with knowledge of banking and their businesses, freedom from legacy IT systems, a focused product line and an existing customer base already entrenched in their ecosystems.\nFor example, it was said [1] that a key impetus for DBS’s digital transformation was the increasing competition from fintech firms in the region. Alibaba had expanded aggressively in Southeast Asia through its controlling stake in e-commerce player Lazada. Alibaba’s Ant Financial then merged Lazada’s payments system with its flagship Alipay wallet. Similarly, Grab also introduced Grab Pay, a mobile e-wallet that could be linked to customers’ credit and debit cards, along with other financial services such as loans and insurance offered in its wholly digital proposition.\nIn traditional banks, technology had only been used to address the operational, reliability and security aspects of banking services. In comparison, fintech firms brought with them the use of disruptive technologies consisting of cloud computing, artificial intelligence, machine learning, big data, blockchain etc to bring about new novel solutions and ways to service customers. With the entrants of new non-bank entities providing basic financial services and utilizing technology to entice customers, banks like DBS began to think more seriously about how to compete and how to deploy technology in their banking business."
  },
  {
    "objectID": "Article-ex/Article-Ex2/Article_Ex2.html#reduction-in-physical-touch-points",
    "href": "Article-ex/Article-Ex2/Article_Ex2.html#reduction-in-physical-touch-points",
    "title": "Banks have changed the way they deploy technology",
    "section": "",
    "text": "In 2017, DBS introduced the first Video Teller Machines (VTMs)[2] across Singapore. These teller machines provide 24 hour banking services to customers, with the option of interacting with bank officers through live video streaming. The combination of redesigning the function and utility of an ATM to include more self-serve functions, as well as the ability of having ‘face to face’ help through live video enabled the bank to present a new way of delivering services in physical branches. This improved customer experience and helped reduce waiting times in branches.   \nSimilarly, in 2018, OCBC announced that half of its bank tellers would be redeployed to other roles with the introduction of new ATMs and digital service kiosks in branches [3]. These new machines took 2 years to design and incorporated 15 of the most frequent counter services, effectively functioning as mini bank branches. The new machines also include new capabilities like biometric authentication and signature pads for signed transactions. OCBC estimates that these new machines would save customers an average of 15 mins compared to previous.\nIn comparison to fintech firms which offer solely online, digital, and cashless services; banks like DBS and OCBC recognize that they are unable to repackage their legacy services to compete with these digital offerings. However, using technology, they could improve the delivery of existing services and in the process, improve customer journeys and experiences, keeping them happy and satisfied."
  },
  {
    "objectID": "Article-ex/Article-Ex2/Article_Ex2.html#agile-operations",
    "href": "Article-ex/Article-Ex2/Article_Ex2.html#agile-operations",
    "title": "Banks have changed the way they deploy technology",
    "section": "",
    "text": "Unencumbered by legacy IT systems that serve multiple businesses and product lines, fintech firms which offered a smaller suite of services were able to leverage on their existing technologies to lower cost of entry, increase speed to market, and directly pass on benefits to customers in the form of lower priced services and products. On the other hand, traditional banks were managing multiple IT systems, most of which were not interoperable with each other, supporting needs for multiple business units and product lines.\nIn the case of DBS, they made the decision to convert their core IT systems to a component-based infrastructure by means of scalable cloud-based deployment. Legacy banking applications and systems were gradually moved into modern hardware or implemented to the cloud, empowering the delivery of efficient and scalable solutions.\nCloud computing can be defined as the on-demand delivery of IT resources, especially computing power, and data storage, through a cloud services platform via the internet. As they decommissioned servers and began to shrink data centres, DBS instituted a policy of “everything goes cloud” [1]. Its IT systems changed from being cloud-ready to cloud-optimized and, eventually, to cloud-native. DBS also partnered with Amazon Web Services (AWS) to develop competence in cloud engineering.\nWhen developing new applications, DBS had focused on usability and re-usability. Hence, they made a conscious decision to develop applications which could function in the cloud, work as micro-services, and be API-ready. This ensured that they did not invest in technologies that were not scalable and flexible for future disruptions. This also accorded speed, flexibility, and experimentation when developing new digital solutions.\nSimilarly, for managing customer data, multiple Customer Relationship Management (CRM) systems were revamped, and data lakes implemented. Data lakes provide a single place where data can be collected across the company and be effectively analysed. This served to instil some level of interoperability between legacy systems, with the ability to draw insightful data points for the same customer. Previously, different business units had collaborated by means of bulk csv files or Manage Information systems (MIS); bankers could now leverage on real time updates directly uploaded to their respective CRM systems to gain more information on customer activities and transactions. This has enabled better data analytics of customer behaviour and preferences, improving cross-sell and ability to deliver more customised solutions."
  },
  {
    "objectID": "Article-ex/Article-Ex2/Article_Ex2.html#invisible-banking",
    "href": "Article-ex/Article-Ex2/Article_Ex2.html#invisible-banking",
    "title": "Banks have changed the way they deploy technology",
    "section": "",
    "text": "DBS CEO Piyush Gupta [4] describes invisible banking as “hiding banking services inside whatever it is that the customer really wants to do with his or her life, so that we can serve the customer at their convenience, in their domain, and on their time”.  Invisible banking enables customers to make financial transactions and decisions without consciously associating it as something separate from their other daily tasks. Banking services are delivered seamlessly such that it becomes embedded in their everyday activities. For example, paying your bills or sending a funds transfer should be as simple as sending a message to a friend or uploading a post to social media.\nIn the case of banks like DBS and OCBC, this starts from removing physical touchpoints, improving current frictions and manual processes, and delivering services in ways and medium that users can conveniently incorporate into their daily lives. For many banks, this is consistently embodied in the form of their mobile banking applications.\nIn 2016, DBS launched its rebranded and redesigned mobile banking application, Digibank [5] which delivered a significant number of self-service options and improvements over previous versions. Customers could open accounts through the app without the need to visit a branch, and other industry firsts were also introduced, which include:\n1) Viewing account balances with a single swipe on their mobile devices or Apple Watch without logging in\n2) Customising profiles and creating shortcuts for frequently used services\n3) Using a list of recent and favourite payees for faster transactions\n4) Using existing smartphone biometrics (eg Touch ID) to do faster log-ins\nOften described as a ‘super-app’ for banking services, Digibank allows customers to perform basic banking, apply for new accounts and cards, trade stocks, invest, convert currencies, send overseas remittances, analyse net worth, plan for retirement, and even track their carbon footprint; providing integrated and instantaneous services by means of a single application and touchpoint [6].\nIt is worth noting that newer functions like Net Asset Value planner, leverages on AI and data analytics to provide useful insights for customers to track their wealth accumulation journey, empowering them to make better decisions to meet their retirement goals. Through predictively analysis, the app also provides useful insights on investment strategies and suggests investments aligned to customer risk profiles. \nThe success of DigiBank has allowed DBS to launch a fully digital and branch-less service in India (2016) [7] and Indonesia (2017) [8]; where customer service is powered by a 24-hour AI-driven virtual assistant. Making use of Natural Language Processing, the virtual assistant is able to understand customer questions and respond real-time to urgent enquiries. With seamless non face to face onboarding, clients are also able to open their accounts faster and more conveniently. It is not surprising then that DBS shares the same advantage as online fintech firms in these countries and are able to provide clients with lower cost fees and competitive interest rates due to the benefits derived from technologies utilized.\nIn conclusion, banks like DBS exemplify how banks globally are using technology to stay competitive and penetrate new markets. Rather than inventing new novel solutions, they are concentrating on removing frictions, improving pain points and manual processes, revamping legacy systems and investing in new scalable technologies to make their services more accessible and to improve user experiences and journeys.\nDBS CFO Chng Sok Hui [9] mentioned that digital segment customers had a cost-to-income ratio of 34% Versus traditional segment clients of 54%. It is not surprising then, why banks like DBS have changed the way they deploy technology to stay relevant and competitive."
  },
  {
    "objectID": "Article-ex/Article-Ex2/Article_Ex2.html#references",
    "href": "Article-ex/Article-Ex2/Article_Ex2.html#references",
    "title": "Banks have changed the way they deploy technology",
    "section": "",
    "text": "[1] Sia, Weill, P., & Zhang, N. (2021). Designing a Future-Ready Enterprise: The Digital Transformation of DBS Bank. California Management Review, 63(3), 35–57. https://doi.org/10.1177/0008125621992583\n[2] DBS/POSB launches Singapore’s first video teller machines across nine locations. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/newsroom/DBS_POSB_launches_Singapores_first_video_teller_machines_across_nine_locations\n[3] Ming, T. E. (2018, July 23). OCBC to cut half of bank teller jobs in next two years; affected staff to be reassigned. TODAY. Retrieved September 28, 2022, from https://www.todayonline.com/singapore/ocbc-cut-half-bank-teller-jobs-next-two-years-affected-staff-be-reassigned\n[4] What will (and won’t) change in the future of Asia: An interview with the CEO of DBS. (2020, September 29). McKinsey & Company. Retrieved September 28, 2022, from https://www.mckinsey.com/featured-insights/asia-pacific/what-will-and-wont-change-in-the-future-of-asia-an-interview-with-the-ceo-of-dbs\n[5] DBS breaks new ground in digital banking. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/NewsPrinter.page?newsId=jy1w61p0&locale=en\n[6] Services available on digibank | DBS Singapore. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com.sg/personal/deposits/bank-with-ease/manage-account-and-more.page\n[7] DBS launches India’s first mobile-only bank, heralds ‘WhatsApp moment in banking.’ (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/newsroom/DBS_launches_Indias_first_mobile_only_bank_heralds_WhatsApp_moment_in_banking\n[8] DBS launches digibank, an entire bank in the phone, in Indonesia. (n.d.). Retrieved September 28, 2022, from https://www.dbs.com/newsroom/DBS_launches_digibank_an_entire_bank_in_the_phone_in_Indonesia\n[9] Becoming more than a bank: Digital transformation at DBS. (n.d.). McKinsey & Company. Retrieved September 28, 2022, from https://www.mckinsey.com/industries/financial-services/our-insights/banking-matters/becoming-more-than-a-bank-digital-transformation-at-dbs"
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "Select and discuss four (4) key characteristics that create or inject value for a non-fungible token (NFT).\n\n\nIn its early versions, NFTs derived their value from being collectible and scarce. In recent times, the demand for NFTs have evolved beyond the rarity aspect and new value propositions have grown in importance. The four characteristics that create value for NFTs are verifiable ownership, utility and benefits, community and inclusion, and social impact.\n\n\n\nBefore NFTs, there was no straightforward way to distinguish owners of digital art from someone who just copied an image of the artwork. Making a copy of a piece of art does not diminish the value of the original, provided we can prove which is the original and who owns it. Similarly, having multiple digital copies of the same artwork does not make someone the exclusive owner of that piece of art.\nNFTs facilitate transactions between buyers and sellers by providing a verifiable proof and history of ownership. In doing so, they make it possible to build new markets around new types of transactions. For example, buying and selling artwork online that could never be sold before because of a lack of trust and inability to verify authenticity.\nArtists without connections or platforms to showcase their art can struggle to make a living and to maintain a sustainable career in the arts.  Through new public marketplaces like Open Sea, Foundation and Nifty gateway, digital creators and artists are now able to launch content without the need for intermediaries, having complete control over their artistic vision, while also being able to monetize their work and engage supporters on a global scale.\nWhere in the past artists and creators could only profit from a meagre percentage of the initial sale, today they are earning higher incomes from their digital work. For example, in 2021, Central and Southeast Asia accounted for 35% of the global US$ 22 billion global NFT industry(Ng, 2022).\nThe proliferation of NFTs has enabled a new generation of artists who had previously been hindered by barriers of geography, societal norms, censorship, economic means, and unfair compensation to come online and affordably mint NFTs on open platforms; enabling them to reach an audience previously unattainable whilst securing fair and sustainable compensation for their work.\nFor creators and artists from poorer geographies especially, NFTs have given them access to the digital economy via cryptocurrencies; improving financial inclusion among those who were previously unbanked and underserved. Earning income in cryptocurrencies have also given them access to currencies that function better as a store of wealth and medium of exchange without risks of forgery, inflation, and theft.\nDigital creators can set royalty clauses for all future sales of their work, generating recurring income as their works exchange hands and appreciate. NFTs make this possible through smart contracts that automatically enforce terms and clauses which are then permanently enshrined in the blockchain; immutable and transparent for anyone to check and verify against. This contrasts with the current system where artists are only paid for the initial sale of their works and where no redress can be automatically programmed to prevent exploitation from intermediaries and resellers.\nWith NFTs, the need for trust between buyers and sellers is minimized as there is a verifiable decentralized system to record ownership and track ownership changes; without which there would be no incentive for buyers to purchase these digital artworks or for sellers to sell them. Buyers need to be able to verify that Sellers can sell the assets and Sellers need to be able to transfer ownership rights to Buyers in a secure and transparent manner. Without clear property rights and an efficient process for exchange, transactions cannot be facilitated, and markets cannot function.\nBecause NFT ownership is easy to verify and transfer, new markets in a variety of goods can also be made possible. Real world assets like property, event passes, and services could also come with their own corresponding NFTs, solving age old barriers of trust and authenticity. For example, NFTs can speed up the process of buying property by replacing intermediaries with smart contracts to enable efficient and secure transfer of ownership. Converting a real estate property into an NFT enables for a quick and simple settlement process. As proof and history of ownership is directly recorded on the blockchain, transactions can be verified quickly and easily.\nTickets for sporting events, concerts and conferences could also be sold in an NFT form. Buyers would not have to worry about counterparty risk, fraudulent exchanges, and counterfeit tickets. Holders of these NFT ticket/passes can also securely sell to anyone in an efficient and transparent manner, where changes in ownership rights are instantaneously updated and verifiable.\nNFTs can also enable digital ownership for physical rare artifacts, without the need to relinquish physical ownership rights. For example, museums can sell paintings and art objects in the form of digital assets, which then enable them to create new revenue streams. In 2021, the Russian Hermitage Museum sold several digital representations of its exclusive art collection via NFTs in a move it described as making luxury artwork more accessible (Dafoe, 2022).\n\n\n\nUtility is the value that an asset provides to its holder. The more useful an asset is, the more people are willing to pay for that asset because of its associated benefits and practical uses after the initial purchase. The value of an NFT can increase by increasing the number of ways it can be used in the real world.\nNFT projects can offer utilities such as voting rights, real world items, memberships and access to events, private communities, fractional ownership and even the ability to earn passive income.\nAs NFT holders have a personal stake in the success of the project, several projects enable their holders to be able to vote and decide on the future direction of the project. NFT projects could have a treasury or pool of funds raised from initial sales, from which the NFT community can vote and decide on how to spend those funds; for example, creating new projects, planning new collaborations, or even advancing important social issues.\nThe decision-making process in the NFT project is shared with all members of the project by means of a Decentralized Autonomous Organization (DAO) where decision-making and influence is shared with all its members. In a traditional company, the CEO or management team would have made all decisions, having the final say in the company’s strategy and future direction. In NFT projects with voting rights, there is no centralization in the decision-making process and instead every NFT holder can have a voice and a say in what happens to the project. The decentralization of the decision-making process incentivises NFT holders to observe good behaviour to ensure success of the project.\nSome NFTs enable holders to obtain physical items in the real world. Fashion brands like Gucci and Nike use NFTs as a ticket to real world items. Holders of these NFTs can use their items both in the Metaverse or virtual worlds as well as in the real world. Nike, with its acquisition of RFTKT in December 2021 enables its NFT holders to obtain physical versions of its NFT sneakers (Ernest, 2022).\nIncreasingly, NFTs are also being used to as proof of memberships to exclusive benefits or access to unique events. For example, the music festival Coachella sells NFTs that grant lifetime access to virtual and physical events as well as physical and digital items (Bierman, 2022). Holders of these NFTs can also enjoy benefits like first access to product launches and exclusive entry passes for virtual and in-person events.\nNFT projects like Gary Vaynerchuk’s VeeFriends include access to events like conferences, educational talks, and direct access to thought leaders and leading entrepreneurs (Thomas, 2022). NFT holders can buy and sell these NFTs in secondary markets, where the real-world benefits further enhance the value of these NFTs.\nNFT projects also provide their holders access to private communities and Discord channels, allowing holders the opportunity to participate and connect to exclusive communities or to create their own exclusive communities. Discord especially, has been the go-to online platform, enabling a diverse set of people to come together to collaborate and share information. In NFT projects like the World of Women project, these channels function as valuable platform for members to share ideas, knowledge, opportunities, and to connect with future business partners (Beyer, 2022).\nSome NFT projects enable fractional ownership, where one could own part of an NFT that would be otherwise be too expensive to own outright. Sellers can sell part of their asset without giving up complete ownership and buyers can own part of the NFT, reducing their risk of committing substantial amounts of capital for whole ownership. Fractional ownership also injects liquidity to the NFT project by allowing smaller investors to own assets collectively.\nIn June 2021, the Doge meme NFT, was bought for $4 million by PleasrDAO which then sold fractional ownerships to fans (Locke, 2021). Axie Infinity, an NFT-based online video game, also sells fractionalized ownerships of rare ‘Axies’, one of its most popular in-game NFT assets (Takyar, 2022).\nAs NFTs become more popular, owning a complete NFT become more expensive. While fractional ownership can lower the cost of ownership, it does not necessarily dilute the overall value of that NFT. When a fractional owner re-sells his fraction at a lower price, it does not lower the value of the remaining token for other fractional holders. Similarly, even if the token’s value skyrockets, buyers can still come in to buy at lower prices to own a smaller fraction of that NFT.\nAnother utility common for gaming NFTs is merging or breeding. For example, in the Crypto Kitties project, NFT holders can ‘breed’ their existing Crypto Cats to create new NFTs which they can choose to add to their collection or sell in the secondary market. For some, the new NFTs can fetch a higher value than existing tokens (Cryptokittes, 2018).\nSome NFT projects allow their holders to stake their NFTs to earn additional tokens or yield. NFT holders send their NFTs to a pool or a wallet to lock up their tokens, reducing the supply of tokens and maintaining the scarcity value of remaining tokens. Holders are then rewarded with native tokens during the period of staking.\nTo reward NFT holders for their support and efforts in building awareness and interest for the project, many projects also frequently ‘airdrop’ new benefits and free NFTs into holder’s wallets. NFT ownership facilitates a direct relationship between creators/founders and their NFT holders who function as the unofficial marketing team of the project. This close bond between community and project shifts the company-customer relationship to be more of a partnership with shared and aligned interests.\nDesigning utilities that provides recurring benefits to NFT holders encourage continued support for these NFT projects and instils greater satisfaction in ownership. In comparison, owning physical pieces of art or collectibles provides no real benefit other than price appreciation, which can only be realised upon selling those assets.\n\n\n\nA network effect is a phenomenon where the intrinsic value of a product or good increases as more people use it. For example, a social media platform is only valuable if many people use it. The more active users an NFT project has, the more valuable it becomes for everyone involved. Like cryptocurrencies, where a token’s worth comes from users’ shared agreement, the bigger and more engaged the NFT community is, the higher the project’s value.\nIt is not surprising why the two most valuable NFT projects Cryptopunks and Bored Ape Yacht Club have been able to rise in prominence, with their legions of fans and recognition from social and mainstream media and influential people.\nFor these projects, buyers are not necessarily buying the NFTs for the art itself but for the community and exclusivity it accords to the holder. For example, Bored Ape Yacht Club (BAYC) NFT holders are immediately welcomed into an exclusive community with private Discord channels, member-only events, and networking sessions with other BAYC holders including celebrities and high-profile individuals.\nBeyond hype and profitability, communities that rally behind the same beliefs and connect to a greater cause have also grown in numbers and prominence within the NFT ecosystem. Projects with clearly defined roadmaps and social missions can effectively rally people together and add value to their lives. NFT communities provide a space and platform for people to share ideas, collaborate and support each other across borders, social circumstances, and languages. They can disrupt traditional industries, provide a platform to encourage entrepreneurship and come together to advance various social causes.\nThe World of Women (WOW) NFT project is an artwork collection depicting women-centric characters and profile pictures (Beyer, 2022). The project builds and drives a community that rallies behind a movement to promote greater diversity, representation, and inclusion of women in the web3 space. The WOW project provides free mentoring and education, as well as channels for members to network and develop business opportunities across their userbase and network of corporate partners.\nThis project also supports emerging artists and non-profits globally, with a percentage of all sales going to women-centric causes. Through the funding of projects important to women interests, the project inspires and nurtures a new generation of women to explore new opportunities in Web3 and technology.\nFor any NFT project to be successful and have long-term growth and value, a constantly engaged community is crucial. Belonging to a community is intrinsic to human nature. Whether it be niche interests, identity, or beliefs, we all wish to form groups where we can interact with like-minded individuals. The more a community becomes part of people’s personal identities, the more the value of that community is reinforced.\nWithout an active and engaged community of users, NFT projects can fail to take off, or would rapidly collapse as users lose interest. This lack of engagement and vigour in the community can eventually lead to the devaluation of the project and the NFTs directly.\n\n\n\nNFTs and its communities has enabled new ways of thinking about social responsibility. There is an increasing trend of new NFT projects with clearly defined road maps that explain how they intend to use their proceeds to fund projects to advance social causes and how to use their technology to improve the world around them. Projects have also purposefully allocated part of their proceeds to charitable and social causes, which in turn increase the interest in these NFT projects from people who identify with these causes.\nSome of the charitable and social causes adopted by NFT projects include climate change, mental health, saving endangered animals, among many others.\nFor example, Apocalyptic Apes is a side project of the Bored Ape Yacht Club NFT project, focusing on climate issues through ocean clean up and reduction of plastic waste (Vick, 2022).\nThe CleanOcean NFT project provides a reward system for volunteering activities like cleaning polluted beaches, parks, or lands (block.co, 2022). The project receives funds from donors and brings together volunteers and enthusiasts who are rewarded with tokens for their contributions to cleaning up contaminated oceans and lakes.\nThe Blazed Cats NFT project features an artwork collection of cat jpegs with a variety of customizations (Blazed Cats, n.d.). 25% of new mint profits and 100% of secondary royalties goes to Mental Health America, which was chosen as the sole beneficiary by its discord community. This project raised over $215,000 in its first two weeks and enables Mental Health America to generate perpetual revenue from secondary sales of their NFTs.\nNFTs provide a new and innovative way to raise funds for Non-profit organizations and charities where proceeds from NFT auctions are used to support the non-profit’s mission. Selling NFTs enable non-profits to access new sources of funding outside of established channels and help them diversify their fund-raising streams. NFTs have also enabled non-profits and charities to continually receive income from secondary sales of these tokens. Each NFT that is purchased and subsequently resold provides perpetual revenue streams.\nFor example, a Singaporean non-profit organization called The Good Blockchain minted unique National Day Parade-themed NFTs featuring artworks by artists with disabilities to raise funds for SG enable, a local agency that empowers persons with disabilities to integrate with society (Automata Network, 2022). Similarly, in April 2022, The Food Bank Singapore also auctioned several NFTs of its mascot to raise funds for its meals and pantry programme(Auto, 2022).\n\n\n\nNFTs have become the new social identity, enabling people to display the causes to which they are aligned to; functioning as social signalling tools to show what communities they belong to and what they believe in. By fostering a large and vibrant community centred around more than art or profits, the NFT space has become a place where people can collectively come together to effect real change and do positive things to improve the world around them. This is a good example on how NFTs have grown beyond the initial speculative hype by offering a utility that promotes positive change. Through its own way, NFTs help tackle important problems like climate, inequalities, and socio-economic issues by increasing awareness, building and driving community engagement and enabling participation.\n\n\n\nAuto, H. (2022, April 21). S’pore food charity auctions art NFTs to raise funds for meals and pantry programme. The Straits Times. https://www.straitstimes.com/singapore/community/spore-food-charity-is-auctioning-art-nfts-to-raise-funds-for-meals-and-pantry-programme  [Accessed 26 October 2022]\nAutomata Network. (2022, August 9). NFTFair supports NDP2022 NFT fundraising campaign - Automata Network. Medium. https://medium.com/atanetwork/nftfair-supports-ndp2022-nft-fundraising-campaign-32398b03bcd8 [Accessed 26 October 2022]\nBeyer, E. J. (2022, July 5). The Ultimate Guide to World of Women: A Pillar of Inclusivity in NFTs. Nft Now. https://nftnow.com/guides/the-ultimate-guide-to-world-of-women-a-pillar-of-inclusivity-in-nfts/ [Accessed 26 October 2022]\nBierman, B. (2022, February 18). Inside the Sale of Coachella’s Lifetime Passes and $1.4 Million NFT Collection. EDM.com - the Latest Electronic Dance Music News, Reviews & Artists. https://edm.com/gear-tech/coachella-sells-1-4-million-nft-lifetime-passes [Accessed 26 October 2022]\nBlazed Cats. (n.d.). Retrieved October 25, 2022, from https://blazedcats.com/ [Accessed 26 October 2022]\nBlock.co. (2022, September 12). NFTs for Social Good - block.co. Medium. https://blockdotco.medium.com/nfts-for-social-good-2b3e43b31e [Accessed 26 October 2022]\nCryptoKitties. (2018, July 26). Getting started with CryptoKitties Part Two: Buying and Breeding. Medium. https://medium.com/cryptokitties/getting-started-with-cryptokitties-part-two-buying-and-breeding-792502e54a4d [Accessed 26 October 2022]\nDafoe, T. (2022, January 27). Russia’s Hermitage Museum Will Auction Off NFTs of Prized Works by Leonardo, Van Gogh, and Other Artists in Its Collection. Artnet News. https://news.artnet.com/art-world/hermitage-museum-auctioning-nfts-1992830 [Accessed 26 October 2022]\nErnest, M. (2022, June 29). Nike’s RTFKT Studios is offering Air Force 1 NFTs with real sneakers to match. Input. https://www.inputmag.com/style/nike-rtfkt-studios-air-force-1-nft-real-sneakers-redeem [Accessed 26 October 2022]\nKaczynski, S., & Kominers, S. D. (2021, November 19). How NFTs Create Value. Harvard Business Review. https://hbr.org/2021/11/how-nfts-create-value [Accessed 26 October 2022]\nKostic, E. (2022, March 10). What Could NFTs Mean for Nonprofits? Ad Council Org. https://www.adcouncil.org/all-articles/what-could-nfts-mean-for-nonprofits [Accessed 26 October 2022]\nNg, K. (2022, April 12). How Southeast Asia’s NFT ecosystem lifts up local artists. Forkast. https://forkast.news/how-southeast-asias-nft-ecosystem-lifts-up-local-artists/ [Accessed 26 October 2022]\nThomas, L. (2022, October 4). Gary Vee’s NFTs: A Guide to VeeFriends and the Man Taking Over Web3. Nft Now. https://nftnow.com/guides/ultimate-guide-to-veefriends/ [Accessed 26 October 2022]\nVick, A. (2022, February 24). How NFTs Are Creating Social Value. Forbes. https://www.forbes.com/sites/forbestechcouncil/2022/02/24/how-nfts-are-creating-social-value/?sh=62a217ecb98c [Accessed 26 October 2022]"
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#introduction",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#introduction",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "In its early versions, NFTs derived their value from being collectible and scarce. In recent times, the demand for NFTs have evolved beyond the rarity aspect and new value propositions have grown in importance. The four characteristics that create value for NFTs are verifiable ownership, utility and benefits, community and inclusion, and social impact."
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#verifiable-ownership-enabling-new-markets-and-transactions",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#verifiable-ownership-enabling-new-markets-and-transactions",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "Before NFTs, there was no straightforward way to distinguish owners of digital art from someone who just copied an image of the artwork. Making a copy of a piece of art does not diminish the value of the original, provided we can prove which is the original and who owns it. Similarly, having multiple digital copies of the same artwork does not make someone the exclusive owner of that piece of art.\nNFTs facilitate transactions between buyers and sellers by providing a verifiable proof and history of ownership. In doing so, they make it possible to build new markets around new types of transactions. For example, buying and selling artwork online that could never be sold before because of a lack of trust and inability to verify authenticity.\nArtists without connections or platforms to showcase their art can struggle to make a living and to maintain a sustainable career in the arts.  Through new public marketplaces like Open Sea, Foundation and Nifty gateway, digital creators and artists are now able to launch content without the need for intermediaries, having complete control over their artistic vision, while also being able to monetize their work and engage supporters on a global scale.\nWhere in the past artists and creators could only profit from a meagre percentage of the initial sale, today they are earning higher incomes from their digital work. For example, in 2021, Central and Southeast Asia accounted for 35% of the global US$ 22 billion global NFT industry(Ng, 2022).\nThe proliferation of NFTs has enabled a new generation of artists who had previously been hindered by barriers of geography, societal norms, censorship, economic means, and unfair compensation to come online and affordably mint NFTs on open platforms; enabling them to reach an audience previously unattainable whilst securing fair and sustainable compensation for their work.\nFor creators and artists from poorer geographies especially, NFTs have given them access to the digital economy via cryptocurrencies; improving financial inclusion among those who were previously unbanked and underserved. Earning income in cryptocurrencies have also given them access to currencies that function better as a store of wealth and medium of exchange without risks of forgery, inflation, and theft.\nDigital creators can set royalty clauses for all future sales of their work, generating recurring income as their works exchange hands and appreciate. NFTs make this possible through smart contracts that automatically enforce terms and clauses which are then permanently enshrined in the blockchain; immutable and transparent for anyone to check and verify against. This contrasts with the current system where artists are only paid for the initial sale of their works and where no redress can be automatically programmed to prevent exploitation from intermediaries and resellers.\nWith NFTs, the need for trust between buyers and sellers is minimized as there is a verifiable decentralized system to record ownership and track ownership changes; without which there would be no incentive for buyers to purchase these digital artworks or for sellers to sell them. Buyers need to be able to verify that Sellers can sell the assets and Sellers need to be able to transfer ownership rights to Buyers in a secure and transparent manner. Without clear property rights and an efficient process for exchange, transactions cannot be facilitated, and markets cannot function.\nBecause NFT ownership is easy to verify and transfer, new markets in a variety of goods can also be made possible. Real world assets like property, event passes, and services could also come with their own corresponding NFTs, solving age old barriers of trust and authenticity. For example, NFTs can speed up the process of buying property by replacing intermediaries with smart contracts to enable efficient and secure transfer of ownership. Converting a real estate property into an NFT enables for a quick and simple settlement process. As proof and history of ownership is directly recorded on the blockchain, transactions can be verified quickly and easily.\nTickets for sporting events, concerts and conferences could also be sold in an NFT form. Buyers would not have to worry about counterparty risk, fraudulent exchanges, and counterfeit tickets. Holders of these NFT ticket/passes can also securely sell to anyone in an efficient and transparent manner, where changes in ownership rights are instantaneously updated and verifiable.\nNFTs can also enable digital ownership for physical rare artifacts, without the need to relinquish physical ownership rights. For example, museums can sell paintings and art objects in the form of digital assets, which then enable them to create new revenue streams. In 2021, the Russian Hermitage Museum sold several digital representations of its exclusive art collection via NFTs in a move it described as making luxury artwork more accessible (Dafoe, 2022)."
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#utility-and-benefits",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#utility-and-benefits",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "Utility is the value that an asset provides to its holder. The more useful an asset is, the more people are willing to pay for that asset because of its associated benefits and practical uses after the initial purchase. The value of an NFT can increase by increasing the number of ways it can be used in the real world.\nNFT projects can offer utilities such as voting rights, real world items, memberships and access to events, private communities, fractional ownership and even the ability to earn passive income.\nAs NFT holders have a personal stake in the success of the project, several projects enable their holders to be able to vote and decide on the future direction of the project. NFT projects could have a treasury or pool of funds raised from initial sales, from which the NFT community can vote and decide on how to spend those funds; for example, creating new projects, planning new collaborations, or even advancing important social issues.\nThe decision-making process in the NFT project is shared with all members of the project by means of a Decentralized Autonomous Organization (DAO) where decision-making and influence is shared with all its members. In a traditional company, the CEO or management team would have made all decisions, having the final say in the company’s strategy and future direction. In NFT projects with voting rights, there is no centralization in the decision-making process and instead every NFT holder can have a voice and a say in what happens to the project. The decentralization of the decision-making process incentivises NFT holders to observe good behaviour to ensure success of the project.\nSome NFTs enable holders to obtain physical items in the real world. Fashion brands like Gucci and Nike use NFTs as a ticket to real world items. Holders of these NFTs can use their items both in the Metaverse or virtual worlds as well as in the real world. Nike, with its acquisition of RFTKT in December 2021 enables its NFT holders to obtain physical versions of its NFT sneakers (Ernest, 2022).\nIncreasingly, NFTs are also being used to as proof of memberships to exclusive benefits or access to unique events. For example, the music festival Coachella sells NFTs that grant lifetime access to virtual and physical events as well as physical and digital items (Bierman, 2022). Holders of these NFTs can also enjoy benefits like first access to product launches and exclusive entry passes for virtual and in-person events.\nNFT projects like Gary Vaynerchuk’s VeeFriends include access to events like conferences, educational talks, and direct access to thought leaders and leading entrepreneurs (Thomas, 2022). NFT holders can buy and sell these NFTs in secondary markets, where the real-world benefits further enhance the value of these NFTs.\nNFT projects also provide their holders access to private communities and Discord channels, allowing holders the opportunity to participate and connect to exclusive communities or to create their own exclusive communities. Discord especially, has been the go-to online platform, enabling a diverse set of people to come together to collaborate and share information. In NFT projects like the World of Women project, these channels function as valuable platform for members to share ideas, knowledge, opportunities, and to connect with future business partners (Beyer, 2022).\nSome NFT projects enable fractional ownership, where one could own part of an NFT that would be otherwise be too expensive to own outright. Sellers can sell part of their asset without giving up complete ownership and buyers can own part of the NFT, reducing their risk of committing substantial amounts of capital for whole ownership. Fractional ownership also injects liquidity to the NFT project by allowing smaller investors to own assets collectively.\nIn June 2021, the Doge meme NFT, was bought for $4 million by PleasrDAO which then sold fractional ownerships to fans (Locke, 2021). Axie Infinity, an NFT-based online video game, also sells fractionalized ownerships of rare ‘Axies’, one of its most popular in-game NFT assets (Takyar, 2022).\nAs NFTs become more popular, owning a complete NFT become more expensive. While fractional ownership can lower the cost of ownership, it does not necessarily dilute the overall value of that NFT. When a fractional owner re-sells his fraction at a lower price, it does not lower the value of the remaining token for other fractional holders. Similarly, even if the token’s value skyrockets, buyers can still come in to buy at lower prices to own a smaller fraction of that NFT.\nAnother utility common for gaming NFTs is merging or breeding. For example, in the Crypto Kitties project, NFT holders can ‘breed’ their existing Crypto Cats to create new NFTs which they can choose to add to their collection or sell in the secondary market. For some, the new NFTs can fetch a higher value than existing tokens (Cryptokittes, 2018).\nSome NFT projects allow their holders to stake their NFTs to earn additional tokens or yield. NFT holders send their NFTs to a pool or a wallet to lock up their tokens, reducing the supply of tokens and maintaining the scarcity value of remaining tokens. Holders are then rewarded with native tokens during the period of staking.\nTo reward NFT holders for their support and efforts in building awareness and interest for the project, many projects also frequently ‘airdrop’ new benefits and free NFTs into holder’s wallets. NFT ownership facilitates a direct relationship between creators/founders and their NFT holders who function as the unofficial marketing team of the project. This close bond between community and project shifts the company-customer relationship to be more of a partnership with shared and aligned interests.\nDesigning utilities that provides recurring benefits to NFT holders encourage continued support for these NFT projects and instils greater satisfaction in ownership. In comparison, owning physical pieces of art or collectibles provides no real benefit other than price appreciation, which can only be realised upon selling those assets."
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#community-and-inclusion",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#community-and-inclusion",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "A network effect is a phenomenon where the intrinsic value of a product or good increases as more people use it. For example, a social media platform is only valuable if many people use it. The more active users an NFT project has, the more valuable it becomes for everyone involved. Like cryptocurrencies, where a token’s worth comes from users’ shared agreement, the bigger and more engaged the NFT community is, the higher the project’s value.\nIt is not surprising why the two most valuable NFT projects Cryptopunks and Bored Ape Yacht Club have been able to rise in prominence, with their legions of fans and recognition from social and mainstream media and influential people.\nFor these projects, buyers are not necessarily buying the NFTs for the art itself but for the community and exclusivity it accords to the holder. For example, Bored Ape Yacht Club (BAYC) NFT holders are immediately welcomed into an exclusive community with private Discord channels, member-only events, and networking sessions with other BAYC holders including celebrities and high-profile individuals.\nBeyond hype and profitability, communities that rally behind the same beliefs and connect to a greater cause have also grown in numbers and prominence within the NFT ecosystem. Projects with clearly defined roadmaps and social missions can effectively rally people together and add value to their lives. NFT communities provide a space and platform for people to share ideas, collaborate and support each other across borders, social circumstances, and languages. They can disrupt traditional industries, provide a platform to encourage entrepreneurship and come together to advance various social causes.\nThe World of Women (WOW) NFT project is an artwork collection depicting women-centric characters and profile pictures (Beyer, 2022). The project builds and drives a community that rallies behind a movement to promote greater diversity, representation, and inclusion of women in the web3 space. The WOW project provides free mentoring and education, as well as channels for members to network and develop business opportunities across their userbase and network of corporate partners.\nThis project also supports emerging artists and non-profits globally, with a percentage of all sales going to women-centric causes. Through the funding of projects important to women interests, the project inspires and nurtures a new generation of women to explore new opportunities in Web3 and technology.\nFor any NFT project to be successful and have long-term growth and value, a constantly engaged community is crucial. Belonging to a community is intrinsic to human nature. Whether it be niche interests, identity, or beliefs, we all wish to form groups where we can interact with like-minded individuals. The more a community becomes part of people’s personal identities, the more the value of that community is reinforced.\nWithout an active and engaged community of users, NFT projects can fail to take off, or would rapidly collapse as users lose interest. This lack of engagement and vigour in the community can eventually lead to the devaluation of the project and the NFTs directly."
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#social-impact",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#social-impact",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "NFTs and its communities has enabled new ways of thinking about social responsibility. There is an increasing trend of new NFT projects with clearly defined road maps that explain how they intend to use their proceeds to fund projects to advance social causes and how to use their technology to improve the world around them. Projects have also purposefully allocated part of their proceeds to charitable and social causes, which in turn increase the interest in these NFT projects from people who identify with these causes.\nSome of the charitable and social causes adopted by NFT projects include climate change, mental health, saving endangered animals, among many others.\nFor example, Apocalyptic Apes is a side project of the Bored Ape Yacht Club NFT project, focusing on climate issues through ocean clean up and reduction of plastic waste (Vick, 2022).\nThe CleanOcean NFT project provides a reward system for volunteering activities like cleaning polluted beaches, parks, or lands (block.co, 2022). The project receives funds from donors and brings together volunteers and enthusiasts who are rewarded with tokens for their contributions to cleaning up contaminated oceans and lakes.\nThe Blazed Cats NFT project features an artwork collection of cat jpegs with a variety of customizations (Blazed Cats, n.d.). 25% of new mint profits and 100% of secondary royalties goes to Mental Health America, which was chosen as the sole beneficiary by its discord community. This project raised over $215,000 in its first two weeks and enables Mental Health America to generate perpetual revenue from secondary sales of their NFTs.\nNFTs provide a new and innovative way to raise funds for Non-profit organizations and charities where proceeds from NFT auctions are used to support the non-profit’s mission. Selling NFTs enable non-profits to access new sources of funding outside of established channels and help them diversify their fund-raising streams. NFTs have also enabled non-profits and charities to continually receive income from secondary sales of these tokens. Each NFT that is purchased and subsequently resold provides perpetual revenue streams.\nFor example, a Singaporean non-profit organization called The Good Blockchain minted unique National Day Parade-themed NFTs featuring artworks by artists with disabilities to raise funds for SG enable, a local agency that empowers persons with disabilities to integrate with society (Automata Network, 2022). Similarly, in April 2022, The Food Bank Singapore also auctioned several NFTs of its mascot to raise funds for its meals and pantry programme(Auto, 2022)."
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#conclusion",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#conclusion",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "NFTs have become the new social identity, enabling people to display the causes to which they are aligned to; functioning as social signalling tools to show what communities they belong to and what they believe in. By fostering a large and vibrant community centred around more than art or profits, the NFT space has become a place where people can collectively come together to effect real change and do positive things to improve the world around them. This is a good example on how NFTs have grown beyond the initial speculative hype by offering a utility that promotes positive change. Through its own way, NFTs help tackle important problems like climate, inequalities, and socio-economic issues by increasing awareness, building and driving community engagement and enabling participation."
  },
  {
    "objectID": "Article-ex/Article-Ex4/Article_Ex4.html#references",
    "href": "Article-ex/Article-Ex4/Article_Ex4.html#references",
    "title": "Key characteristics that create or inject value for a non-fungible token",
    "section": "",
    "text": "Auto, H. (2022, April 21). S’pore food charity auctions art NFTs to raise funds for meals and pantry programme. The Straits Times. https://www.straitstimes.com/singapore/community/spore-food-charity-is-auctioning-art-nfts-to-raise-funds-for-meals-and-pantry-programme  [Accessed 26 October 2022]\nAutomata Network. (2022, August 9). NFTFair supports NDP2022 NFT fundraising campaign - Automata Network. Medium. https://medium.com/atanetwork/nftfair-supports-ndp2022-nft-fundraising-campaign-32398b03bcd8 [Accessed 26 October 2022]\nBeyer, E. J. (2022, July 5). The Ultimate Guide to World of Women: A Pillar of Inclusivity in NFTs. Nft Now. https://nftnow.com/guides/the-ultimate-guide-to-world-of-women-a-pillar-of-inclusivity-in-nfts/ [Accessed 26 October 2022]\nBierman, B. (2022, February 18). Inside the Sale of Coachella’s Lifetime Passes and $1.4 Million NFT Collection. EDM.com - the Latest Electronic Dance Music News, Reviews & Artists. https://edm.com/gear-tech/coachella-sells-1-4-million-nft-lifetime-passes [Accessed 26 October 2022]\nBlazed Cats. (n.d.). Retrieved October 25, 2022, from https://blazedcats.com/ [Accessed 26 October 2022]\nBlock.co. (2022, September 12). NFTs for Social Good - block.co. Medium. https://blockdotco.medium.com/nfts-for-social-good-2b3e43b31e [Accessed 26 October 2022]\nCryptoKitties. (2018, July 26). Getting started with CryptoKitties Part Two: Buying and Breeding. Medium. https://medium.com/cryptokitties/getting-started-with-cryptokitties-part-two-buying-and-breeding-792502e54a4d [Accessed 26 October 2022]\nDafoe, T. (2022, January 27). Russia’s Hermitage Museum Will Auction Off NFTs of Prized Works by Leonardo, Van Gogh, and Other Artists in Its Collection. Artnet News. https://news.artnet.com/art-world/hermitage-museum-auctioning-nfts-1992830 [Accessed 26 October 2022]\nErnest, M. (2022, June 29). Nike’s RTFKT Studios is offering Air Force 1 NFTs with real sneakers to match. Input. https://www.inputmag.com/style/nike-rtfkt-studios-air-force-1-nft-real-sneakers-redeem [Accessed 26 October 2022]\nKaczynski, S., & Kominers, S. D. (2021, November 19). How NFTs Create Value. Harvard Business Review. https://hbr.org/2021/11/how-nfts-create-value [Accessed 26 October 2022]\nKostic, E. (2022, March 10). What Could NFTs Mean for Nonprofits? Ad Council Org. https://www.adcouncil.org/all-articles/what-could-nfts-mean-for-nonprofits [Accessed 26 October 2022]\nNg, K. (2022, April 12). How Southeast Asia’s NFT ecosystem lifts up local artists. Forkast. https://forkast.news/how-southeast-asias-nft-ecosystem-lifts-up-local-artists/ [Accessed 26 October 2022]\nThomas, L. (2022, October 4). Gary Vee’s NFTs: A Guide to VeeFriends and the Man Taking Over Web3. Nft Now. https://nftnow.com/guides/ultimate-guide-to-veefriends/ [Accessed 26 October 2022]\nVick, A. (2022, February 24). How NFTs Are Creating Social Value. Forbes. https://www.forbes.com/sites/forbestechcouncil/2022/02/24/how-nfts-are-creating-social-value/?sh=62a217ecb98c [Accessed 26 October 2022]"
  },
  {
    "objectID": "Python-ex/Python-Ex1/Python_Ex1.html",
    "href": "Python-ex/Python-Ex1/Python_Ex1.html",
    "title": "Automate the design of a 3-stocks portfolio for a given target return",
    "section": "",
    "text": "Overview\n\n\n\n\n\n\n\n\nImporting modules\n\n\n\nRead and Format the data\n\n\n\nGetting month-end dates and prices of data file\n\n\n\nGetting month-end dates and prices of analysis period\n\n\n\nGetting the stock universe\n\n\n\n\nIdentifying the 3 stocks for portfolio construction\n\n\n\nAllocation of optimised portfolio\nDesign consideration for only 3 stocks chosen to form the portfolio. Based on 3 stock design and the min_weight variable that is assigned to each of the 3 stocks, the procedure to find the optimised portfolio will be as follows:\n\nMaximum weight to be distributed will be determined as 100% - (min weight * no. of stocks)\nThe weight will be randomly distributed over the 3 stocks, and the random distribution will be iterated over 10% of the total possible combinations possible based on max weight to be distributed and 3 stocks in portfolio\nAssumption that the random sample will be enough to get the optimal distribution and decision for the random sample is to alleviate the computational power required to calculate all possible combinations\nEach random distribution of weight will then be test by calculating the Sharpe ratio of the portfolio with that weight distribution\nThe results of the random test will be plotted on a graph to find the distribution with the highest Sharpe Ratio\n\n\n\n\n\n\n\n\n\nReporting the required results\n\n\n\n\nAppendix\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "",
    "text": "This study focuses on applying and evaluating various time series analysis models for the prediction of stock prices, with an emphasis on understanding the nuances, behaviour, and potential accuracy of each model. While our study conducts a comparative analysis, its primary purpose is to delve deep into the insights and observations we obtained during the application of these models on the same dataset. Our work revolves around using ARIMA, LSTM, and FB Prophet models to forecast future stock prices of Microsoft (MSFT). This stock was selected based on its high trading volumes and considerable market influence. For our dataset, we chose a period of five years, from June 2018 to June 2023, which covers the period before, during and after the pandemic. While longer time periods were considered, we felt that the market environment and price action during this 5-year period would suffice to represent a varied set of data more meaningful than any recent period in the past.  FB Prophet serves as the benchmark tool against which the other models were evaluated, utilizing the Root Mean Squared Error (RMSE) as the primary performance metric. An interesting facet of this investigation was the exploration of a hybrid model approach, combining the strengths of ARIMA and LSTM. This allowed for the examination of both short-term and long-term predictive capabilities of these models, and their effectiveness when combined. Ultimately, this study underscores that there is no one-size-fits-all model for stock price prediction. Each model’s effectiveness depends significantly on various factors such as the length of the forecast period, the nature of the data, and the specific application context."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#data-collection",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#data-collection",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.1 Data collection",
    "text": "2.1 Data collection\nWe gathered historical stock price data for Microsoft (MSFT) from Yahoo Finance. Our data spans a period of five years, from June 2018 to June 2023, which includes both pre- and post-pandemic periods."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#data-pre-processing",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#data-pre-processing",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.2 Data Pre-processing",
    "text": "2.2 Data Pre-processing\nTo prepare the data for our models, we carried out several pre-processing steps. These steps may vary between models, but they generally include handling missing data, normalizing or standardizing the data, and potentially detrending or de-seasonalizing the data where necessary."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#model-selection-and-implementation",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#model-selection-and-implementation",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.3 Model Selection and Implementation",
    "text": "2.3 Model Selection and Implementation\nWe selected several time-series forecasting models for our study, including ARIMA, LSTM, and a novel hybrid approach. These models were implemented using the Python programming language with the help of libraries such as statsmodels for ARIMA, Keras for LSTM, and FB Prophet for our benchmark model. We are also exploring some other models to see if they have any novel results."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#parameter-adjustments",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#parameter-adjustments",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.4 Parameter Adjustments",
    "text": "2.4 Parameter Adjustments\nEach model was fine-tuned to optimize performance. This involved adjusting various parameters specific to each model to minimize forecasting error. For example, for ARIMA, we determined the optimal order parameters (p, d, q) using techniques like the AIC criterion or PACF and ACF plots."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#model-evaluation",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#model-evaluation",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.5 Model Evaluation",
    "text": "2.5 Model Evaluation\nTo evaluate the performance of our models, we split our data set into a training set (2018-06-15 to 2023-1-15) and a test set (2023-1-15 to 2023-6-15). The models were trained on the training data and their predictions were compared to the actual prices in the test set. We used several metrics to measure the accuracy of the models’ predictions, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE)."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#analysis-and-conclusion",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#analysis-and-conclusion",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.6 Analysis and Conclusion",
    "text": "2.6 Analysis and Conclusion\nFinally, we conducted a comparative analysis of the performance of each model. We discussed the strengths and weaknesses of each model, insights gained, and the implications for real-world investment decision-making."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#common-metrics-for-time-series-analyses",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#common-metrics-for-time-series-analyses",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "2.7 Common metrics for Time series analyses",
    "text": "2.7 Common metrics for Time series analyses\nThe below metrics will be used to evaluate our model results."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#auto-regressive-intergration-moving-ave-arima",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#auto-regressive-intergration-moving-ave-arima",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "3.1 Auto-Regressive Intergration Moving Ave (ARIMA)",
    "text": "3.1 Auto-Regressive Intergration Moving Ave (ARIMA)\nThe traditional statistical model we adopted in the time series analysis is the ARIMA model. It is a combination of autoregressive (AR) and moving average (MA). “I” in the middle means that the model employs differencing of raw observations to make the time-series stationary. ARIMA can be used for modeling stationary time series or non-stationary time series that can become stationary through differentiation. Stationary series are those whose expected value and variance do not change over time.\nAn ARIMA model is defined by its three order parameters, p, d, q. The parameter p represents the number of autoregressive terms or the number of “lag observations”. It determines the outcome of the model by providing lagged data points. The parameter d, known as the degree of differencing, indicates how many times the lagged indicators have been subtracted to achieve data stationarity. The parameter q signifies the number of forecast errors in the model and is often referred to as the size of the moving average window.\nThe process of building the ARIMA model can be divided into three phases. In the first phase is to check if the time series is stationary by examining ACF, PACF, and Dickey-Fuller tests. The second phase is to choose parameters and the performance of the model would be tested in the third phase."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#long-short-term-memory-lstm",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#long-short-term-memory-lstm",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "3.2 Long Short-Term Memory (LSTM)",
    "text": "3.2 Long Short-Term Memory (LSTM)\nLSTM is a recurrent neural network (RNN) model and is widely adopted in the time series analysis. The main idea behind LSTM cells is to learn the important parts of the sequence seen so far and forget the less important ones. LSTM cells are designed to deal with the problem of “long-term dependence” and the memory capacity of standard recurrent cell is improved by introducing a gate into the cell. In particular, three types of gates are involved in each cell, namely forget gate, memory gate, and output gate. In cells, data can be disposed, filtered, or added for the next cells by using gates. The amount of data entering a cell at a time is referred to as “batch size”. The number of complete runs through the data is known as an “epoch”.\nIn the stock price prediction, we can put the historical close prices into the model and tune the hyperparameters to optimize the model performance. Some important hyperparameters include units, epochs, optimizer, input shape, batch size etc."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#facebook-prophet",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#facebook-prophet",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "3.3 Facebook Prophet",
    "text": "3.3 Facebook Prophet\nProphet was developed by Facebook as an algorithm for the prediction of time series values for different business applications. Prophet is a useful tool for time series forecasting. It incorporates non-linear trends, as well as yearly, weekly, and daily seasonality patterns, and prides itself on being able to predict holiday effects. It excels in forecasting time series that exhibit significant seasonal variations and possess a substantial historical data spanning multiple seasons.    \nIt is an additive model consisting of four components: g(t) is a trend to capture the general trend of the series. s(t) is a seasonality component and h(t) is a Holidays component. The error term εt stands for random fluctuations that cannot be explained by the model."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#arima-lstm-hybrid",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#arima-lstm-hybrid",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "3.4 ARIMA & LSTM Hybrid",
    "text": "3.4 ARIMA & LSTM Hybrid\nBoth ARIMA and LSTM models have their own strengths and weaknesses. ARIMA has difficulties with non-linear time series data whereas neural networks can work with linear and non-linear time series data. However, LSTM requires long training time and parameter selection can be challenging. To leverage the strengths of both models, a hybrid approach is implemented to capitalize on their respective expertise. The aim is to achieve more accurate predictions compared to using either model individually. In this hybrid model, the ARIMA component is applied to capture the trend component, while the LSTM component is utilized to model both the seasonal and residual components of the time series. By combining the two models, it is expected to enhance the overall forecasting performance and capture a broader range of patterns in the data.\nHere is a summary of all the adopted models in our project:\n\nTable 1: Model Comparison\n\n\n\n\n\n\n\n\n\n\nType\nName\nConcept\nApplications\nPros\nCons\n\n\n\n\nStatistical model\nARIMA\nA model that characterizes time series by the aspects of autoregressive, integrated, and moving average terms.\nAnalyse and forecast time series data\nSimple and interpretable.\nAccurate in short term prediction\nLimitation in non-linear patterns\nSensitivity in parameter selection\n\n\nRecurrent neural network (RNN) model\nLSTM\nLong-Short Term Memory model capable of learning order dependence for sequence prediction issues.\nDeep learning, speech recognition, machine translation\nCapture long-term and non-linear patterns\nDo not rely on specific data assumptions\nComputation extensive\nRequire more data\nLess interpretable\n\n\nStatistical & machine learning\nFB PROPHET\nAn additive model developed by Facebook, for forecasting time series data. Used best for time series with strong seasonal effects and a long history.\nForecast business results for planning and goal setting.\nSimple interface\nCapture seasonality and patterns\nLess data prepossessing\nLimited to univariant forecasting\nLess customized\n\n\nHybrid Model\nARIMA + LSTM (hybrid)\nHybrid LSTM and ARIMA model combines LSTM’s non-linear capabilities with ARIMA’s ability to capture linear trends.\nTime series prediction and integrate non-linear trend\nComprehensive\nImprove model performance\nAdaptability\nComplexity\nFurther studies required in model architecture"
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#arima",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#arima",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "4.1 ARIMA",
    "text": "4.1 ARIMA\nWhen using ARIMA, we initially wanted to see how accurate the model could be for predicting longer time frames. For this purpose, we used a Train-test split configuration where our test range would span a 5- month period (2023-1-15 to 2023-6-15). We then checked the data for stationarity using the Dickey-Fuller test. We obtained a p-value of 0.857 which confirmed that the data was not stationary. We performed a pairwise differencing to make this data stationary. The resultant stationarity can be observed visually below, with the differenced data now centred around the mean. We can also confirm this via another Dickey-Fuller test, where we now obtain a revised p-value closer to zero.\nWhen determining the parameters for the AR and MA parts of the model, we plotted the ACF and PACF plots for the differenced data to determine the values of p and q. Here, an ARIMA model of order (1,1,1) seemed reasonable to be used for our Manual-Arima workflow.  At the same time, we also used the Auto-Arima function to obtain the optimal (p,d,q) values which had the lowest values of AIC and BIC (Auto-Arima workflow). This recommended an ARIMA model of (4,1,1).  We compared both workflows and proceeded on with the Auto-Arima which had scored better in the MAE, MAPE and RSME metrics.\n\n\n\nFig 2: Pairwise Differencing\n\n\n\n\n\nFig 3: ACF and PACF\n\n\nWe then applied this model to our test data range (5 months) and obtained predictions as show in the chart below, which seems to predict a “flat” stock price for the next 5 months. This runs contrary to price dynamics for actively traded stocks, where prices are not likely to remain the same daily. Here, we concluded that the ARIMA model could not derive meaningful results when predicting for longer time frames and that longer term forecasting using ARIMA were prone to converging at the mean.  The ARIMA model only uses the previous X observations to make a prediction. Furthermore, prediction errors accumulate, and this results in the ARIMA model simply predicting a straight line for longer time frames. We therefore changed our test range to a shorter time frame of 1 month and re-visited the above steps to see the difference. We noted that our RSME score improved to 16.399 from the previous 46.55, and that our model no longer makes a ‘flat’ prediction of prices.\n\n\n\nFig 4: ARIMA results\n\n\n\n\n\nFig 5: Auto Arima Results\n\n\nThis leads us to conclude that the ARIMA model, on its own, would only be suited to make short term price predictions (a few days out). Next, we explore other models which may be better suited for making longer term predictions."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#lstm",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#lstm",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "4.2 LSTM",
    "text": "4.2 LSTM\nFor our testing we used LSTM in 2 ways – the first was with “default” settings and the second was with parameter hyper-tuning. The results varied wildly and yielded many interesting insights. Both used the Adaptive Moment Estimation (Adam) optimizer. Although there are many other alternative optimizers, adam was selected not only because it is widely used (and as a result, widely watched), but also because it is specifically designed for training deep learning neural networks. We would have used others such as adagrad, adadelta, RMSprop and the plethora of optimizers out there but time is a factor, and it seems it might stray from our topic at hand.\nThe “default” LSTM was run with a batch size of 1 and an epoch of 3. The RMSE hovers around 7-9 across runs and although the model was unable to predict precise price points, it accurately predicted trends on when the stock went up and down. (figure below)\n\n\n\nFig 6: LSTM Model 1 results\n\n\nThe second model, with hyper-tuning, showed much more promise but tended to overfit. It trades robustness for more accuracy. The reverse of the above happened, where stock prices were predicted with a much higher degree of precision, some days a few cents off but overfit. RMSE drops to as low as 1 or even 0.05 (figure of extreme example below). What happens is that the model ends up using the previous day’s close price to predict the next day’s price. If you shift the predictions one day backwards it makes an almost perfect overlay. I propose that the best would be to use hyper-tuning with a bigger range of stocks so that the model can learn more meaningful patterns. Finer tuning of parameters also affects the results as well, and as such further experimentation could be done.\n\n\n\nFig 7: LSTM Model 2 results"
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#fb-prophet",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#fb-prophet",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "4.3 FB Prophet",
    "text": "4.3 FB Prophet\nThere is not much to mention on our process for using FB Prophet since most of it was straightforward. There was no parameter tuning and such. The Prophet package provides intuitive parameters which are easy to use and specify for trend, seasonality and holiday.\nHowever, to note is that the RMSE of this model is quite high as compared to the LSTM since it tries to give more of a ballpark estimate rather than a precise number. By that standard, if we consider whether the actual number lands within the predicted upper and lower bounds it is more accurate than simply taking it at face value. The RMSE typically floats around 18.8 with a minor deviation past 2 decimal places.\nThe 3 figures below in order of presentation shows the overall prediction period (“zoomed out”), day-to-day view for predictions (“zoomed in”), and finally the table of numbers for a more numerical view of the predictions. One thing to note is that each red dot is a single day.\n\n\n\nFig 8: FB Prophet Predictions\n\n\n\n\n\nFig 9: FB Prophet Prediction Range"
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#moving-averages-linear-regression-k-nearest-neighbours",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#moving-averages-linear-regression-k-nearest-neighbours",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "4.4 Moving Averages, Linear Regression & K-Nearest Neighbours",
    "text": "4.4 Moving Averages, Linear Regression & K-Nearest Neighbours\nAs part of our study, we also explored the use of other models commonly used in trading. Moving averages and Linear regression form the basis for commonly used Technical Analysis indicators like Exponential Moving Averages, Bollinger bands and Linear Regression Channels. We applied the same dataset and specified a 75-25% train-test split.\nMoving averages are widely used to smooth out fluctuations and identify trends in stock prices. They calculate the average value of a stock’s price over a specified period and are commonly used to generate buy or sell signals. Our moving average model drew a RMSE score of 38.46 but was not able to adjust to sudden shifts in trends, opting for an average between the two price boundaries.\nLinear regression is a statistical technique used to model the relationship between a dependent variable (stock price) and one or more independent variables (time). In stock price prediction, linear regression could be used to fit a straight line to historical data, capturing the overall trend. However, our Linear Regression model seemed to fare worse than our Moving Average model, with an RMSE score of 89.25. The model was only able to project future values based on a fitted linear trendline. While Linear Regression can be simple to use and intuitive, it fails to account for more complex, non-linear trends. Furthermore, it assumes that the relationship between the dependent and independent variables remain constant.\n\n\n\nFig 10: MA predictions\n\n\n\n\n\nFig 11: Linear Regression Predictions\n\n\n\nK-Nearest Neighbours (KNN)\nK-nearest neighbour is a commonly used Machine Learning algorithm used for classification and regression tasks in various fields including finance. In the context of stock price prediction, we wanted to explore if the KNN model could be used to predict future prices based on the historical prices of K nearest neighbours. While KNN is more commonly used for classification tasks, it could also be applied to regression problems.  Our KNN model fared better with an RMSE score of 28.65 but was not able to pick up on any trends in future prices.\n\n\n\nFig 12: KNN Model Results\n\n\n\n\n\nFig 13: LSTM Baseline Model results\n\n\nWe concluded that the 3 models performed worse that our LSTM base model, leading us to conclude that they are unable to capture temporal dynamics and patterns inherent in stock price data as effectively as other time series forecasting models."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#arima-lstm-hybrid-1",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#arima-lstm-hybrid-1",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "4.5 ARIMA & LSTM HYBRID",
    "text": "4.5 ARIMA & LSTM HYBRID\nThe dominant model is in the hybrid is LSTM, as the ARIMA model does not have enough lags to predict the long-term modelling conditions. To improve our model, we have considered additional features from technical analysis such as Bollinger Bands, but ARIMA cannot incorporate those features as it lacks feature selection.\n\n\n\nFig 14: Combined Predictions"
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#network-models",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#network-models",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "4.6 Network Models",
    "text": "4.6 Network Models\n\n\n\nFig 15: Simple Network Model\n\n\n\n\n\nFig 16: Ideal Network model (Future scope)\n\n\nNetwork Models aid in modelling relationships between varioud stocks. The key advantage of network modelling is capturing complex relationships between various stocks through network dynamics (opinion, adoption, decision), and this would aid in predicting trends through various similarities such as Eigen Vector Similarity, Jaccard Similarity etc. These models would boost the accuracy of predictions as features generated here can be used for our LSTM models. While the network we generated was simple and able to model stocks closest to and most different from MSFT for hedging, a more complex model can build more features and explore relationships such as Key Opinion Leader Stocks for systemic importance, Diversity Centrality to enable diversified portfolios etc."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#model-limitations",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#model-limitations",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "5.1 Model Limitations",
    "text": "5.1 Model Limitations\nThe ARIMA Model is limited by the number of lags available in the equation. This makes feature selection inherently impossible. Additionally, many of our other models such as the KNN were found to not capture information on the time series. LSTM while being the best performing model, is extremely computationally intense."
  },
  {
    "objectID": "Python-ex/Python-Ex3/Python_Ex3.html#data-set-and-hyper-parameter-tuning",
    "href": "Python-ex/Python-Ex3/Python_Ex3.html#data-set-and-hyper-parameter-tuning",
    "title": "Project - Predicting Stock Prices through Time series Analysis",
    "section": "5.2 Data set and Hyper parameter tuning",
    "text": "5.2 Data set and Hyper parameter tuning\nWhile prediction on just the time series trend may give us some information, considering more features such as the technical analysis or fundamental analysis would improve our results considerably. To model complex relationships, we could consider network analysis as well as building more technical analysis indicators as features in our model (as we have in our hybrid model)"
  },
  {
    "objectID": "R-ex/R-Ex2/R_Ex2.html",
    "href": "R-ex/R-Ex2/R_Ex2.html",
    "title": "Take-home Exercise 1a- 5 Exploratory Data Analyses on Pisa data",
    "section": "",
    "text": "Setting the Scene\nOECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several of Singapore’s ministers for Education also started an “every school a good school” slogan. The general public, however, believes that there are still disparities that exist, especially between “elite” and neighborhood schools, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\n\n\nThe Task\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA’s global education survey runs every three years to assess education systems worldwide through the testing 15 year old students in the subjects of mathematics, reading, and science.\nIn this take-home exercise, we will use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\n\n\n\nGetting Started\nLoading R packages\n\npacman::p_load(tidyverse, haven, ggdist, ggridges, ggthemes,\n               colorspace)\n\nImporting the Data\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\nLoading the instvy package.\n\n# install.packages(\"intsvy\",repos = \"http://cran.us.r-project.org\")\n\nlibrary(\"intsvy\")\n\nExtracting the overall student mean score values for each subject\n\nMath_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", data=stu_qqq_SG)\n\nRead_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", data=stu_qqq_SG)\n\nSCIE_mean_SG &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", data=stu_qqq_SG)\n\n\n\n1) Distribution of scores in the student cohort\nWe will use the code below to plot histograms to show the distribution of scores across the 3 subjects.\n\n\nShow the code\n# Create the histogram plot with an annotated mean line using Math_mean_SG\nplt1 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightblue') +\n  labs(x = \"Math Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = Math_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Math_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Math_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create the histogram plot with an annotated mean line using Read_mean_SG\nplt2 &lt;- ggplot(stu_qqq_SG, aes(x = PV1READ)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightgreen') +\n  labs(x = \"Reading Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = Read_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = Read_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(Read_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n\n# Create the histogram plot with an annotated mean line using Science_mean_SG\nplt3 &lt;- ggplot(stu_qqq_SG, aes(x = PV1SCIE)) +\n  geom_histogram(binwidth = 20, color = \"white\", fill='lightpink') +\n  labs(x = \"Science Scores\",\n       y = \"Count\") +\n  geom_vline(xintercept = SCIE_mean_SG$Mean,\n             col = 'black',\n             size = 0.5,\n             linetype = \"dashed\") +\n  geom_text(aes(x = SCIE_mean_SG$Mean, y = 100, label = paste(\"Mean =\", round(SCIE_mean_SG$Mean, 2))),\n            color = \"black\", hjust = -0.1, vjust = 1.0) +  # Adjust label position\n  theme_minimal()\n\n# Create a single plot with density plots for Math, Reading, and Science scores\nplt4 &lt;- ggplot(stu_qqq_SG, aes(x = PV1MATH, fill = \"Math\")) +\n  geom_density(alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1READ, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(data = stu_qqq_SG, aes(x = PV1SCIE, fill = \"Science\"), alpha = 0.5) +\n  labs(x = \"Subject Scores\",\n       y = \"Density\") +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  guides(fill = FALSE) +  # Remove the legend\n  theme_minimal()\n\n\nWe use the code below to create a composite plot.\n\n\nShow the code\nlibrary(patchwork)\n\npatch1 &lt;- (plt1+plt2) / (plt3+plt4)  + \n              plot_annotation(\n                title = \"Distribution of student performance in Math, Reading and Science\")\n\npatch1 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 1\n\n\n\nThe distribution of scores seem to resemble a normal distribution across all 3 subjects. Singaporean students seem to have a higher mean score In Mathematics relative to Reading and Science.\nFurther statistical tests like the Anderson-Darling or Shapiro-Wilk tests will need to be conducted to confirm the normality in distribution.\n\n\n\n\n2) Relationship between Scores and School ID\nThe pisa.mean.pv() function from the instvy package enables us to calculate the mean scores from the 10 Plausible Values and enables us to further group by the School ID (CNTSCHID).\nIn the code below, we will create separate tables for the mean scores for each subject by different School Ids.\n\nSchoolid_math &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"MATH\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_read &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"READ\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nSchoolid_scie &lt;- pisa.mean.pv(pvlabel = paste0(\"PV\",1:10,\"SCIE\"), by = \"CNTSCHID\", data = stu_qqq_SG)\n\nNext, we use the below code to plot bubble plots to examine the number of students and their mean scores for each school. We will also use the plotly package for added interactivity.\n\nMean Math scores across SchoolsMean Reading scores across SchoolsMean Science scores across Schools\n\n\n\n\nShow the code\nlibrary(plotly)\n\nbest_sch_math &lt;- Schoolid_math %&gt;% filter(Mean == max(Mean))\nworst_sch_math &lt;- Schoolid_math %&gt;% filter(Mean == min(Mean))\n\n\np_1 &lt;- ggplot(Schoolid_math, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"skyblue\", high = \"darkblue\") +\n  labs(title = \"Mean Math Scores per School\",\n    y = \"Mean Math Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_1 &lt;- p_1 + \n  geom_text(data = best_sch_math, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_math, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n\n# Convert to an interactive plot\nggplotly(p_1, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_sch_read &lt;- Schoolid_read %&gt;% filter(Mean == max(Mean))\nworst_sch_read &lt;- Schoolid_read %&gt;% filter(Mean == min(Mean))\n\np_2 &lt;- ggplot(Schoolid_read, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"yellow\", high = \"darkorchid\") +\n  labs(title = \"Mean Reading Scores per School\",\n    y = \"Mean Reading Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_2 &lt;- p_2 + \n  geom_text(data = best_sch_read, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_read, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n# Convert to an interactive plot\nggplotly(p_2, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\nShow the code\nbest_sch_scie &lt;- Schoolid_scie %&gt;% filter(Mean == max(Mean))\nworst_sch_scie &lt;- Schoolid_scie %&gt;% filter(Mean == min(Mean))\n\n\np_3 &lt;- ggplot(Schoolid_scie, aes(x = CNTSCHID, y = Mean)) +\n  geom_point(aes(size = Freq, color = Freq), alpha = 0.5) +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"lightpink\", high = \"darkred\") +\n  labs(title = \"Mean Science Scores per School\",\n    y = \"Mean Science Scores\", \n    size = \"Number of Students\", \n    color = \"Number of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank())  # Remove minor grid lines\n\n# Annotate the best and worst mean scores\np_3 &lt;- p_3 + \n  geom_text(data = best_sch_scie, aes(label = \"Best\", y = Mean + 12), vjust = 1, hjust=-1) +\n  geom_text(data = worst_sch_scie, aes(label = \"Worst\", y = Mean - 12), vjust = 1)\n\n# Convert to an interactive plot\nggplotly(p_3, tooltip = c(\"x\", \"y\", \"size\", \"color\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 2\n\n\n\nThe ability to extract and assign Mean scores to individual schools enables us to further explore and examine the disparity in performance between schools. For example, looking at the two extremes of score results, we note that Schools (70200001 & 70200003) out perform other schools in Math and Science. On the other hand, Schools (7020115 & 70200149) under perform other schools in Math and Science.\nThis seems to indicate that there are still marked differences between the ‘’best” schools and the’‘worst’’ schools. Additional analysis could be done to identify the differences between these two sets of schools in terms of resources, teaching quality, and students attitudes or motivation etc, in order to fully understand the reason behind the difference in the scores.\n\n\n\n\n3) Relationship Between Gender and Scores\nFirst we create a subset of Gender and PV1 scores using the below code. We also convert the levels from 1 and 2, to Female and Male respectively.\n\n# Create a subset of the data with gender and PV1 score columns\nsubset_gender_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ST004D01T, PV1MATH, PV1SCIE, PV1READ)\n\n# Convert the \"ST004D01T\" column to a factor \nsubset_gender_PV1$ST004D01T &lt;- factor(subset_gender_PV1$ST004D01T, levels = c(1, 2), labels = c(\"Female\", \"Male\"))\n\nNext we plot the ridgeline plots with quantile lines.\n\n\nShow the code\nrp1 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1MATH, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Math Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\nrp2 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1READ, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Reading Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\nrp3 &lt;- ggplot(subset_gender_PV1,\n       aes(x = PV1SCIE, \n           y = ST004D01T, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  scale_x_continuous(\n    name = \"Science Scores\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  theme(legend.position = \"none\")\n\n\nWe use the code below to create a composite plot via patchwork\n\n\nShow the code\nlibrary(patchwork)\n\npatch6 &lt;- rp1 / rp2 / rp3 + \n              plot_annotation(\n                title = \"Male students seem to perform better in Math and Science\")\n\npatch6 & theme(panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 3\n\n\n\nMales students seem to outperform Female students in both Maths and Science. Female students seem to outperform Male students in Reading.\n\n\n\n\n4) Relationship between Scores and Socioeconomic status of students\nWe will create a new subset with ESCS and the PV1 scores for this visualization.\n\nsubset_ESCS_PV1 &lt;- stu_qqq_SG %&gt;%\n  select(ESCS, PV1MATH, PV1SCIE, PV1READ)\n\n#omiting NA values\nsubset_ESCS_PV1 &lt;- na.omit(subset_ESCS_PV1)\n\nUsing our new table subset_ESCS_PV1, we will create scatter plots for ESCS versus each PV1 score for each subject using the code below.\n\n\nShow the code\nc_coeff_ESCS_Math &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1MATH)\n\nC_plt1 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1MATH)) +\n  geom_point(color = \"lightblue\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1MATH),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Math, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n    y = \"Math Scores\") +\n  theme_minimal()\n\nc_coeff_ESCS_Read &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1READ)\n\nC_plt2 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1READ)) +\n  geom_point(color = \"lightgreen\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1READ),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Read, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n    y = \"Reading Scores\") +\n  theme_minimal()\n\nc_coeff_ESCS_Scie &lt;- cor(subset_ESCS_PV1$ESCS, subset_ESCS_PV1$PV1SCIE)\n\nC_plt3 &lt;- ggplot(subset_ESCS_PV1, aes(x = ESCS, y = PV1SCIE)) +\n  geom_point(color = \"lightpink\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, color = \"black\") +\n  geom_text(\n    x = max(subset_ESCS_PV1$ESCS),  \n    y = max(subset_ESCS_PV1$PV1SCIE),  \n    label = paste(\"Corr Coeff:\", round(c_coeff_ESCS_Scie, 2)),\n    hjust = 1,  # Adjust horizontal justification\n    vjust = 1   # Adjust vertical justification\n  ) +\n  labs(x = \"Socio-Economic Status (ESCS)\",\n       y = \"Science Scores\") +\n  theme_minimal()\n\n\nWe will use patchwork to create a composite plot for our scatter plots.\n\n\nShow the code\npatch4 &lt;- C_plt1 / C_plt2 / C_plt3 + \n              plot_annotation(\n                title = \"Weak positive relationship between Scores and ESCS\")\n\npatch4 & theme( axis.text.y = element_blank(),panel.grid.major = element_blank(),)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 4\n\n\n\nThere is a weak positive relationship between subject scores and Socioeconomic statuses. The ESCS score is a composite score calculated from three indicators: highest parental occupation status (HISEI), highest education of parents in years (PAREDINT), and home possessions (HOMEPOS).\nFurther analysis could be conducted on the individual components of the ESCS score to check for their individual influence on student performance.\n\n\n\n\n5) Examining the Breakdown of scores per Subject\nWe can further examine the percentage of students per score range for each subject. This might help us examine whether there are specific strengths or weaknesses in the student cohort.\nFirst, we use the pisa.ben.pv() function from the instvy package which calculates student scores from the 10 plausible values and calculates the percentage of students at each proficiency level (Score range) as defined by PISA.\nIn the code below, we will create separate tables for the percentage breakdown of scores for each subject.\n\n\nShow the code\nMath_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"MATH\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nRead_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"READ\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\nScie_Breakdown &lt;- pisa.ben.pv(pvlabel= paste0(\"PV\",1:10,\"SCIE\"), by=\"CNT\", atlevel=TRUE, data=stu_qqq_SG)\n\n\nNext, we can combine these tables and plots into one plot to show the percentage of students per Score Range for all subjects.\n\n\nShow the code\n# Creating a new combined table\n\nMath_Breakdown$Subject &lt;- 'Math'\nRead_Breakdown$Subject &lt;- 'Reading'\nScie_Breakdown$Subject &lt;- 'Science'\n\nCombined_Breakdown &lt;- bind_rows(Math_Breakdown, Read_Breakdown, Scie_Breakdown)\n\n\nThe code below enables us to plot the breakdown of scores for all subjects.\n\n\nShow the code\n# Order the Benchmarks factor based on the order it appears in the dataset\nCombined_Breakdown &lt;- Combined_Breakdown %&gt;%\n  mutate(Benchmarks = fct_inorder(Benchmarks))\n\n# Now plot using ggplot\nggplot(Combined_Breakdown, aes(x = Benchmarks, y = Percentage, fill = Subject)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +  # Dodge position for the bars\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percentage)),  # This will format the label to have 1 decimal place and a percentage sign\n    position = position_dodge(width = 0.9),  # Match the position of the text with the dodged bars\n    vjust = -0.25,   \n    size = 2  \n  ) +\n  scale_fill_manual(values = c(\"Math\" = \"lightblue\", \"Reading\" = \"lightgreen\", \"Science\" = \"lightpink\")) +\n  labs(title = \"For lower Score ranges, students seem to do better in Reading\",\n       x = \"Score Range\",\n       y = \"Percentage of Students\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 5\n\n\n\nCombined bar plots can allow us to obtain insights on relative performance. For example, for scores below 544.68, we can see that at lower score ranges, students seem to do better in Reading relative to Math and Science. However, at higher score ranges, students do worse in Reading relative to Math and Science.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html",
    "href": "R-ex/R-Ex4/R_Ex4.html",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to an official report as shown in the infographic below,\n\nThe intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer, and\nThe contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#installing-and-loading-r-packages",
    "href": "R-ex/R-Ex4/R_Ex4.html#installing-and-loading-r-packages",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "2.1 Installing and Loading R packages",
    "text": "2.1 Installing and Loading R packages\n\npacman::p_load(ungeviz, plotly, crosstalk, patchwork,\n               DT, ggdist, ggridges, ggstatsplot,ggthemes,\n               colorspace, gganimate, tidyverse, dplyr, \n               readr)"
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#importing-data-and-data-preparation",
    "href": "R-ex/R-Ex4/R_Ex4.html#importing-data-and-data-preparation",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "2.2 Importing Data and Data Preparation",
    "text": "2.2 Importing Data and Data Preparation\nI will download and import data for July (Dry) and December (Wet) for 1983, 1993, 2003, 2013, 2023.\n\nI have chosen Changi weather station, as it is one of the older weather stations with daily rainfall data recorded since 1981. For more information on the weather stations and the historical data available, please refer to this link.\n\n\nShow the code\n# List of file names\nDecember_files &lt;- c(\"data/DAILYDATA_S24_198312.csv\", \"data/DAILYDATA_S24_199312.csv\", \n                \"data/DAILYDATA_S24_200312.csv\", \"data/DAILYDATA_S24_201312.csv\", \n                \"data/DAILYDATA_S24_202312.csv\")\n\nJuly_files &lt;- c(\"data/DAILYDATA_S24_198307.csv\", \"data/DAILYDATA_S24_199307.csv\", \n                \"data/DAILYDATA_S24_200307.csv\", \"data/DAILYDATA_S24_201307.csv\", \n                \"data/DAILYDATA_S24_202307.csv\")\n\n\nUpon inspecting the csv files for the 5 years, the below columns were removed as they were only recorded since 2014:\n\nHighest  30-min Rainfall (mm)\nHighest  60-min Rainfall (mm)\nHighest 120-min Rainfall (mm)\n\nI will use the code below to import the csv files into our R environment\n\n# Reading and combining the CSV files for December\nDecember_data &lt;- lapply(December_files, read_csv, col_names = FALSE,\n                        col_select = c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13),\n                        skip = 1) %&gt;%\n  bind_rows(.id = \"file\")\n\n\n# Reading and combining the CSV files for July\nJuly_data &lt;- lapply(July_files, read_csv, col_names = FALSE,\n                        col_select = c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13),\n                        skip = 1) %&gt;%\n  bind_rows(.id = \"file\")\n\nI will use the code below to rename the column names for the data sets.\n\n\nShow the code\n# Renaming the columns\ncolnames(December_data) &lt;- c(\"ID\", \"Station\", \"Year\", \"Month\", \"Day\", \n                              \"Daily_Rainfall_Total_mm\", \"Mean_Temperature_C\", \n                              \"Maximum_Temperature_C\", \"Minimum_Temperature_C\", \n                              \"Mean_Wind_Speed_km_h\", \"Max_Wind_Speed_km_h\")\n\nDecember_data$Year &lt;- as.factor(December_data$Year)\n\ncolnames(July_data) &lt;- c(\"ID\", \"Station\", \"Year\", \"Month\", \"Day\", \n                              \"Daily_Rainfall_Total_mm\", \"Mean_Temperature_C\", \n                              \"Maximum_Temperature_C\", \"Minimum_Temperature_C\", \n                              \"Mean_Wind_Speed_km_h\", \"Max_Wind_Speed_km_h\")\n\nJuly_data$Year &lt;- as.factor(July_data$Year)\n\n\nI will use the code below to combine the July and December data sets.\n\nJuly_data$Month &lt;- 'July'\nDecember_data$Month &lt;- 'December'\n\ncombined_data &lt;- rbind(July_data, December_data)\n\nI will use the datatable() function to inspect the combined data set.\n\nData TableData StructureMissing Values?\n\n\n\nDT::datatable(combined_data, class= \"compact\")\n\n\n\n\n\n\n\n\nstr(combined_data)\n\ntibble [310 × 11] (S3: tbl_df/tbl/data.frame)\n $ ID                     : chr [1:310] \"1\" \"1\" \"1\" \"1\" ...\n $ Station                : chr [1:310] \"Changi\" \"Changi\" \"Changi\" \"Changi\" ...\n $ Year                   : Factor w/ 5 levels \"1983\",\"1993\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Month                  : chr [1:310] \"July\" \"July\" \"July\" \"July\" ...\n $ Day                    : num [1:310] 1 2 3 4 5 6 7 8 9 10 ...\n $ Daily_Rainfall_Total_mm: num [1:310] 8.7 0 5.3 6.2 39.5 14.9 0 0 10.5 55.5 ...\n $ Mean_Temperature_C     : num [1:310] 27.5 28.7 27.9 28 25.4 27.1 26.2 28.2 27.8 25.4 ...\n $ Maximum_Temperature_C  : num [1:310] 33 32.6 32 31.9 27.4 31.1 28.1 31.9 31.1 27.2 ...\n $ Minimum_Temperature_C  : num [1:310] 24.4 25.5 24.4 25.7 21.4 24.1 22.6 25.6 26 23 ...\n $ Mean_Wind_Speed_km_h   : num [1:310] 3.7 10.5 5.8 7.6 3.6 8.4 5 11.8 9 5.3 ...\n $ Max_Wind_Speed_km_h    : num [1:310] 28.8 38.2 44.6 51.8 36 31.3 42.5 39.6 43.9 46.8 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   X1 = col_character(),\n  ..   X2 = col_double(),\n  ..   X3 = col_double(),\n  ..   X4 = col_double(),\n  ..   X5 = col_double(),\n  ..   X6 = col_skip(),\n  ..   X7 = col_skip(),\n  ..   X8 = col_skip(),\n  ..   X9 = col_double(),\n  ..   X10 = col_double(),\n  ..   X11 = col_double(),\n  ..   X12 = col_double(),\n  ..   X13 = col_double()\n  .. )\n\n\n\n\n\nsum(is.na(combined_data))\n\n[1] 0\n\n\nThere are no missing values in our data set."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#frequency-and-intensity-of-rainfall-events",
    "href": "R-ex/R-Ex4/R_Ex4.html#frequency-and-intensity-of-rainfall-events",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.1 Frequency and Intensity of Rainfall events",
    "text": "3.1 Frequency and Intensity of Rainfall events\nFirst, I will use the code below to create a static line plot to visualize July’s and December’s rainfall over the 5 years.\n\n\nShow the code\nggplot(combined_data, aes(x = Day, \n        y = Daily_Rainfall_Total_mm, group = Month)) +\n  \n  # Use geom_area for one of the months, July in this case\n  geom_area(data = subset(combined_data, Month == \"July\"), \n            aes(fill = Year), alpha = 0.3) +\n  \n  # Use geom_line for both months to ensure the line is on top of the fill\n  geom_line(aes(color = Year, \n                linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year (1983 - 2023)\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       fill = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  \n  guides(fill = guide_legend(\"Year\"), color = guide_legend(\"Year\"), \n         linetype = guide_legend(\"Month\"))\n\n\n\n\n\n\n\n\n\nNext, to enable readers to specifically examine the rainfall measures in more detail, I will use the code below to add interactivity.\n\n\nShow the code\np3 &lt;- ggplot(combined_data, aes(x = Day, y = Daily_Rainfall_Total_mm)) +\n  \n  # Use geom_line for both months, specifying linetype based on Month\n  geom_line(aes(color = Year, linetype = Month)) +\n  facet_wrap(~Year, ncol = 1) +\n  labs(title = \"Daily Rainfall in July and December by Year (1983-2023)\",\n       x = \"Day of the Month\",\n       y = \"Rainfall (mm)\",\n       color = \"Year\",\n       linetype = \"Month\") +\n  theme_minimal() +\n  \n  guides(fill = guide_legend(\"Year\"), color = guide_legend(\"Year\"), \n         linetype = guide_legend(\"Month\")) + scale_linetype_manual(values = c(\"July\" = \"dotted\", \"December\" = \"solid\"))\n\n\n\nggplotly(p3)\n\n\n\n\n\n\n\n\n\n\n\nObservation 1 - Frequency and Intensity of Rainfall events show no substantial increase\n\n\n\nThe infographics had reported that the Intensity and frequency of heavy rain fall events is expected to increase as the world gets warmer.\nFrom the visualisations above, in general, there are visibly more rain days in December relative to July over the years.\nHowever, there seems to be no substantial increase in the intensity of heavy rain events. For example, we can see that the highest recorded daily rainfall event occured in December 1983 at 164.4 mm. However, this level of intensity has not been surpassed in subsequent years.\n\nAdditionally, the volume of rainfall for both July and December 2023 is the least within the 5 years.\nThis further challenges the claim that the intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#one-way-anova-test-on-daily-rainfall-for-the-same-month-by-year",
    "href": "R-ex/R-Ex4/R_Ex4.html#one-way-anova-test-on-daily-rainfall-for-the-same-month-by-year",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.2 One-way Anova test on Daily Rainfall for the same month, by year",
    "text": "3.2 One-way Anova test on Daily Rainfall for the same month, by year\nThe infographics had reported that the Intensity and frequency of heavy rain fall events is expected to increase as the world gets warmer.\nOur visualisations in the previous section have showed otherwise.\nWe can validate this statistically and examine if there are any significant statistical differences between the daily rainfall for each July, and each December, across the five years.\nFirst, we will need to ascertain the nature of the distribution, and see if it is normal or non-normal distributed.\nWe can first visualise the distribution of daily rainfall using ridgeline plots, using the code below.\n\n\nShow the code\nggplot(July_data, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\"),\n    alpha = 0.6,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Daily Rainfall (mm)\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Distribution of the Daily Rainfall for July (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(December_data, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = Year)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\"),\n    alpha = 0.6,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Daily Rainfall (mm)\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Distribution of the Daily Rainfall for December (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the ridgeline plots above, we can see that the distribution for both July and December rainfall do not resemble a normal distribution, and that there is a right skewness.\n\n\nSince the distribution of rainfall is non-normal, I will conduct non-parametric tests.\nThe Hypothesis will be as such:\nH0: There is no difference between the median daily rainfall for the same month across the 5 years.\nH1: There is a difference between the median daily rainfall for the same month across the 5 years.\nI will use ggstatsbetween() from the ggstatplot package.\n\n\nShow the code\nggbetweenstats(\n  data = July_data,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  ggtitle(\"Daily Rainfall for July across the years (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggbetweenstats(\n  data = December_data,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  ggtitle(\"Daily Rainfall for December across the years (1983-2023)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation results\n\n\n\nIn both the tests above, the p-value is &gt; 0.05.\nHence, we have insufficient evidence to reject the Null Hypothesis and can conclude that there is no strong evidence to indicate that there is a difference in the Daily rainfall for the same month across the years.\nThis supports Observation 1, where we concluded that there seems to be no substantial increase in the frequency and intensity of heavy rain fall events."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#differences-between-july-and-december-across-the-years-dry-vs-wet-months",
    "href": "R-ex/R-Ex4/R_Ex4.html#differences-between-july-and-december-across-the-years-dry-vs-wet-months",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.3 Differences between July and December across the years (Dry Vs Wet Months)",
    "text": "3.3 Differences between July and December across the years (Dry Vs Wet Months)\nTo visualise the differences for the June-December periods, I will use the codes below to examine:\n\nThe differences in the number of Rain days, and\nThe differences in the Daily Rainfall, between the two months, across the years\n\n\n\nShow the code\nrain_fall_summary &lt;- combined_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarize(\n    MeanRainfall = mean(Daily_Rainfall_Total_mm, na.rm = TRUE),\n    RainyDays = sum(Daily_Rainfall_Total_mm &gt; 0, na.rm = TRUE), # Count days with rain\n    .groups = 'drop'\n  )\n\n\n\n\nShow the code\np_1 &lt;- ggplot(rain_fall_summary, aes(x = Year, y = RainyDays, group = Month, color = Month)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"The largest difference in the number of Rain Days between\\nJuly and December was in 2023\",\n       x = \"Year\",\n       y = \"Number of Rainy Days\",\n       color = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np_2 &lt;- ggplot(rain_fall_summary, aes(x = Year, y = MeanRainfall, group = Month, color = Month)) +\n  geom_line() +\n  geom_point() +\n  \n  labs(title = \"Both July's and December's mean Daily Rainfall have\\ntrended lower over the Years\",\n       x = \"Year\",\n       y = \"Mean Daily Rainfall (mm)\",\n       color = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_3 &lt;- ggplot(combined_data, aes(x = Year, y = Daily_Rainfall_Total_mm, fill = Month)) +\n  geom_boxplot() +\n  labs(title = \"The gap between July and December has reduced in 2023 \",\n       subtitle = \"The intensity of December's heavy rainfall events seem to be diminishing\",\n       x = \"Year\",\n       y = \"Daily Rainfall (mm)\",\n       fill = \"Month\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\nUsing Patchwork to combine the plots.\n\n\nShow the code\ncombined_plot &lt;- (p_1 + p_2) / p_3 + \n  plot_annotation(\n    title = \"The contrast between July and December has become more pronounced over the years\",\n    theme = theme(\n      plot.title = element_text(size = 20, face = \"bold\")\n    )\n  ) +\n  theme(plot.title.position = \"plot\")\n\ncombined_plot\n\n\n\n\n\n\n\n\n\nNext, to enable readers to specifically examine the differences in the rainfall between the two months in more detail, I will use the code below to add interactivity.\n\n\nShow the code\ncombined_mean &lt;- combined_data %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarise(MeanRainfall = round(mean(Daily_Rainfall_Total_mm, na.rm = TRUE), 2)) %&gt;%\n  ungroup()\n\n# Create the ggplot object\np_combined &lt;- ggplot(combined_data, aes(x = Year, y = Daily_Rainfall_Total_mm, color = Month)) +\n  geom_jitter(aes(text = paste('Day:', Day, 'Month:', Month)), width = 0.2, alpha = 0.5) + \n  geom_line(data = combined_mean, aes(x = Year, y = MeanRainfall, group = Month), \n            size = 0.5, linetype = \"dotted\") + \n  geom_point(data = combined_mean, aes(x = Year, y = MeanRainfall), \n             size = 3, show.legend = FALSE) + \n  scale_color_manual(values = c(\"July\" = \"blue\", \"December\" = \"red\")) +\n  labs(title = \"Daily Rainfall in July and December (1983-2023)\",\n       x = \"Year\",\n       y = \"Daily Rainfall Total (mm)\") +\n  theme_minimal() +\n  \n  \n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(title = \"Month\"))\n\n\n\n\nShow the code\np_plotly &lt;- ggplotly(p_combined) %&gt;%\n  layout(hovermode = 'closest') \n\np_plotly\n\n\n\n\n\n\n\n\n\n\n\n\nObservation 2 - Contrast between Dry and Wet Months are likely to be more pronounced\n\n\n\nThe infographics had reported that the contrast between Dry and Wet months is likely to be more pronounced.\nOur plots above show that:-\n\nThe difference in the number of rain days between July and December, in 2023, was at its highest, relative to previous years. The number of rain days in July has decreased, and the number of rain days in December has increased, over the years.\nThe mean daily rain fall for both July and December 2023 was at its lowest relative to other years.\nIn 2023, the gap of heavy rain fall events have been reduced. In previous years, there were more days with heavy rain fall in December relative to July.\n\nHence, this lends credence to the claim that the contrast between Dry and Wet months is likely to be more pronounced."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#two-sample-mean-test-july-vs-december-by-year",
    "href": "R-ex/R-Ex4/R_Ex4.html#two-sample-mean-test-july-vs-december-by-year",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "3.4 Two-sample mean test: July Vs December, by Year",
    "text": "3.4 Two-sample mean test: July Vs December, by Year\nThe infographics had reported that the contrast between wet months and dry months is likely to be more pronounced.\nOur visualisations in the previous section have corroborated this claim.\nWe can validate this statistically and examine if there is any evidence to suggest that there is indeed a difference in the daily rainfall between the two months (July and December).\nThe Hypothesis will be as such:\nH0: There is no difference in the median daily rainfall between July and December across the 5 years\nH1: There is a difference in the median daily rainfall between July and December across the 5 years\nSince we have rain fall data for 5 different years, I will use grouped_ggbetweenstats() from the ggstatplot package.\n\n\nShow the code\ngroup_plot &lt;- grouped_ggbetweenstats(\n  data = combined_data,\n  x = Month,\n  y = Daily_Rainfall_Total_mm,\n  grouping.var = Year,\n  type = \"np\", # for non-parametric\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\", \n  output = \"plot\" \n) \n\ngroup_plot + plot_annotation(\n    title = \"Differences between July and December over the years (1983-2023)\",\n    theme = theme(\n      plot.title = element_text(size = 20, face = \"bold\")\n    )\n  ) +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Results\n\n\n\nIn the tests above, the p-value is &gt; 0.05 for the years 1983,1993,2003 and 2013.\n\nHowever the p-value is &lt; 0.05 for year 2023.\nHence we can reject the Null Hypothesis and can conclude that there is some evidence to suggest that there is some difference in the daily rainfall between July to December, across the 5 years.\nThis supports Observation 2, where we concluded that the contrast between Dry and Wet months is likely to be more pronounced."
  },
  {
    "objectID": "R-ex/R-Ex4/R_Ex4.html#interactive-error-bars",
    "href": "R-ex/R-Ex4/R_Ex4.html#interactive-error-bars",
    "title": "Take-Home Exercise 2 - Be Weatherwise or Otherwise",
    "section": "4.1 Interactive Error Bars",
    "text": "4.1 Interactive Error Bars\nWe can also visualise the uncertainty of point estimates by plotting interactive error bars for the 99% confidence interval of mean Daily Rainfall in July and December by year as shown in the figures below.\n\n\nShow the code\nshared_df = SharedData$new(July_rain)\n\nbscols(widths = c(6,6),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(Year, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=Year, \n                     y=mean, \n                     text = paste(\"Year:\", `Year`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Rainfall:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Year\") + \n                   ylab(\"Average Daily Rainfall (mm)\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of&lt;br&gt;Avg Rainfall in July by Year\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 14,\n                                    scrollX=T), \n                     colnames = c(\"No. of observations\", \n                                  \"Avg Daily Rainfall\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nshared_df2 = SharedData$new(Dec_rain)\n\nbscols(widths = c(6,6),\n       ggplotly((ggplot(shared_df2) +\n                   geom_errorbar(aes(\n                     x=reorder(Year, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=Year, \n                     y=mean, \n                     text = paste(\"Year:\", `Year`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Rainfall:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Year\") + \n                   ylab(\"Average Daily Rainfall (mm)\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of&lt;br&gt;Avg Rainfall in Dec by Year\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df2, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 14,\n                                    scrollX=T), \n                     colnames = c(\"No. of observations\", \n                                  \"Avg Daily Rainfall\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations from Interactive Error bars\n\n\n\nJuly Trends:\n\n2023’s Average rainfall was the lowest, at 5.10 mm.\nThe increasing standard deviations (sd) from 2013 to 2023, indicates a trend of more variability in daily rainfall relative to the past.\n\nDecember Trends:\n\n2023’s Average rainfall was the lowest, at 8.26 mm.\nThe standard deviation shows a decrease over the 40-year span, indicating less variability in daily rainfall in December over time.\n\nJuly vs. December Relationship:\nThe differences in standard deviation (sd) between December and July for each respective year are as follows:\n\n1983: 18.92 mm\n1993: 4.24 mm\n2003: 10.12 mm\n2013: 15.59 mm\n2023: 0.82 mm\n\nThe difference in variability between December and July was much higher in 1983, but has significantly decreased by 2023.\nThe year 2023 stands out, with the variability in December being almost similar to that in July, indicating a convergence in the variability of rainfall between the two months in that year."
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html",
    "href": "R-ex/R-Ex6/R_Ex6.html",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this page, I will be exploring the codes for the plots in our Geospatial Analysis module of our Shiny Application. Specifically, I will be plotting for Local Measures of Spatial Autocorrelation\n\n\n\npacman::p_load(tidyverse, dplyr, tidyr, \n               sf, lubridate,plotly,\n               tmap, spdep, sfdep)\n\n\n\n\n\nACLED_MMR &lt;- read_csv(\"data/MMR.csv\")\n\nLoading in the Shape files for our Admin2 District boundaries.\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex6\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\nThis is to enable our admin1 region names and admin2 district names in our data set to be in sync with our shape files.\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\n\n\nFor our LISA analysis, we will set up our data set for 2021-2023, and in quarterly periods.\n\nACLED_MMR_2 &lt;- ACLED_MMR_1 %&gt;%\n  filter(year &gt;= 2021 & year &lt;= 2023) %&gt;%\n  mutate(event_date = dmy(event_date),\n         year_month = format(event_date, \"%Y-%m\"),\n         quarter = paste0(year, \"Q\", ceiling(month(event_date) / 3))) %&gt;%\n  filter(event_type != \"Strategic developments\")\n\n\nEvents2 &lt;- ACLED_MMR_2 %&gt;%\n  group_by(year, quarter, admin2, event_type) %&gt;%\n    summarise(Incidents = n(),\n              Fatalities = sum(fatalities, na.rm = TRUE)) %&gt;%\n              \n    ungroup()\n\n\n\nShow the code\nEvents_2 &lt;- read_csv(\"data/df_complete.csv\")\n\n\n\n#checking the total no of Incidents and Fatalities from 2021-2023\n\ntotal_incidents1 &lt;- sum(Events2$Incidents)\ntotal_incidents2 &lt;- sum(Events_2$Incidents)\ntotal_fatalities1 &lt;- sum(Events2$Fatalities)\ntotal_fatalities2 &lt;- sum(Events_2$Fatalities)\n\n\ntotal_incidents1 \n\n[1] 33955\n\ntotal_incidents2 \n\n[1] 33955\n\ntotal_fatalities1 \n\n[1] 46047\n\ntotal_fatalities2 \n\n[1] 46047\n\n\nCombining our attribute data to our map shape files.\n\nEvents_admin2 &lt;- left_join(mmr_shp_mimu_2, Events_2,\n                            by = c(\"DT\" = \"admin2\"))\n\n\nEvents_admin2 &lt;- Events_admin2 %&gt;%\n                      select(-OBJECTID, -ST, -ST_PCODE, \n                             -DT_PCODE, -DT_MMR, -PCode_V)\n\n\nclass(Events_admin2)\n\n[1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#loading-r-packages-and-data-prep",
    "href": "R-ex/R-Ex6/R_Ex6.html#loading-r-packages-and-data-prep",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "pacman::p_load(tidyverse, dplyr, tidyr, \n               sf, lubridate,plotly,\n               tmap, spdep, sfdep)"
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#loading-in-the-dataset",
    "href": "R-ex/R-Ex6/R_Ex6.html#loading-in-the-dataset",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "ACLED_MMR &lt;- read_csv(\"data/MMR.csv\")\n\nLoading in the Shape files for our Admin2 District boundaries.\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex6\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#renaming-admin1-and-admin2-names",
    "href": "R-ex/R-Ex6/R_Ex6.html#renaming-admin1-and-admin2-names",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "This is to enable our admin1 region names and admin2 district names in our data set to be in sync with our shape files.\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\n\n\nFor our LISA analysis, we will set up our data set for 2021-2023, and in quarterly periods.\n\nACLED_MMR_2 &lt;- ACLED_MMR_1 %&gt;%\n  filter(year &gt;= 2021 & year &lt;= 2023) %&gt;%\n  mutate(event_date = dmy(event_date),\n         year_month = format(event_date, \"%Y-%m\"),\n         quarter = paste0(year, \"Q\", ceiling(month(event_date) / 3))) %&gt;%\n  filter(event_type != \"Strategic developments\")\n\n\nEvents2 &lt;- ACLED_MMR_2 %&gt;%\n  group_by(year, quarter, admin2, event_type) %&gt;%\n    summarise(Incidents = n(),\n              Fatalities = sum(fatalities, na.rm = TRUE)) %&gt;%\n              \n    ungroup()\n\n\n\nShow the code\nEvents_2 &lt;- read_csv(\"data/df_complete.csv\")\n\n\n\n#checking the total no of Incidents and Fatalities from 2021-2023\n\ntotal_incidents1 &lt;- sum(Events2$Incidents)\ntotal_incidents2 &lt;- sum(Events_2$Incidents)\ntotal_fatalities1 &lt;- sum(Events2$Fatalities)\ntotal_fatalities2 &lt;- sum(Events_2$Fatalities)\n\n\ntotal_incidents1 \n\n[1] 33955\n\ntotal_incidents2 \n\n[1] 33955\n\ntotal_fatalities1 \n\n[1] 46047\n\ntotal_fatalities2 \n\n[1] 46047\n\n\nCombining our attribute data to our map shape files.\n\nEvents_admin2 &lt;- left_join(mmr_shp_mimu_2, Events_2,\n                            by = c(\"DT\" = \"admin2\"))\n\n\nEvents_admin2 &lt;- Events_admin2 %&gt;%\n                      select(-OBJECTID, -ST, -ST_PCODE, \n                             -DT_PCODE, -DT_MMR, -PCode_V)\n\n\nclass(Events_admin2)\n\n[1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#global-measures-of-spatial-association",
    "href": "R-ex/R-Ex6/R_Ex6.html#global-measures-of-spatial-association",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\nFirst we need to derive the contiguity weights."
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#deriving-contiguity-weights-queens-method",
    "href": "R-ex/R-Ex6/R_Ex6.html#deriving-contiguity-weights-queens-method",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "Deriving contiguity weights: Queen’s method",
    "text": "Deriving contiguity weights: Queen’s method\nIn the code chunk below, queen method is used to derive the contiguity weights.\n\nwm_q &lt;- Battles_data %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\n\nShow the code\n#wm_q_whole &lt;- Battles_data_whole %&gt;%\n#  mutate(nb = st_contiguity(geometry),\n#         wt = st_weights(nb,\n#                         style = \"W\"),\n#         .before = 1) \n\n\nst_weights() provides 3 arguments, they are:\n\nnb: A neighbor list object as created by st_neighbours().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”.\nB is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\nwm_q\n\nSimple feature collection with 80 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                         nb\n1       3, 5, 9, 10, 34, 78\n2                      4, 6\n3        1, 4, 5, 6, 78, 79\n4                2, 3, 5, 6\n5               1, 3, 4, 34\n6               2, 3, 4, 79\n7     8, 10, 22, 72, 78, 79\n8  7, 9, 10, 21, 22, 29, 73\n9          1, 8, 10, 29, 34\n10           1, 7, 8, 9, 78\n                                                                            wt\n1             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n2                                                                     0.5, 0.5\n3             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                       0.25, 0.25, 0.25, 0.25\n7             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9                                                      0.2, 0.2, 0.2, 0.2, 0.2\n10                                                     0.2, 0.2, 0.2, 0.2, 0.2\n           DT quarter event_type year Incidents Fatalities\n1    Hinthada  2023Q4    Battles 2023         2          2\n2     Labutta  2023Q4    Battles 2023         0          0\n3      Maubin  2023Q4    Battles 2023         0          0\n4   Myaungmya  2023Q4    Battles 2023         0          0\n5     Pathein  2023Q4    Battles 2023         0          0\n6      Pyapon  2023Q4    Battles 2023         0          0\n7        Bago  2023Q4    Battles 2023        13         50\n8     Taungoo  2023Q4    Battles 2023        27        223\n9        Pyay  2023Q4    Battles 2023        11         27\n10 Thayarwady  2023Q4    Battles 2023        19         25\n                         geometry\n1  MULTIPOLYGON (((95.12637 18...\n2  MULTIPOLYGON (((95.04462 15...\n3  MULTIPOLYGON (((95.38231 17...\n4  MULTIPOLYGON (((94.6942 16....\n5  MULTIPOLYGON (((94.27572 15...\n6  MULTIPOLYGON (((95.20798 15...\n7  MULTIPOLYGON (((95.90674 18...\n8  MULTIPOLYGON (((96.17964 19...\n9  MULTIPOLYGON (((95.70458 19...\n10 MULTIPOLYGON (((95.85173 18...\n\n\n\nComputing Global Moran’s I statistics\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\nmoranI &lt;- global_moran(wm_q$Incidents,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.237\n $ K: num 16.7\n\n\n\n\nPerforming Global Moran’s I test\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\nglobal_moran_test(wm_q$Incidents,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 3.713, p-value = 0.0001024\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.236932783      -0.012658228       0.004518538 \n\n\n\n\nPerforming Global Moran’s I permutation test\nIn practice, monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by global_moran_perm()\nIt is a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\nset.seed(1234)\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\nglobal_test &lt;- global_moran_perm(wm_q$Incidents,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 999)\n\nglobal_test\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.23693, observed rank = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe statistical report above show that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of Incidents for event type==Battle, resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.\n\n\nVisualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(global_test$res[1:999])\n\n[1] -0.01332017\n\n\n\nvar(global_test$res[1:999])\n\n[1] 0.004026595\n\n\n\nsummary(global_test$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.17783 -0.05633 -0.01746 -0.01332  0.02571  0.19633 \n\n\n\nhist(global_test$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#computing-local-morans-i",
    "href": "R-ex/R-Ex6/R_Ex6.html#computing-local-morans-i",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nNext, we compute Local Moran’s I of Incidents at admin 2 level (Districts) by using local_moran() of sfdep package.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    Incidents, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa\n\nSimple feature collection with 80 features and 20 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n# A tibble: 80 × 21\n        ii      eii  var_ii   z_ii  p_ii p_ii_sim p_folded_sim skewness kurtosis\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.264  -1.63e-2 0.0553   1.19  0.233     0.24         0.12   -1.43     4.26 \n 2  0.451   2.41e-2 0.221    0.908 0.364     0.08         0.06   -2.36     6.93 \n 3  0.432   4.45e-4 0.0908   1.43  0.152     0.02         0.01   -1.48     2.61 \n 4  0.451   1.53e-2 0.101    1.37  0.170     0.02         0.01   -1.48     2.60 \n 5  0.429  -1.87e-3 0.137    1.16  0.245     0.06         0.03   -1.62     2.40 \n 6  0.436  -1.11e-2 0.122    1.28  0.200     0.02         0.01   -1.46     2.17 \n 7  0.0242  2.74e-3 0.00260  0.420 0.674     0.88         0.44   -1.17     1.13 \n 8 -0.185  -2.51e-2 0.0244  -1.03  0.305     0.3          0.15    0.713    0.298\n 9  0.0509 -1.38e-3 0.00705  0.622 0.534     0.62         0.31   -1.13     1.86 \n10 -0.0307  9.01e-4 0.00567 -0.419 0.675     0.76         0.38    1.98     5.72 \n# ℹ 70 more rows\n# ℹ 12 more variables: mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;, nb &lt;nb&gt;,\n#   wt &lt;list&gt;, DT &lt;chr&gt;, quarter &lt;chr&gt;, event_type &lt;chr&gt;, year &lt;dbl&gt;,\n#   Incidents &lt;dbl&gt;, Fatalities &lt;dbl&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations\np_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviation based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\nVisualising local Moran’s I\nIn this code chunk below, tmap functions are used to prepare a choropleth map by using value in the ii field.\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Incidents\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\nVisualising p-value of local Moran’s I\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\nVisuaising local Moran’s I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other as shown below.\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Incidents\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nVisualising LISA map\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low clusters. LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "R-ex/R-Ex6/R_Ex6.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#computing-local-gi-statistics",
    "href": "R-ex/R-Ex6/R_Ex6.html#computing-local-gi-statistics",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "Computing local Gi* statistics",
    "text": "Computing local Gi* statistics\nSimilarly, we will need to first derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nwm_idw &lt;- Battles_data %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods are used to derive the spatial weight matrix.\nNext, we will compute the local Gi* by using the code below.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    Incidents, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 80 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n# A tibble: 80 × 19\n   gi_star cluster    e_gi    var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  -1.29  Low     0.0118  0.0000519  -1.11    0.266  0.18         0.09     1.18\n 2  -1.18  Low     0.00688 0.0000358  -1.15    0.250  0.08         0.01     1.07\n 3  -1.78  Low     0.0103  0.0000410  -1.54    0.124  0.02         0.01     1.76\n 4  -1.54  Low     0.0104  0.0000558  -1.40    0.163  0.02         0.01     1.74\n 5  -1.48  Low     0.0101  0.0000580  -1.27    0.206  0.02         0.01     2.32\n 6  -1.50  Low     0.00966 0.0000426  -1.43    0.152  0.04         0.01     1.19\n 7  -0.537 Low     0.0129  0.0000549  -0.547   0.585  0.68         0.34     1.14\n 8  -0.818 High    0.0138  0.0000428  -0.992   0.321  0.28         0.14     1.13\n 9  -0.618 Low     0.0110  0.0000464  -0.452   0.651  0.66         0.33     1.59\n10  -0.401 High    0.0122  0.0000402  -0.424   0.671  0.82         0.41     1.42\n# ℹ 70 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, DT &lt;chr&gt;,\n#   quarter &lt;chr&gt;, event_type &lt;chr&gt;, year &lt;dbl&gt;, Incidents &lt;dbl&gt;,\n#   Fatalities &lt;dbl&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\nVisualising Gi*\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\n\n\n\nVisualising p-value of HCSA\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nVisualising local HCSA\nFor effective comparison, we can plot both maps next to each other as shown below.\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Incidents\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "R-ex/R-Ex6/R_Ex6.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "Visualising hot spot and cold spot areas",
    "text": "Visualising hot spot and cold spot areas\nFinally, we plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure above reveals that there are several hot spot areas and these areas also coincide with the High-High cluster identified by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "R-ex/R-Ex6/R_Ex6.html#references",
    "href": "R-ex/R-Ex6/R_Ex6.html#references",
    "title": "Geospatial Analysis1 - Local Measures of Spatial Autocorrelation",
    "section": "References",
    "text": "References\nMain reference: Kam, T.S. (2024). Global and Local Measures of Spatial Autocorrelation - sfdep methods"
  },
  {
    "objectID": "R-ex/R-Ex8/R_Ex8.html",
    "href": "R-ex/R-Ex8/R_Ex8.html",
    "title": "Geospatial Analysis3 - Spatially Constrained Clustering: ClustGeo method",
    "section": "",
    "text": "pacman::p_load(spdep, sp, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\nACLED_MMR &lt;- read_csv(\"data/MMR.csv\")\n\n\nmmr_shp_mimu_2 &lt;-  st_read(dsn = \"data/geospatial3\",  \n                  layer = \"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\imranmi\\imran's data sc\\R-ex\\R-Ex8\\data\\geospatial3' using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nACLED_MMR_1 &lt;- ACLED_MMR %&gt;%\n  mutate(admin1 = case_when(\n    admin1 == \"Bago-East\" ~ \"Bago (East)\",\n    admin1 == \"Bago-West\" ~ \"Bago (West)\",\n    admin1 == \"Shan-North\" ~ \"Shan (North)\",\n    admin1 == \"Shan-South\" ~ \"Shan (South)\",\n    admin1 == \"Shan-East\" ~ \"Shan (East)\",\n    TRUE ~ as.character(admin1)\n  ))\n\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  mutate(admin2 = case_when(\n    admin2 == \"Yangon-East\" ~ \"Yangon (East)\",\n    admin2 == \"Yangon-West\" ~ \"Yangon (West)\",\n    admin2 == \"Yangon-North\" ~ \"Yangon (North)\",\n    admin2 == \"Yangon-South\" ~ \"Yangon (South)\",\n    admin2 == \"Mong Pawk (Wa SAD)\" ~ \"Tachileik\",\n    admin2 == \"Nay Pyi Taw\" ~ \"Det Khi Na\",\n    admin2 == \"Yangon\" ~ \"Yangon (West)\",\n    TRUE ~ as.character(admin2)\n  ))\n\n\nACLED_MMR_1 &lt;- ACLED_MMR_1 %&gt;%\n  filter(year &gt;= 2020 & year &lt;= 2023)\n\n\nData2 &lt;- ACLED_MMR_1 %&gt;%\n    group_by(year, admin2, event_type) %&gt;%\n    summarise(Incidents = n(),\n              Fatalities = sum(fatalities, na.rm = TRUE)) %&gt;%\n              \n    ungroup()\n\n\nlibrary(tidyr)\n\n\nyears &lt;- unique(Data2$year)\nevent_types &lt;- unique(Data2$event_type)\ndistricts &lt;- unique(Data2$admin2)\n\n# Using complete() to create all combinations of year, admin2, and event_type\n# and replacing NA values with 0 for Incidents and Fatalities\nData2_complete &lt;- Data2 %&gt;%\n  complete(year = years, admin2 = districts, event_type = event_types, fill = list(Incidents = 0, Fatalities = 0))\n\n\nlibrary(dplyr)\nlibrary(tidyr)\n\ndata2_summary &lt;- Data2_complete %&gt;%\n  group_by(admin2, event_type, year) %&gt;%\n  summarise(Incidents = sum(Incidents), Fatalities = sum(Fatalities), .groups = 'drop')  # Summarize and drop grouping\n\n# Now we'll spread this into a wider format\ndata2_long &lt;- data2_summary %&gt;%\n  pivot_wider(\n    names_from = c(event_type, year), \n    values_from = c(Incidents, Fatalities),\n    names_glue = \"{event_type}_{year}_{.value}\"  \n  )\n\n\n\n\nselected &lt;- data2_long %&gt;%\n  select(admin2, Battles_2022_Incidents)\n\n\nselected\n\n# A tibble: 80 × 2\n   admin2                      Battles_2022_Incidents\n   &lt;chr&gt;                                        &lt;int&gt;\n 1 Bago                                            12\n 2 Bawlake                                         16\n 3 Bhamo                                           54\n 4 Danu Self-Administered Zone                     15\n 5 Dawei                                          141\n 6 Det Khi Na                                       0\n 7 Falam                                           59\n 8 Gangaw                                         143\n 9 Hakha                                           43\n10 Hinthada                                         6\n# ℹ 70 more rows\n\n\n\n\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(selected, method = 'euclidean')\n\n\nproxmat\n\n            1          2          3          4          5          6          7\n2    5.656854                                                                  \n3   59.396970  53.740115                                                       \n4    4.242641   1.414214  55.154329                                            \n5  182.433550 176.776695 123.036580 178.190909                                 \n6   16.970563  22.627417  76.367532  21.213203 199.404112                      \n7   66.468037  60.811183   7.071068  62.225397 115.965512  83.438600           \n8  185.261977 179.605122 125.865007 181.019336   2.828427 202.232539 118.793939\n9   43.840620  38.183766  15.556349  39.597980 138.592929  60.811183  22.627417\n10   8.485281  14.142136  67.882251  12.727922 190.918831   8.485281  74.953319\n11   2.828427   8.485281  62.225397   7.071068 185.261977  14.142136  69.296465\n12  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n13  21.213203  15.556349  38.183766  16.970563 161.220346  38.183766  45.254834\n14  38.183766  32.526912  21.213203  33.941125 144.249783  55.154329  28.284271\n15 213.546248 207.889394 154.149278 209.303607  31.112698 230.516811 147.078210\n16  31.112698  25.455844  28.284271  26.870058 151.320851  48.083261  35.355339\n17 206.475180 200.818326 147.078210 202.232539  24.041631 223.445743 140.007143\n18 108.894444 103.237590  49.497475 104.651804  73.539105 125.865007  42.426407\n19  43.840620  38.183766  15.556349  39.597980 138.592929  60.811183  22.627417\n20   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n21  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n22   8.485281  14.142136  67.882251  12.727922 190.918831   8.485281  74.953319\n23  38.183766  32.526912  21.213203  33.941125 144.249783  55.154329  28.284271\n24   7.071068  12.727922  66.468037  11.313708 189.504617   9.899495  73.539105\n25   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n26  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n27  12.727922  18.384776  72.124892  16.970563 195.161472   4.242641  79.195959\n28  11.313708   5.656854  48.083261   7.071068 171.119841  28.284271  55.154329\n29 359.210245 353.553391 299.813275 354.967604 176.776695 376.180808 292.742207\n30   5.656854  11.313708  65.053824   9.899495 188.090404  11.313708  72.124892\n31   5.656854  11.313708  65.053824   9.899495 188.090404  11.313708  72.124892\n32  46.669048  41.012193  12.727922  42.426407 135.764502  63.639610  19.798990\n33  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n34  97.580736  91.923882  38.183766  93.338095  84.852814 114.551299  31.112698\n35  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n36  67.882251  62.225397   8.485281  63.639610 114.551299  84.852814   1.414214\n37  14.142136   8.485281  45.254834   9.899495 168.291414  31.112698  52.325902\n38 108.894444 103.237590  49.497475 104.651804  73.539105 125.865007  42.426407\n39  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n40   0.000000   5.656854  59.396970   4.242641 182.433550  16.970563  66.468037\n41  77.781746  72.124892  18.384776  73.539105 104.651804  94.752309  11.313708\n42  70.710678  65.053824  11.313708  66.468037 111.722871  87.681241   4.242641\n43  15.556349  21.213203  74.953319  19.798990 197.989899   1.414214  82.024387\n44   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n45 195.161472 189.504617 135.764502 190.918831  12.727922 212.132034 128.693434\n46   8.485281   2.828427  50.911688   4.242641 173.948268  25.455844  57.982756\n47 142.835570 137.178716  83.438600 138.592929  39.597980 159.806133  76.367532\n48  15.556349  21.213203  74.953319  19.798990 197.989899   1.414214  82.024387\n49 135.764502 130.107648  76.367532 131.521861  46.669048 152.735065  69.296465\n50 179.605122 173.948268 120.208153 175.362482   2.828427 196.575685 113.137085\n51  52.325902  46.669048   7.071068  48.083261 130.107648  69.296465  14.142136\n52  41.012193  35.355339  18.384776  36.769553 141.421356  57.982756  25.455844\n53  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n54  11.313708  16.970563  70.710678  15.556349 193.747258   5.656854  77.781746\n55  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n56   7.071068   1.414214  52.325902   2.828427 175.362482  24.041631  59.396970\n57   9.899495  15.556349  69.296465  14.142136 192.333044   7.071068  76.367532\n58 299.813275 294.156421 240.416306 295.570635 117.379726 316.783838 233.345238\n59  11.313708  16.970563  70.710678  15.556349 193.747258   5.656854  77.781746\n60  25.455844  19.798990  33.941125  21.213203 156.977705  42.426407  41.012193\n61  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n62   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n63  48.083261  42.426407  11.313708  43.840620 134.350288  65.053824  18.384776\n64 107.480231 101.823376  48.083261 103.237590  74.953319 124.450793  41.012193\n65 275.771645 270.114790 216.374675 271.529004  93.338095 292.742207 209.303607\n66   4.242641   1.414214  55.154329   0.000000 178.190909  21.213203  62.225397\n67  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n68  42.426407  36.769553  16.970563  38.183766 140.007143  59.396970  24.041631\n69  90.509668  84.852814  31.112698  86.267027  91.923882 107.480231  24.041631\n70  52.325902  46.669048   7.071068  48.083261 130.107648  69.296465  14.142136\n71  12.727922  18.384776  72.124892  16.970563 195.161472   4.242641  79.195959\n72 131.521861 125.865007  72.124892 127.279221  50.911688 148.492424  65.053824\n73  12.727922   7.071068  46.669048   8.485281 169.705627  29.698485  53.740115\n74   5.656854  11.313708  65.053824   9.899495 188.090404  11.313708  72.124892\n75  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n76   5.656854   0.000000  53.740115   1.414214 176.776695  22.627417  60.811183\n77   7.071068  12.727922  66.468037  11.313708 189.504617   9.899495  73.539105\n78   2.828427   2.828427  56.568542   1.414214 179.605122  19.798990  63.639610\n79   2.828427   8.485281  62.225397   7.071068 185.261977  14.142136  69.296465\n80 282.842712 277.185858 223.445743 278.600072 100.409163 299.813275 216.374675\n            8          9         10         11         12         13         14\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9  141.421356                                                                  \n10 193.747258  52.325902                                                       \n11 188.090404  46.669048   5.656854                                            \n12 202.232539  60.811183   8.485281  14.142136                                 \n13 164.048773  22.627417  29.698485  24.041631  38.183766                      \n14 147.078210   5.656854  46.669048  41.012193  55.154329  16.970563           \n15  28.284271 169.705627 222.031529 216.374675 230.516811 192.333044 175.362482\n16 154.149278  12.727922  39.597980  33.941125  48.083261   9.899495   7.071068\n17  21.213203 162.634560 214.960461 209.303607 223.445743 185.261977 168.291414\n18  76.367532  65.053824 117.379726 111.722871 125.865007  87.681241  70.710678\n19 141.421356   0.000000  52.325902  46.669048  60.811183  22.627417   5.656854\n20 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n21 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n22 193.747258  52.325902   0.000000   5.656854   8.485281  29.698485  46.669048\n23 147.078210   5.656854  46.669048  41.012193  55.154329  16.970563   0.000000\n24 192.333044  50.911688   1.414214   4.242641   9.899495  28.284271  45.254834\n25 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n26 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n27 197.989899  56.568542   4.242641   9.899495   4.242641  33.941125  50.911688\n28 173.948268  32.526912  19.798990  14.142136  28.284271   9.899495  26.870058\n29 173.948268 315.369624 367.695526 362.038672 376.180808 337.997041 321.026479\n30 190.918831  49.497475   2.828427   2.828427  11.313708  26.870058  43.840620\n31 190.918831  49.497475   2.828427   2.828427  11.313708  26.870058  43.840620\n32 138.592929   2.828427  55.154329  49.497475  63.639610  25.455844   8.485281\n33 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n34  87.681241  53.740115 106.066017 100.409163 114.551299  76.367532  59.396970\n35 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n36 117.379726  24.041631  76.367532  70.710678  84.852814  46.669048  29.698485\n37 171.119841  29.698485  22.627417  16.970563  31.112698   7.071068  24.041631\n38  76.367532  65.053824 117.379726 111.722871 125.865007  87.681241  70.710678\n39 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n40 185.261977  43.840620   8.485281   2.828427  16.970563  21.213203  38.183766\n41 107.480231  33.941125  86.267027  80.610173  94.752309  56.568542  39.597980\n42 114.551299  26.870058  79.195959  73.539105  87.681241  49.497475  32.526912\n43 200.818326  59.396970   7.071068  12.727922   1.414214  36.769553  53.740115\n44 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n45   9.899495 151.320851 203.646753 197.989899 212.132034 173.948268 156.977705\n46 176.776695  35.355339  16.970563  11.313708  25.455844  12.727922  29.698485\n47  42.426407  98.994949 151.320851 145.663997 159.806133 121.622366 104.651804\n48 200.818326  59.396970   7.071068  12.727922   1.414214  36.769553  53.740115\n49  49.497475  91.923882 144.249783 138.592929 152.735065 114.551299  97.580736\n50   5.656854 135.764502 188.090404 182.433550 196.575685 158.391919 141.421356\n51 132.936075   8.485281  60.811183  55.154329  69.296465  31.112698  14.142136\n52 144.249783   2.828427  49.497475  43.840620  57.982756  19.798990   2.828427\n53 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n54 196.575685  55.154329   2.828427   8.485281   5.656854  32.526912  49.497475\n55 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n56 178.190909  36.769553  15.556349   9.899495  24.041631  14.142136  31.112698\n57 195.161472  53.740115   1.414214   7.071068   7.071068  31.112698  48.083261\n58 114.551299 255.972655 308.298557 302.641702 316.783838 278.600072 261.629509\n59 196.575685  55.154329   2.828427   8.485281   5.656854  32.526912  49.497475\n60 159.806133  18.384776  33.941125  28.284271  42.426407   4.242641  12.727922\n61 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n62 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n63 137.178716   4.242641  56.568542  50.911688  65.053824  26.870058   9.899495\n64  77.781746  63.639610 115.965512 110.308658 124.450793  86.267027  69.296465\n65  90.509668 231.931024 284.256926 278.600072 292.742207 254.558441 237.587878\n66 181.019336  39.597980  12.727922   7.071068  21.213203  16.970563  33.941125\n67 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n68 142.835570   1.414214  50.911688  45.254834  59.396970  21.213203   4.242641\n69  94.752309  46.669048  98.994949  93.338095 107.480231  69.296465  52.325902\n70 132.936075   8.485281  60.811183  55.154329  69.296465  31.112698  14.142136\n71 197.989899  56.568542   4.242641   9.899495   4.242641  33.941125  50.911688\n72  53.740115  87.681241 140.007143 134.350288 148.492424 110.308658  93.338095\n73 172.534055  31.112698  21.213203  15.556349  29.698485   8.485281  25.455844\n74 190.918831  49.497475   2.828427   2.828427  11.313708  26.870058  43.840620\n75 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n76 179.605122  38.183766  14.142136   8.485281  22.627417  15.556349  32.526912\n77 192.333044  50.911688   1.414214   4.242641   9.899495  28.284271  45.254834\n78 182.433550  41.012193  11.313708   5.656854  19.798990  18.384776  35.355339\n79 188.090404  46.669048   5.656854   0.000000  14.142136  24.041631  41.012193\n80  97.580736 239.002092 291.327994 285.671140 299.813275 261.629509 244.658946\n           15         16         17         18         19         20         21\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16 182.433550                                                                  \n17   7.071068 175.362482                                                       \n18 104.651804  77.781746  97.580736                                            \n19 169.705627  12.727922 162.634560  65.053824                                 \n20 212.132034  29.698485 205.060967 107.480231  42.426407                      \n21 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776           \n22 222.031529  39.597980 214.960461 117.379726  52.325902   9.899495   8.485281\n23 175.362482   7.071068 168.291414  70.710678   5.656854  36.769553  55.154329\n24 220.617316  38.183766 213.546248 115.965512  50.911688   8.485281   9.899495\n25 212.132034  29.698485 205.060967 107.480231  42.426407   0.000000  18.384776\n26 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n27 226.274170  43.840620 219.203102 121.622366  56.568542  14.142136   4.242641\n28 202.232539  19.798990 195.161472  97.580736  32.526912   9.899495  28.284271\n29 145.663997 328.097546 152.735065 250.315801 315.369624 357.796031 376.180808\n30 219.203102  36.769553 212.132034 114.551299  49.497475   7.071068  11.313708\n31 219.203102  36.769553 212.132034 114.551299  49.497475   7.071068  11.313708\n32 166.877200  15.556349 159.806133  62.225397   2.828427  45.254834  63.639610\n33 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n34 115.965512  66.468037 108.894444  11.313708  53.740115  96.166522 114.551299\n35 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n36 145.663997  36.769553 138.592929  41.012193  24.041631  66.468037  84.852814\n37 199.404112  16.970563 192.333044  94.752309  29.698485  12.727922  31.112698\n38 104.651804  77.781746  97.580736   0.000000  65.053824 107.480231 125.865007\n39 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n40 213.546248  31.112698 206.475180 108.894444  43.840620   1.414214  16.970563\n41 135.764502  46.669048 128.693434  31.112698  33.941125  76.367532  94.752309\n42 142.835570  39.597980 135.764502  38.183766  26.870058  69.296465  87.681241\n43 229.102597  46.669048 222.031529 124.450793  59.396970  16.970563   1.414214\n44 212.132034  29.698485 205.060967 107.480231  42.426407   0.000000  18.384776\n45  18.384776 164.048773  11.313708  86.267027 151.320851 193.747258 212.132034\n46 205.060967  22.627417 197.989899 100.409163  35.355339   7.071068  25.455844\n47  70.710678 111.722871  63.639610  33.941125  98.994949 141.421356 159.806133\n48 229.102597  46.669048 222.031529 124.450793  59.396970  16.970563   1.414214\n49  77.781746 104.651804  70.710678  26.870058  91.923882 134.350288 152.735065\n50  33.941125 148.492424  26.870058  70.710678 135.764502 178.190909 196.575685\n51 161.220346  21.213203 154.149278  56.568542   8.485281  50.911688  69.296465\n52 172.534055   9.899495 165.462987  67.882251   2.828427  39.597980  57.982756\n53 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n54 224.859956  42.426407 217.788889 120.208153  55.154329  12.727922   5.656854\n55 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n56 206.475180  24.041631 199.404112 101.823376  36.769553   5.656854  24.041631\n57 223.445743  41.012193 216.374675 118.793939  53.740115  11.313708   7.071068\n58  86.267027 268.700577  93.338095 190.918831 255.972655 298.399062 316.783838\n59 224.859956  42.426407 217.788889 120.208153  55.154329  12.727922   5.656854\n60 188.090404   5.656854 181.019336  83.438600  18.384776  24.041631  42.426407\n61 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n62 212.132034  29.698485 205.060967 107.480231  42.426407   0.000000  18.384776\n63 165.462987  16.970563 158.391919  60.811183   4.242641  46.669048  65.053824\n64 106.066017  76.367532  98.994949   1.414214  63.639610 106.066017 124.450793\n65  62.225397 244.658946  69.296465 166.877200 231.931024 274.357431 292.742207\n66 209.303607  26.870058 202.232539 104.651804  39.597980   2.828427  21.213203\n67 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n68 171.119841  11.313708 164.048773  66.468037   1.414214  41.012193  59.396970\n69 123.036580  59.396970 115.965512  18.384776  46.669048  89.095454 107.480231\n70 161.220346  21.213203 154.149278  56.568542   8.485281  50.911688  69.296465\n71 226.274170  43.840620 219.203102 121.622366  56.568542  14.142136   4.242641\n72  82.024387 100.409163  74.953319  22.627417  87.681241 130.107648 148.492424\n73 200.818326  18.384776 193.747258  96.166522  31.112698  11.313708  29.698485\n74 219.203102  36.769553 212.132034 114.551299  49.497475   7.071068  11.313708\n75 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n76 207.889394  25.455844 200.818326 103.237590  38.183766   4.242641  22.627417\n77 220.617316  38.183766 213.546248 115.965512  50.911688   8.485281   9.899495\n78 210.717821  28.284271 203.646753 106.066017  41.012193   1.414214  19.798990\n79 216.374675  33.941125 209.303607 111.722871  46.669048   4.242641  14.142136\n80  69.296465 251.730014  76.367532 173.948268 239.002092 281.428499 299.813275\n           22         23         24         25         26         27         28\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23  46.669048                                                                  \n24   1.414214  45.254834                                                       \n25   9.899495  36.769553   8.485281                                            \n26   5.656854  52.325902   7.071068  15.556349                                 \n27   4.242641  50.911688   5.656854  14.142136   1.414214                      \n28  19.798990  26.870058  18.384776   9.899495  25.455844  24.041631           \n29 367.695526 321.026479 366.281313 357.796031 373.352380 371.938167 347.896536\n30   2.828427  43.840620   1.414214   7.071068   8.485281   7.071068  16.970563\n31   2.828427  43.840620   1.414214   7.071068   8.485281   7.071068  16.970563\n32  55.154329   8.485281  53.740115  45.254834  60.811183  59.396970  35.355339\n33   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n34 106.066017  59.396970 104.651804  96.166522 111.722871 110.308658  86.267027\n35   5.656854  52.325902   7.071068  15.556349   0.000000   1.414214  25.455844\n36  76.367532  29.698485  74.953319  66.468037  82.024387  80.610173  56.568542\n37  22.627417  24.041631  21.213203  12.727922  28.284271  26.870058   2.828427\n38 117.379726  70.710678 115.965512 107.480231 123.036580 121.622366  97.580736\n39   5.656854  52.325902   7.071068  15.556349   0.000000   1.414214  25.455844\n40   8.485281  38.183766   7.071068   1.414214  14.142136  12.727922  11.313708\n41  86.267027  39.597980  84.852814  76.367532  91.923882  90.509668  66.468037\n42  79.195959  32.526912  77.781746  69.296465  84.852814  83.438600  59.396970\n43   7.071068  53.740115   8.485281  16.970563   1.414214   2.828427  26.870058\n44   9.899495  36.769553   8.485281   0.000000  15.556349  14.142136   9.899495\n45 203.646753 156.977705 202.232539 193.747258 209.303607 207.889394 183.847763\n46  16.970563  29.698485  15.556349   7.071068  22.627417  21.213203   2.828427\n47 151.320851 104.651804 149.906638 141.421356 156.977705 155.563492 131.521861\n48   7.071068  53.740115   8.485281  16.970563   1.414214   2.828427  26.870058\n49 144.249783  97.580736 142.835570 134.350288 149.906638 148.492424 124.450793\n50 188.090404 141.421356 186.676190 178.190909 193.747258 192.333044 168.291414\n51  60.811183  14.142136  59.396970  50.911688  66.468037  65.053824  41.012193\n52  49.497475   2.828427  48.083261  39.597980  55.154329  53.740115  29.698485\n53   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n54   2.828427  49.497475   4.242641  12.727922   2.828427   1.414214  22.627417\n55   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n56  15.556349  31.112698  14.142136   5.656854  21.213203  19.798990   4.242641\n57   1.414214  48.083261   2.828427  11.313708   4.242641   2.828427  21.213203\n58 308.298557 261.629509 306.884343 298.399062 313.955411 312.541197 288.499567\n59   2.828427  49.497475   4.242641  12.727922   2.828427   1.414214  22.627417\n60  33.941125  12.727922  32.526912  24.041631  39.597980  38.183766  14.142136\n61   5.656854  52.325902   7.071068  15.556349   0.000000   1.414214  25.455844\n62   9.899495  36.769553   8.485281   0.000000  15.556349  14.142136   9.899495\n63  56.568542   9.899495  55.154329  46.669048  62.225397  60.811183  36.769553\n64 115.965512  69.296465 114.551299 106.066017 121.622366 120.208153  96.166522\n65 284.256926 237.587878 282.842712 274.357431 289.913780 288.499567 264.457936\n66  12.727922  33.941125  11.313708   2.828427  18.384776  16.970563   7.071068\n67   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n68  50.911688   4.242641  49.497475  41.012193  56.568542  55.154329  31.112698\n69  98.994949  52.325902  97.580736  89.095454 104.651804 103.237590  79.195959\n70  60.811183  14.142136  59.396970  50.911688  66.468037  65.053824  41.012193\n71   4.242641  50.911688   5.656854  14.142136   1.414214   0.000000  24.041631\n72 140.007143  93.338095 138.592929 130.107648 145.663997 144.249783 120.208153\n73  21.213203  25.455844  19.798990  11.313708  26.870058  25.455844   1.414214\n74   2.828427  43.840620   1.414214   7.071068   8.485281   7.071068  16.970563\n75   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n76  14.142136  32.526912  12.727922   4.242641  19.798990  18.384776   5.656854\n77   1.414214  45.254834   0.000000   8.485281   7.071068   5.656854  18.384776\n78  11.313708  35.355339   9.899495   1.414214  16.970563  15.556349   8.485281\n79   5.656854  41.012193   4.242641   4.242641  11.313708   9.899495  14.142136\n80 291.327994 244.658946 289.913780 281.428499 296.984848 295.570635 271.529004\n           29         30         31         32         33         34         35\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30 364.867099                                                                  \n31 364.867099   0.000000                                                       \n32 312.541197  52.325902  52.325902                                            \n33 376.180808  11.313708  11.313708  63.639610                                 \n34 261.629509 103.237590 103.237590  50.911688 114.551299                      \n35 373.352380   8.485281   8.485281  60.811183   2.828427 111.722871           \n36 291.327994  73.539105  73.539105  21.213203  84.852814  29.698485  82.024387\n37 345.068109  19.798990  19.798990  32.526912  31.112698  83.438600  28.284271\n38 250.315801 114.551299 114.551299  62.225397 125.865007  11.313708 123.036580\n39 373.352380   8.485281   8.485281  60.811183   2.828427 111.722871   0.000000\n40 359.210245   5.656854   5.656854  46.669048  16.970563  97.580736  14.142136\n41 281.428499  83.438600  83.438600  31.112698  94.752309  19.798990  91.923882\n42 288.499567  76.367532  76.367532  24.041631  87.681241  26.870058  84.852814\n43 374.766594   9.899495   9.899495  62.225397   1.414214 113.137085   1.414214\n44 357.796031   7.071068   7.071068  45.254834  18.384776  96.166522  15.556349\n45 164.048773 200.818326 200.818326 148.492424 212.132034  97.580736 209.303607\n46 350.724963  14.142136  14.142136  38.183766  25.455844  89.095454  22.627417\n47 216.374675 148.492424 148.492424  96.166522 159.806133  45.254834 156.977705\n48 374.766594   9.899495   9.899495  62.225397   1.414214 113.137085   1.414214\n49 223.445743 141.421356 141.421356  89.095454 152.735065  38.183766 149.906638\n50 179.605122 185.261977 185.261977 132.936075 196.575685  82.024387 193.747258\n51 306.884343  57.982756  57.982756   5.656854  69.296465  45.254834  66.468037\n52 318.198052  46.669048  46.669048   5.656854  57.982756  56.568542  55.154329\n53 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n54 370.523953   5.656854   5.656854  57.982756   5.656854 108.894444   2.828427\n55 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n56 352.139177  12.727922  12.727922  39.597980  24.041631  90.509668  21.213203\n57 369.109740   4.242641   4.242641  56.568542   7.071068 107.480231   4.242641\n58  59.396970 305.470129 305.470129 253.144228 316.783838 202.232539 313.955411\n59 370.523953   5.656854   5.656854  57.982756   5.656854 108.894444   2.828427\n60 333.754401  31.112698  31.112698  21.213203  42.426407  72.124892  39.597980\n61 373.352380   8.485281   8.485281  60.811183   2.828427 111.722871   0.000000\n62 357.796031   7.071068   7.071068  45.254834  18.384776  96.166522  15.556349\n63 311.126984  53.740115  53.740115   1.414214  65.053824  49.497475  62.225397\n64 251.730014 113.137085 113.137085  60.811183 124.450793   9.899495 121.622366\n65  83.438600 281.428499 281.428499 229.102597 292.742207 178.190909 289.913780\n66 354.967604   9.899495   9.899495  42.426407  21.213203  93.338095  18.384776\n67 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n68 316.783838  48.083261  48.083261   4.242641  59.396970  55.154329  56.568542\n69 268.700577  96.166522  96.166522  43.840620 107.480231   7.071068 104.651804\n70 306.884343  57.982756  57.982756   5.656854  69.296465  45.254834  66.468037\n71 371.938167   7.071068   7.071068  59.396970   4.242641 110.308658   1.414214\n72 227.688384 137.178716 137.178716  84.852814 148.492424  33.941125 145.663997\n73 346.482323  18.384776  18.384776  33.941125  29.698485  84.852814  26.870058\n74 364.867099   0.000000   0.000000  52.325902  11.313708 103.237590   8.485281\n75 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n76 353.553391  11.313708  11.313708  41.012193  22.627417  91.923882  19.798990\n77 366.281313   1.414214   1.414214  53.740115   9.899495 104.651804   7.071068\n78 356.381818   8.485281   8.485281  43.840620  19.798990  94.752309  16.970563\n79 362.038672   2.828427   2.828427  49.497475  14.142136 100.409163  11.313708\n80  76.367532 288.499567 288.499567 236.173665 299.813275 185.261977 296.984848\n           36         37         38         39         40         41         42\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37  53.740115                                                                  \n38  41.012193  94.752309                                                       \n39  82.024387  28.284271 123.036580                                            \n40  67.882251  14.142136 108.894444  14.142136                                 \n41   9.899495  63.639610  31.112698  91.923882  77.781746                      \n42   2.828427  56.568542  38.183766  84.852814  70.710678   7.071068           \n43  83.438600  29.698485 124.450793   1.414214  15.556349  93.338095  86.267027\n44  66.468037  12.727922 107.480231  15.556349   1.414214  76.367532  69.296465\n45 127.279221 181.019336  86.267027 209.303607 195.161472 117.379726 124.450793\n46  59.396970   5.656854 100.409163  22.627417   8.485281  69.296465  62.225397\n47  74.953319 128.693434  33.941125 156.977705 142.835570  65.053824  72.124892\n48  83.438600  29.698485 124.450793   1.414214  15.556349  93.338095  86.267027\n49  67.882251 121.622366  26.870058 149.906638 135.764502  57.982756  65.053824\n50 111.722871 165.462987  70.710678 193.747258 179.605122 101.823376 108.894444\n51  15.556349  38.183766  56.568542  66.468037  52.325902  25.455844  18.384776\n52  26.870058  26.870058  67.882251  55.154329  41.012193  36.769553  29.698485\n53  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n54  79.195959  25.455844 120.208153   2.828427  11.313708  89.095454  82.024387\n55  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n56  60.811183   7.071068 101.823376  21.213203   7.071068  70.710678  63.639610\n57  77.781746  24.041631 118.793939   4.242641   9.899495  87.681241  80.610173\n58 231.931024 285.671140 190.918831 313.955411 299.813275 222.031529 229.102597\n59  79.195959  25.455844 120.208153   2.828427  11.313708  89.095454  82.024387\n60  42.426407  11.313708  83.438600  39.597980  25.455844  52.325902  45.254834\n61  82.024387  28.284271 123.036580   0.000000  14.142136  91.923882  84.852814\n62  66.468037  12.727922 107.480231  15.556349   1.414214  76.367532  69.296465\n63  19.798990  33.941125  60.811183  62.225397  48.083261  29.698485  22.627417\n64  39.597980  93.338095   1.414214 121.622366 107.480231  29.698485  36.769553\n65 207.889394 261.629509 166.877200 289.913780 275.771645 197.989899 205.060967\n66  63.639610   9.899495 104.651804  18.384776   4.242641  73.539105  66.468037\n67  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n68  25.455844  28.284271  66.468037  56.568542  42.426407  35.355339  28.284271\n69  22.627417  76.367532  18.384776 104.651804  90.509668  12.727922  19.798990\n70  15.556349  38.183766  56.568542  66.468037  52.325902  25.455844  18.384776\n71  80.610173  26.870058 121.622366   1.414214  12.727922  90.509668  83.438600\n72  63.639610 117.379726  22.627417 145.663997 131.521861  53.740115  60.811183\n73  55.154329   1.414214  96.166522  26.870058  12.727922  65.053824  57.982756\n74  73.539105  19.798990 114.551299   8.485281   5.656854  83.438600  76.367532\n75  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n76  62.225397   8.485281 103.237590  19.798990   5.656854  72.124892  65.053824\n77  74.953319  21.213203 115.965512   7.071068   7.071068  84.852814  77.781746\n78  65.053824  11.313708 106.066017  16.970563   2.828427  74.953319  67.882251\n79  70.710678  16.970563 111.722871  11.313708   2.828427  80.610173  73.539105\n80 214.960461 268.700577 173.948268 296.984848 282.842712 205.060967 212.132034\n           43         44         45         46         47         48         49\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44  16.970563                                                                  \n45 210.717821 193.747258                                                       \n46  24.041631   7.071068 186.676190                                            \n47 158.391919 141.421356  52.325902 134.350288                                 \n48   0.000000  16.970563 210.717821  24.041631 158.391919                      \n49 151.320851 134.350288  59.396970 127.279221   7.071068 151.320851           \n50 195.161472 178.190909  15.556349 171.119841  36.769553 195.161472  43.840620\n51  67.882251  50.911688 142.835570  43.840620  90.509668  67.882251  83.438600\n52  56.568542  39.597980 154.149278  32.526912 101.823376  56.568542  94.752309\n53   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n54   4.242641  12.727922 206.475180  19.798990 154.149278   4.242641 147.078210\n55   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n56  22.627417   5.656854 188.090404   1.414214 135.764502  22.627417 128.693434\n57   5.656854  11.313708 205.060967  18.384776 152.735065   5.656854 145.663997\n58 315.369624 298.399062 104.651804 291.327994 156.977705 315.369624 164.048773\n59   4.242641  12.727922 206.475180  19.798990 154.149278   4.242641 147.078210\n60  41.012193  24.041631 169.705627  16.970563 117.379726  41.012193 110.308658\n61   1.414214  15.556349 209.303607  22.627417 156.977705   1.414214 149.906638\n62  16.970563   0.000000 193.747258   7.071068 141.421356  16.970563 134.350288\n63  63.639610  46.669048 147.078210  39.597980  94.752309  63.639610  87.681241\n64 123.036580 106.066017  87.681241  98.994949  35.355339 123.036580  28.284271\n65 291.327994 274.357431  80.610173 267.286363 132.936075 291.327994 140.007143\n66  19.798990   2.828427 190.918831   4.242641 138.592929  19.798990 131.521861\n67   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n68  57.982756  41.012193 152.735065  33.941125 100.409163  57.982756  93.338095\n69 106.066017  89.095454 104.651804  82.024387  52.325902 106.066017  45.254834\n70  67.882251  50.911688 142.835570  43.840620  90.509668  67.882251  83.438600\n71   2.828427  14.142136 207.889394  21.213203 155.563492   2.828427 148.492424\n72 147.078210 130.107648  63.639610 123.036580  11.313708 147.078210   4.242641\n73  28.284271  11.313708 182.433550   4.242641 130.107648  28.284271 123.036580\n74   9.899495   7.071068 200.818326  14.142136 148.492424   9.899495 141.421356\n75   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n76  21.213203   4.242641 189.504617   2.828427 137.178716  21.213203 130.107648\n77   8.485281   8.485281 202.232539  15.556349 149.906638   8.485281 142.835570\n78  18.384776   1.414214 192.333044   5.656854 140.007143  18.384776 132.936075\n79  12.727922   4.242641 197.989899  11.313708 145.663997  12.727922 138.592929\n80 298.399062 281.428499  87.681241 274.357431 140.007143 298.399062 147.078210\n           50         51         52         53         54         55         56\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51 127.279221                                                                  \n52 138.592929  11.313708                                                       \n53 196.575685  69.296465  57.982756                                            \n54 190.918831  63.639610  52.325902   5.656854                                 \n55 196.575685  69.296465  57.982756   0.000000   5.656854                      \n56 172.534055  45.254834  33.941125  24.041631  18.384776  24.041631           \n57 189.504617  62.225397  50.911688   7.071068   1.414214   7.071068  16.970563\n58 120.208153 247.487373 258.801082 316.783838 311.126984 316.783838 292.742207\n59 190.918831  63.639610  52.325902   5.656854   0.000000   5.656854  18.384776\n60 154.149278  26.870058  15.556349  42.426407  36.769553  42.426407  18.384776\n61 193.747258  66.468037  55.154329   2.828427   2.828427   2.828427  21.213203\n62 178.190909  50.911688  39.597980  18.384776  12.727922  18.384776   5.656854\n63 131.521861   4.242641   7.071068  65.053824  59.396970  65.053824  41.012193\n64  72.124892  55.154329  66.468037 124.450793 118.793939 124.450793 100.409163\n65  96.166522 223.445743 234.759451 292.742207 287.085353 292.742207 268.700577\n66 175.362482  48.083261  36.769553  21.213203  15.556349  21.213203   2.828427\n67 196.575685  69.296465  57.982756   0.000000   5.656854   0.000000  24.041631\n68 137.178716   9.899495   1.414214  59.396970  53.740115  59.396970  35.355339\n69  89.095454  38.183766  49.497475 107.480231 101.823376 107.480231  83.438600\n70 127.279221   0.000000  11.313708  69.296465  63.639610  69.296465  45.254834\n71 192.333044  65.053824  53.740115   4.242641   1.414214   4.242641  19.798990\n72  48.083261  79.195959  90.509668 148.492424 142.835570 148.492424 124.450793\n73 166.877200  39.597980  28.284271  29.698485  24.041631  29.698485   5.656854\n74 185.261977  57.982756  46.669048  11.313708   5.656854  11.313708  12.727922\n75 196.575685  69.296465  57.982756   0.000000   5.656854   0.000000  24.041631\n76 173.948268  46.669048  35.355339  22.627417  16.970563  22.627417   1.414214\n77 186.676190  59.396970  48.083261   9.899495   4.242641   9.899495  14.142136\n78 176.776695  49.497475  38.183766  19.798990  14.142136  19.798990   4.242641\n79 182.433550  55.154329  43.840620  14.142136   8.485281  14.142136   9.899495\n80 103.237590 230.516811 241.830519 299.813275 294.156421 299.813275 275.771645\n           57         58         59         60         61         62         63\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51                                                                             \n52                                                                             \n53                                                                             \n54                                                                             \n55                                                                             \n56                                                                             \n57                                                                             \n58 309.712770                                                                  \n59   1.414214 311.126984                                                       \n60  35.355339 274.357431  36.769553                                            \n61   4.242641 313.955411   2.828427  39.597980                                 \n62  11.313708 298.399062  12.727922  24.041631  15.556349                      \n63  57.982756 251.730014  59.396970  22.627417  62.225397  46.669048           \n64 117.379726 192.333044 118.793939  82.024387 121.622366 106.066017  59.396970\n65 285.671140  24.041631 287.085353 250.315801 289.913780 274.357431 227.688384\n66  14.142136 295.570635  15.556349  21.213203  18.384776   2.828427  43.840620\n67   7.071068 316.783838   5.656854  42.426407   2.828427  18.384776  65.053824\n68  52.325902 257.386868  53.740115  16.970563  56.568542  41.012193   5.656854\n69 100.409163 209.303607 101.823376  65.053824 104.651804  89.095454  42.426407\n70  62.225397 247.487373  63.639610  26.870058  66.468037  50.911688   4.242641\n71   2.828427 312.541197   1.414214  38.183766   1.414214  14.142136  60.811183\n72 141.421356 168.291414 142.835570 106.066017 145.663997 130.107648  83.438600\n73  22.627417 287.085353  24.041631  12.727922  26.870058  11.313708  35.355339\n74   4.242641 305.470129   5.656854  31.112698   8.485281   7.071068  53.740115\n75   7.071068 316.783838   5.656854  42.426407   2.828427  18.384776  65.053824\n76  15.556349 294.156421  16.970563  19.798990  19.798990   4.242641  42.426407\n77   2.828427 306.884343   4.242641  32.526912   7.071068   8.485281  55.154329\n78  12.727922 296.984848  14.142136  22.627417  16.970563   1.414214  45.254834\n79   7.071068 302.641702   8.485281  28.284271  11.313708   4.242641  50.911688\n80 292.742207  16.970563 294.156421 257.386868 296.984848 281.428499 234.759451\n           64         65         66         67         68         69         70\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51                                                                             \n52                                                                             \n53                                                                             \n54                                                                             \n55                                                                             \n56                                                                             \n57                                                                             \n58                                                                             \n59                                                                             \n60                                                                             \n61                                                                             \n62                                                                             \n63                                                                             \n64                                                                             \n65 168.291414                                                                  \n66 103.237590 271.529004                                                       \n67 124.450793 292.742207  21.213203                                            \n68  65.053824 233.345238  38.183766  59.396970                                 \n69  16.970563 185.261977  86.267027 107.480231  48.083261                      \n70  55.154329 223.445743  48.083261  69.296465   9.899495  38.183766           \n71 120.208153 288.499567  16.970563   4.242641  55.154329 103.237590  65.053824\n72  24.041631 144.249783 127.279221 148.492424  89.095454  41.012193  79.195959\n73  94.752309 263.043723   8.485281  29.698485  29.698485  77.781746  39.597980\n74 113.137085 281.428499   9.899495  11.313708  48.083261  96.166522  57.982756\n75 124.450793 292.742207  21.213203   0.000000  59.396970 107.480231  69.296465\n76 101.823376 270.114790   1.414214  22.627417  36.769553  84.852814  46.669048\n77 114.551299 282.842712  11.313708   9.899495  49.497475  97.580736  59.396970\n78 104.651804 272.943218   1.414214  19.798990  39.597980  87.681241  49.497475\n79 110.308658 278.600072   7.071068  14.142136  45.254834  93.338095  55.154329\n80 175.362482   7.071068 278.600072 299.813275 240.416306 192.333044 230.516811\n           71         72         73         74         75         76         77\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51                                                                             \n52                                                                             \n53                                                                             \n54                                                                             \n55                                                                             \n56                                                                             \n57                                                                             \n58                                                                             \n59                                                                             \n60                                                                             \n61                                                                             \n62                                                                             \n63                                                                             \n64                                                                             \n65                                                                             \n66                                                                             \n67                                                                             \n68                                                                             \n69                                                                             \n70                                                                             \n71                                                                             \n72 144.249783                                                                  \n73  25.455844 118.793939                                                       \n74   7.071068 137.178716  18.384776                                            \n75   4.242641 148.492424  29.698485  11.313708                                 \n76  18.384776 125.865007   7.071068  11.313708  22.627417                      \n77   5.656854 138.592929  19.798990   1.414214   9.899495  12.727922           \n78  15.556349 128.693434   9.899495   8.485281  19.798990   2.828427   9.899495\n79   9.899495 134.350288  15.556349   2.828427  14.142136   8.485281   4.242641\n80 295.570635 151.320851 270.114790 288.499567 299.813275 277.185858 289.913780\n           78         79\n2                       \n3                       \n4                       \n5                       \n6                       \n7                       \n8                       \n9                       \n10                      \n11                      \n12                      \n13                      \n14                      \n15                      \n16                      \n17                      \n18                      \n19                      \n20                      \n21                      \n22                      \n23                      \n24                      \n25                      \n26                      \n27                      \n28                      \n29                      \n30                      \n31                      \n32                      \n33                      \n34                      \n35                      \n36                      \n37                      \n38                      \n39                      \n40                      \n41                      \n42                      \n43                      \n44                      \n45                      \n46                      \n47                      \n48                      \n49                      \n50                      \n51                      \n52                      \n53                      \n54                      \n55                      \n56                      \n57                      \n58                      \n59                      \n60                      \n61                      \n62                      \n63                      \n64                      \n65                      \n66                      \n67                      \n68                      \n69                      \n70                      \n71                      \n72                      \n73                      \n74                      \n75                      \n76                      \n77                      \n78                      \n79   5.656854           \n80 280.014285 285.671140\n\n\n\n\n\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn a nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\nclass(nongeo_cluster)\n\n[1] \"hclust\"\n\n\n\n\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nmyanmar_ngeo_cluster &lt;- cbind(mmr_shp_mimu_2, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(myanmar_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we can perform spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(mmr_shp_mimu_2, mmr_shp_mimu_2)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.6 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.6)\n\nNext, cutree() is used to derive the cluster object.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with mmr_shp_mimu polygon feature data frame by using the code chunk below.\n\nMMR_sf_Gcluster &lt;- cbind(mmr_shp_mimu_2, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(MMR_sf_Gcluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode chunk below is used to reveal the distribution of a clustering variable (i.e Battles_2022_Incidents) by cluster.\n\n#ggplot(data = myanmar_ngeo_cluster,\n#       aes(x = CLUSTER, y = Battles_2022_Incidents)) +\n#  geom_boxplot()\n\n\n\n\n\nMain reference: Kam, T.S. (2024). Geographical Segmentation with Spatially Constrained Clustering Techniques"
  },
  {
    "objectID": "R-ex/R-Ex8/R_Ex8.html#filtering-the-data-set-for-just-battles_2022_incidents",
    "href": "R-ex/R-Ex8/R_Ex8.html#filtering-the-data-set-for-just-battles_2022_incidents",
    "title": "Geospatial Analysis3 - Spatially Constrained Clustering: ClustGeo method",
    "section": "",
    "text": "selected &lt;- data2_long %&gt;%\n  select(admin2, Battles_2022_Incidents)\n\n\nselected\n\n# A tibble: 80 × 2\n   admin2                      Battles_2022_Incidents\n   &lt;chr&gt;                                        &lt;int&gt;\n 1 Bago                                            12\n 2 Bawlake                                         16\n 3 Bhamo                                           54\n 4 Danu Self-Administered Zone                     15\n 5 Dawei                                          141\n 6 Det Khi Na                                       0\n 7 Falam                                           59\n 8 Gangaw                                         143\n 9 Hakha                                           43\n10 Hinthada                                         6\n# ℹ 70 more rows"
  },
  {
    "objectID": "R-ex/R-Ex8/R_Ex8.html#spatially-constrained-clustering-clustgeo-method",
    "href": "R-ex/R-Ex8/R_Ex8.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Geospatial Analysis3 - Spatially Constrained Clustering: ClustGeo method",
    "section": "",
    "text": "In R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(selected, method = 'euclidean')\n\n\nproxmat\n\n            1          2          3          4          5          6          7\n2    5.656854                                                                  \n3   59.396970  53.740115                                                       \n4    4.242641   1.414214  55.154329                                            \n5  182.433550 176.776695 123.036580 178.190909                                 \n6   16.970563  22.627417  76.367532  21.213203 199.404112                      \n7   66.468037  60.811183   7.071068  62.225397 115.965512  83.438600           \n8  185.261977 179.605122 125.865007 181.019336   2.828427 202.232539 118.793939\n9   43.840620  38.183766  15.556349  39.597980 138.592929  60.811183  22.627417\n10   8.485281  14.142136  67.882251  12.727922 190.918831   8.485281  74.953319\n11   2.828427   8.485281  62.225397   7.071068 185.261977  14.142136  69.296465\n12  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n13  21.213203  15.556349  38.183766  16.970563 161.220346  38.183766  45.254834\n14  38.183766  32.526912  21.213203  33.941125 144.249783  55.154329  28.284271\n15 213.546248 207.889394 154.149278 209.303607  31.112698 230.516811 147.078210\n16  31.112698  25.455844  28.284271  26.870058 151.320851  48.083261  35.355339\n17 206.475180 200.818326 147.078210 202.232539  24.041631 223.445743 140.007143\n18 108.894444 103.237590  49.497475 104.651804  73.539105 125.865007  42.426407\n19  43.840620  38.183766  15.556349  39.597980 138.592929  60.811183  22.627417\n20   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n21  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n22   8.485281  14.142136  67.882251  12.727922 190.918831   8.485281  74.953319\n23  38.183766  32.526912  21.213203  33.941125 144.249783  55.154329  28.284271\n24   7.071068  12.727922  66.468037  11.313708 189.504617   9.899495  73.539105\n25   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n26  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n27  12.727922  18.384776  72.124892  16.970563 195.161472   4.242641  79.195959\n28  11.313708   5.656854  48.083261   7.071068 171.119841  28.284271  55.154329\n29 359.210245 353.553391 299.813275 354.967604 176.776695 376.180808 292.742207\n30   5.656854  11.313708  65.053824   9.899495 188.090404  11.313708  72.124892\n31   5.656854  11.313708  65.053824   9.899495 188.090404  11.313708  72.124892\n32  46.669048  41.012193  12.727922  42.426407 135.764502  63.639610  19.798990\n33  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n34  97.580736  91.923882  38.183766  93.338095  84.852814 114.551299  31.112698\n35  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n36  67.882251  62.225397   8.485281  63.639610 114.551299  84.852814   1.414214\n37  14.142136   8.485281  45.254834   9.899495 168.291414  31.112698  52.325902\n38 108.894444 103.237590  49.497475 104.651804  73.539105 125.865007  42.426407\n39  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n40   0.000000   5.656854  59.396970   4.242641 182.433550  16.970563  66.468037\n41  77.781746  72.124892  18.384776  73.539105 104.651804  94.752309  11.313708\n42  70.710678  65.053824  11.313708  66.468037 111.722871  87.681241   4.242641\n43  15.556349  21.213203  74.953319  19.798990 197.989899   1.414214  82.024387\n44   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n45 195.161472 189.504617 135.764502 190.918831  12.727922 212.132034 128.693434\n46   8.485281   2.828427  50.911688   4.242641 173.948268  25.455844  57.982756\n47 142.835570 137.178716  83.438600 138.592929  39.597980 159.806133  76.367532\n48  15.556349  21.213203  74.953319  19.798990 197.989899   1.414214  82.024387\n49 135.764502 130.107648  76.367532 131.521861  46.669048 152.735065  69.296465\n50 179.605122 173.948268 120.208153 175.362482   2.828427 196.575685 113.137085\n51  52.325902  46.669048   7.071068  48.083261 130.107648  69.296465  14.142136\n52  41.012193  35.355339  18.384776  36.769553 141.421356  57.982756  25.455844\n53  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n54  11.313708  16.970563  70.710678  15.556349 193.747258   5.656854  77.781746\n55  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n56   7.071068   1.414214  52.325902   2.828427 175.362482  24.041631  59.396970\n57   9.899495  15.556349  69.296465  14.142136 192.333044   7.071068  76.367532\n58 299.813275 294.156421 240.416306 295.570635 117.379726 316.783838 233.345238\n59  11.313708  16.970563  70.710678  15.556349 193.747258   5.656854  77.781746\n60  25.455844  19.798990  33.941125  21.213203 156.977705  42.426407  41.012193\n61  14.142136  19.798990  73.539105  18.384776 196.575685   2.828427  80.610173\n62   1.414214   4.242641  57.982756   2.828427 181.019336  18.384776  65.053824\n63  48.083261  42.426407  11.313708  43.840620 134.350288  65.053824  18.384776\n64 107.480231 101.823376  48.083261 103.237590  74.953319 124.450793  41.012193\n65 275.771645 270.114790 216.374675 271.529004  93.338095 292.742207 209.303607\n66   4.242641   1.414214  55.154329   0.000000 178.190909  21.213203  62.225397\n67  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n68  42.426407  36.769553  16.970563  38.183766 140.007143  59.396970  24.041631\n69  90.509668  84.852814  31.112698  86.267027  91.923882 107.480231  24.041631\n70  52.325902  46.669048   7.071068  48.083261 130.107648  69.296465  14.142136\n71  12.727922  18.384776  72.124892  16.970563 195.161472   4.242641  79.195959\n72 131.521861 125.865007  72.124892 127.279221  50.911688 148.492424  65.053824\n73  12.727922   7.071068  46.669048   8.485281 169.705627  29.698485  53.740115\n74   5.656854  11.313708  65.053824   9.899495 188.090404  11.313708  72.124892\n75  16.970563  22.627417  76.367532  21.213203 199.404112   0.000000  83.438600\n76   5.656854   0.000000  53.740115   1.414214 176.776695  22.627417  60.811183\n77   7.071068  12.727922  66.468037  11.313708 189.504617   9.899495  73.539105\n78   2.828427   2.828427  56.568542   1.414214 179.605122  19.798990  63.639610\n79   2.828427   8.485281  62.225397   7.071068 185.261977  14.142136  69.296465\n80 282.842712 277.185858 223.445743 278.600072 100.409163 299.813275 216.374675\n            8          9         10         11         12         13         14\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9  141.421356                                                                  \n10 193.747258  52.325902                                                       \n11 188.090404  46.669048   5.656854                                            \n12 202.232539  60.811183   8.485281  14.142136                                 \n13 164.048773  22.627417  29.698485  24.041631  38.183766                      \n14 147.078210   5.656854  46.669048  41.012193  55.154329  16.970563           \n15  28.284271 169.705627 222.031529 216.374675 230.516811 192.333044 175.362482\n16 154.149278  12.727922  39.597980  33.941125  48.083261   9.899495   7.071068\n17  21.213203 162.634560 214.960461 209.303607 223.445743 185.261977 168.291414\n18  76.367532  65.053824 117.379726 111.722871 125.865007  87.681241  70.710678\n19 141.421356   0.000000  52.325902  46.669048  60.811183  22.627417   5.656854\n20 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n21 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n22 193.747258  52.325902   0.000000   5.656854   8.485281  29.698485  46.669048\n23 147.078210   5.656854  46.669048  41.012193  55.154329  16.970563   0.000000\n24 192.333044  50.911688   1.414214   4.242641   9.899495  28.284271  45.254834\n25 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n26 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n27 197.989899  56.568542   4.242641   9.899495   4.242641  33.941125  50.911688\n28 173.948268  32.526912  19.798990  14.142136  28.284271   9.899495  26.870058\n29 173.948268 315.369624 367.695526 362.038672 376.180808 337.997041 321.026479\n30 190.918831  49.497475   2.828427   2.828427  11.313708  26.870058  43.840620\n31 190.918831  49.497475   2.828427   2.828427  11.313708  26.870058  43.840620\n32 138.592929   2.828427  55.154329  49.497475  63.639610  25.455844   8.485281\n33 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n34  87.681241  53.740115 106.066017 100.409163 114.551299  76.367532  59.396970\n35 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n36 117.379726  24.041631  76.367532  70.710678  84.852814  46.669048  29.698485\n37 171.119841  29.698485  22.627417  16.970563  31.112698   7.071068  24.041631\n38  76.367532  65.053824 117.379726 111.722871 125.865007  87.681241  70.710678\n39 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n40 185.261977  43.840620   8.485281   2.828427  16.970563  21.213203  38.183766\n41 107.480231  33.941125  86.267027  80.610173  94.752309  56.568542  39.597980\n42 114.551299  26.870058  79.195959  73.539105  87.681241  49.497475  32.526912\n43 200.818326  59.396970   7.071068  12.727922   1.414214  36.769553  53.740115\n44 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n45   9.899495 151.320851 203.646753 197.989899 212.132034 173.948268 156.977705\n46 176.776695  35.355339  16.970563  11.313708  25.455844  12.727922  29.698485\n47  42.426407  98.994949 151.320851 145.663997 159.806133 121.622366 104.651804\n48 200.818326  59.396970   7.071068  12.727922   1.414214  36.769553  53.740115\n49  49.497475  91.923882 144.249783 138.592929 152.735065 114.551299  97.580736\n50   5.656854 135.764502 188.090404 182.433550 196.575685 158.391919 141.421356\n51 132.936075   8.485281  60.811183  55.154329  69.296465  31.112698  14.142136\n52 144.249783   2.828427  49.497475  43.840620  57.982756  19.798990   2.828427\n53 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n54 196.575685  55.154329   2.828427   8.485281   5.656854  32.526912  49.497475\n55 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n56 178.190909  36.769553  15.556349   9.899495  24.041631  14.142136  31.112698\n57 195.161472  53.740115   1.414214   7.071068   7.071068  31.112698  48.083261\n58 114.551299 255.972655 308.298557 302.641702 316.783838 278.600072 261.629509\n59 196.575685  55.154329   2.828427   8.485281   5.656854  32.526912  49.497475\n60 159.806133  18.384776  33.941125  28.284271  42.426407   4.242641  12.727922\n61 199.404112  57.982756   5.656854  11.313708   2.828427  35.355339  52.325902\n62 183.847763  42.426407   9.899495   4.242641  18.384776  19.798990  36.769553\n63 137.178716   4.242641  56.568542  50.911688  65.053824  26.870058   9.899495\n64  77.781746  63.639610 115.965512 110.308658 124.450793  86.267027  69.296465\n65  90.509668 231.931024 284.256926 278.600072 292.742207 254.558441 237.587878\n66 181.019336  39.597980  12.727922   7.071068  21.213203  16.970563  33.941125\n67 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n68 142.835570   1.414214  50.911688  45.254834  59.396970  21.213203   4.242641\n69  94.752309  46.669048  98.994949  93.338095 107.480231  69.296465  52.325902\n70 132.936075   8.485281  60.811183  55.154329  69.296465  31.112698  14.142136\n71 197.989899  56.568542   4.242641   9.899495   4.242641  33.941125  50.911688\n72  53.740115  87.681241 140.007143 134.350288 148.492424 110.308658  93.338095\n73 172.534055  31.112698  21.213203  15.556349  29.698485   8.485281  25.455844\n74 190.918831  49.497475   2.828427   2.828427  11.313708  26.870058  43.840620\n75 202.232539  60.811183   8.485281  14.142136   0.000000  38.183766  55.154329\n76 179.605122  38.183766  14.142136   8.485281  22.627417  15.556349  32.526912\n77 192.333044  50.911688   1.414214   4.242641   9.899495  28.284271  45.254834\n78 182.433550  41.012193  11.313708   5.656854  19.798990  18.384776  35.355339\n79 188.090404  46.669048   5.656854   0.000000  14.142136  24.041631  41.012193\n80  97.580736 239.002092 291.327994 285.671140 299.813275 261.629509 244.658946\n           15         16         17         18         19         20         21\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16 182.433550                                                                  \n17   7.071068 175.362482                                                       \n18 104.651804  77.781746  97.580736                                            \n19 169.705627  12.727922 162.634560  65.053824                                 \n20 212.132034  29.698485 205.060967 107.480231  42.426407                      \n21 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776           \n22 222.031529  39.597980 214.960461 117.379726  52.325902   9.899495   8.485281\n23 175.362482   7.071068 168.291414  70.710678   5.656854  36.769553  55.154329\n24 220.617316  38.183766 213.546248 115.965512  50.911688   8.485281   9.899495\n25 212.132034  29.698485 205.060967 107.480231  42.426407   0.000000  18.384776\n26 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n27 226.274170  43.840620 219.203102 121.622366  56.568542  14.142136   4.242641\n28 202.232539  19.798990 195.161472  97.580736  32.526912   9.899495  28.284271\n29 145.663997 328.097546 152.735065 250.315801 315.369624 357.796031 376.180808\n30 219.203102  36.769553 212.132034 114.551299  49.497475   7.071068  11.313708\n31 219.203102  36.769553 212.132034 114.551299  49.497475   7.071068  11.313708\n32 166.877200  15.556349 159.806133  62.225397   2.828427  45.254834  63.639610\n33 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n34 115.965512  66.468037 108.894444  11.313708  53.740115  96.166522 114.551299\n35 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n36 145.663997  36.769553 138.592929  41.012193  24.041631  66.468037  84.852814\n37 199.404112  16.970563 192.333044  94.752309  29.698485  12.727922  31.112698\n38 104.651804  77.781746  97.580736   0.000000  65.053824 107.480231 125.865007\n39 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n40 213.546248  31.112698 206.475180 108.894444  43.840620   1.414214  16.970563\n41 135.764502  46.669048 128.693434  31.112698  33.941125  76.367532  94.752309\n42 142.835570  39.597980 135.764502  38.183766  26.870058  69.296465  87.681241\n43 229.102597  46.669048 222.031529 124.450793  59.396970  16.970563   1.414214\n44 212.132034  29.698485 205.060967 107.480231  42.426407   0.000000  18.384776\n45  18.384776 164.048773  11.313708  86.267027 151.320851 193.747258 212.132034\n46 205.060967  22.627417 197.989899 100.409163  35.355339   7.071068  25.455844\n47  70.710678 111.722871  63.639610  33.941125  98.994949 141.421356 159.806133\n48 229.102597  46.669048 222.031529 124.450793  59.396970  16.970563   1.414214\n49  77.781746 104.651804  70.710678  26.870058  91.923882 134.350288 152.735065\n50  33.941125 148.492424  26.870058  70.710678 135.764502 178.190909 196.575685\n51 161.220346  21.213203 154.149278  56.568542   8.485281  50.911688  69.296465\n52 172.534055   9.899495 165.462987  67.882251   2.828427  39.597980  57.982756\n53 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n54 224.859956  42.426407 217.788889 120.208153  55.154329  12.727922   5.656854\n55 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n56 206.475180  24.041631 199.404112 101.823376  36.769553   5.656854  24.041631\n57 223.445743  41.012193 216.374675 118.793939  53.740115  11.313708   7.071068\n58  86.267027 268.700577  93.338095 190.918831 255.972655 298.399062 316.783838\n59 224.859956  42.426407 217.788889 120.208153  55.154329  12.727922   5.656854\n60 188.090404   5.656854 181.019336  83.438600  18.384776  24.041631  42.426407\n61 227.688384  45.254834 220.617316 123.036580  57.982756  15.556349   2.828427\n62 212.132034  29.698485 205.060967 107.480231  42.426407   0.000000  18.384776\n63 165.462987  16.970563 158.391919  60.811183   4.242641  46.669048  65.053824\n64 106.066017  76.367532  98.994949   1.414214  63.639610 106.066017 124.450793\n65  62.225397 244.658946  69.296465 166.877200 231.931024 274.357431 292.742207\n66 209.303607  26.870058 202.232539 104.651804  39.597980   2.828427  21.213203\n67 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n68 171.119841  11.313708 164.048773  66.468037   1.414214  41.012193  59.396970\n69 123.036580  59.396970 115.965512  18.384776  46.669048  89.095454 107.480231\n70 161.220346  21.213203 154.149278  56.568542   8.485281  50.911688  69.296465\n71 226.274170  43.840620 219.203102 121.622366  56.568542  14.142136   4.242641\n72  82.024387 100.409163  74.953319  22.627417  87.681241 130.107648 148.492424\n73 200.818326  18.384776 193.747258  96.166522  31.112698  11.313708  29.698485\n74 219.203102  36.769553 212.132034 114.551299  49.497475   7.071068  11.313708\n75 230.516811  48.083261 223.445743 125.865007  60.811183  18.384776   0.000000\n76 207.889394  25.455844 200.818326 103.237590  38.183766   4.242641  22.627417\n77 220.617316  38.183766 213.546248 115.965512  50.911688   8.485281   9.899495\n78 210.717821  28.284271 203.646753 106.066017  41.012193   1.414214  19.798990\n79 216.374675  33.941125 209.303607 111.722871  46.669048   4.242641  14.142136\n80  69.296465 251.730014  76.367532 173.948268 239.002092 281.428499 299.813275\n           22         23         24         25         26         27         28\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23  46.669048                                                                  \n24   1.414214  45.254834                                                       \n25   9.899495  36.769553   8.485281                                            \n26   5.656854  52.325902   7.071068  15.556349                                 \n27   4.242641  50.911688   5.656854  14.142136   1.414214                      \n28  19.798990  26.870058  18.384776   9.899495  25.455844  24.041631           \n29 367.695526 321.026479 366.281313 357.796031 373.352380 371.938167 347.896536\n30   2.828427  43.840620   1.414214   7.071068   8.485281   7.071068  16.970563\n31   2.828427  43.840620   1.414214   7.071068   8.485281   7.071068  16.970563\n32  55.154329   8.485281  53.740115  45.254834  60.811183  59.396970  35.355339\n33   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n34 106.066017  59.396970 104.651804  96.166522 111.722871 110.308658  86.267027\n35   5.656854  52.325902   7.071068  15.556349   0.000000   1.414214  25.455844\n36  76.367532  29.698485  74.953319  66.468037  82.024387  80.610173  56.568542\n37  22.627417  24.041631  21.213203  12.727922  28.284271  26.870058   2.828427\n38 117.379726  70.710678 115.965512 107.480231 123.036580 121.622366  97.580736\n39   5.656854  52.325902   7.071068  15.556349   0.000000   1.414214  25.455844\n40   8.485281  38.183766   7.071068   1.414214  14.142136  12.727922  11.313708\n41  86.267027  39.597980  84.852814  76.367532  91.923882  90.509668  66.468037\n42  79.195959  32.526912  77.781746  69.296465  84.852814  83.438600  59.396970\n43   7.071068  53.740115   8.485281  16.970563   1.414214   2.828427  26.870058\n44   9.899495  36.769553   8.485281   0.000000  15.556349  14.142136   9.899495\n45 203.646753 156.977705 202.232539 193.747258 209.303607 207.889394 183.847763\n46  16.970563  29.698485  15.556349   7.071068  22.627417  21.213203   2.828427\n47 151.320851 104.651804 149.906638 141.421356 156.977705 155.563492 131.521861\n48   7.071068  53.740115   8.485281  16.970563   1.414214   2.828427  26.870058\n49 144.249783  97.580736 142.835570 134.350288 149.906638 148.492424 124.450793\n50 188.090404 141.421356 186.676190 178.190909 193.747258 192.333044 168.291414\n51  60.811183  14.142136  59.396970  50.911688  66.468037  65.053824  41.012193\n52  49.497475   2.828427  48.083261  39.597980  55.154329  53.740115  29.698485\n53   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n54   2.828427  49.497475   4.242641  12.727922   2.828427   1.414214  22.627417\n55   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n56  15.556349  31.112698  14.142136   5.656854  21.213203  19.798990   4.242641\n57   1.414214  48.083261   2.828427  11.313708   4.242641   2.828427  21.213203\n58 308.298557 261.629509 306.884343 298.399062 313.955411 312.541197 288.499567\n59   2.828427  49.497475   4.242641  12.727922   2.828427   1.414214  22.627417\n60  33.941125  12.727922  32.526912  24.041631  39.597980  38.183766  14.142136\n61   5.656854  52.325902   7.071068  15.556349   0.000000   1.414214  25.455844\n62   9.899495  36.769553   8.485281   0.000000  15.556349  14.142136   9.899495\n63  56.568542   9.899495  55.154329  46.669048  62.225397  60.811183  36.769553\n64 115.965512  69.296465 114.551299 106.066017 121.622366 120.208153  96.166522\n65 284.256926 237.587878 282.842712 274.357431 289.913780 288.499567 264.457936\n66  12.727922  33.941125  11.313708   2.828427  18.384776  16.970563   7.071068\n67   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n68  50.911688   4.242641  49.497475  41.012193  56.568542  55.154329  31.112698\n69  98.994949  52.325902  97.580736  89.095454 104.651804 103.237590  79.195959\n70  60.811183  14.142136  59.396970  50.911688  66.468037  65.053824  41.012193\n71   4.242641  50.911688   5.656854  14.142136   1.414214   0.000000  24.041631\n72 140.007143  93.338095 138.592929 130.107648 145.663997 144.249783 120.208153\n73  21.213203  25.455844  19.798990  11.313708  26.870058  25.455844   1.414214\n74   2.828427  43.840620   1.414214   7.071068   8.485281   7.071068  16.970563\n75   8.485281  55.154329   9.899495  18.384776   2.828427   4.242641  28.284271\n76  14.142136  32.526912  12.727922   4.242641  19.798990  18.384776   5.656854\n77   1.414214  45.254834   0.000000   8.485281   7.071068   5.656854  18.384776\n78  11.313708  35.355339   9.899495   1.414214  16.970563  15.556349   8.485281\n79   5.656854  41.012193   4.242641   4.242641  11.313708   9.899495  14.142136\n80 291.327994 244.658946 289.913780 281.428499 296.984848 295.570635 271.529004\n           29         30         31         32         33         34         35\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30 364.867099                                                                  \n31 364.867099   0.000000                                                       \n32 312.541197  52.325902  52.325902                                            \n33 376.180808  11.313708  11.313708  63.639610                                 \n34 261.629509 103.237590 103.237590  50.911688 114.551299                      \n35 373.352380   8.485281   8.485281  60.811183   2.828427 111.722871           \n36 291.327994  73.539105  73.539105  21.213203  84.852814  29.698485  82.024387\n37 345.068109  19.798990  19.798990  32.526912  31.112698  83.438600  28.284271\n38 250.315801 114.551299 114.551299  62.225397 125.865007  11.313708 123.036580\n39 373.352380   8.485281   8.485281  60.811183   2.828427 111.722871   0.000000\n40 359.210245   5.656854   5.656854  46.669048  16.970563  97.580736  14.142136\n41 281.428499  83.438600  83.438600  31.112698  94.752309  19.798990  91.923882\n42 288.499567  76.367532  76.367532  24.041631  87.681241  26.870058  84.852814\n43 374.766594   9.899495   9.899495  62.225397   1.414214 113.137085   1.414214\n44 357.796031   7.071068   7.071068  45.254834  18.384776  96.166522  15.556349\n45 164.048773 200.818326 200.818326 148.492424 212.132034  97.580736 209.303607\n46 350.724963  14.142136  14.142136  38.183766  25.455844  89.095454  22.627417\n47 216.374675 148.492424 148.492424  96.166522 159.806133  45.254834 156.977705\n48 374.766594   9.899495   9.899495  62.225397   1.414214 113.137085   1.414214\n49 223.445743 141.421356 141.421356  89.095454 152.735065  38.183766 149.906638\n50 179.605122 185.261977 185.261977 132.936075 196.575685  82.024387 193.747258\n51 306.884343  57.982756  57.982756   5.656854  69.296465  45.254834  66.468037\n52 318.198052  46.669048  46.669048   5.656854  57.982756  56.568542  55.154329\n53 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n54 370.523953   5.656854   5.656854  57.982756   5.656854 108.894444   2.828427\n55 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n56 352.139177  12.727922  12.727922  39.597980  24.041631  90.509668  21.213203\n57 369.109740   4.242641   4.242641  56.568542   7.071068 107.480231   4.242641\n58  59.396970 305.470129 305.470129 253.144228 316.783838 202.232539 313.955411\n59 370.523953   5.656854   5.656854  57.982756   5.656854 108.894444   2.828427\n60 333.754401  31.112698  31.112698  21.213203  42.426407  72.124892  39.597980\n61 373.352380   8.485281   8.485281  60.811183   2.828427 111.722871   0.000000\n62 357.796031   7.071068   7.071068  45.254834  18.384776  96.166522  15.556349\n63 311.126984  53.740115  53.740115   1.414214  65.053824  49.497475  62.225397\n64 251.730014 113.137085 113.137085  60.811183 124.450793   9.899495 121.622366\n65  83.438600 281.428499 281.428499 229.102597 292.742207 178.190909 289.913780\n66 354.967604   9.899495   9.899495  42.426407  21.213203  93.338095  18.384776\n67 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n68 316.783838  48.083261  48.083261   4.242641  59.396970  55.154329  56.568542\n69 268.700577  96.166522  96.166522  43.840620 107.480231   7.071068 104.651804\n70 306.884343  57.982756  57.982756   5.656854  69.296465  45.254834  66.468037\n71 371.938167   7.071068   7.071068  59.396970   4.242641 110.308658   1.414214\n72 227.688384 137.178716 137.178716  84.852814 148.492424  33.941125 145.663997\n73 346.482323  18.384776  18.384776  33.941125  29.698485  84.852814  26.870058\n74 364.867099   0.000000   0.000000  52.325902  11.313708 103.237590   8.485281\n75 376.180808  11.313708  11.313708  63.639610   0.000000 114.551299   2.828427\n76 353.553391  11.313708  11.313708  41.012193  22.627417  91.923882  19.798990\n77 366.281313   1.414214   1.414214  53.740115   9.899495 104.651804   7.071068\n78 356.381818   8.485281   8.485281  43.840620  19.798990  94.752309  16.970563\n79 362.038672   2.828427   2.828427  49.497475  14.142136 100.409163  11.313708\n80  76.367532 288.499567 288.499567 236.173665 299.813275 185.261977 296.984848\n           36         37         38         39         40         41         42\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37  53.740115                                                                  \n38  41.012193  94.752309                                                       \n39  82.024387  28.284271 123.036580                                            \n40  67.882251  14.142136 108.894444  14.142136                                 \n41   9.899495  63.639610  31.112698  91.923882  77.781746                      \n42   2.828427  56.568542  38.183766  84.852814  70.710678   7.071068           \n43  83.438600  29.698485 124.450793   1.414214  15.556349  93.338095  86.267027\n44  66.468037  12.727922 107.480231  15.556349   1.414214  76.367532  69.296465\n45 127.279221 181.019336  86.267027 209.303607 195.161472 117.379726 124.450793\n46  59.396970   5.656854 100.409163  22.627417   8.485281  69.296465  62.225397\n47  74.953319 128.693434  33.941125 156.977705 142.835570  65.053824  72.124892\n48  83.438600  29.698485 124.450793   1.414214  15.556349  93.338095  86.267027\n49  67.882251 121.622366  26.870058 149.906638 135.764502  57.982756  65.053824\n50 111.722871 165.462987  70.710678 193.747258 179.605122 101.823376 108.894444\n51  15.556349  38.183766  56.568542  66.468037  52.325902  25.455844  18.384776\n52  26.870058  26.870058  67.882251  55.154329  41.012193  36.769553  29.698485\n53  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n54  79.195959  25.455844 120.208153   2.828427  11.313708  89.095454  82.024387\n55  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n56  60.811183   7.071068 101.823376  21.213203   7.071068  70.710678  63.639610\n57  77.781746  24.041631 118.793939   4.242641   9.899495  87.681241  80.610173\n58 231.931024 285.671140 190.918831 313.955411 299.813275 222.031529 229.102597\n59  79.195959  25.455844 120.208153   2.828427  11.313708  89.095454  82.024387\n60  42.426407  11.313708  83.438600  39.597980  25.455844  52.325902  45.254834\n61  82.024387  28.284271 123.036580   0.000000  14.142136  91.923882  84.852814\n62  66.468037  12.727922 107.480231  15.556349   1.414214  76.367532  69.296465\n63  19.798990  33.941125  60.811183  62.225397  48.083261  29.698485  22.627417\n64  39.597980  93.338095   1.414214 121.622366 107.480231  29.698485  36.769553\n65 207.889394 261.629509 166.877200 289.913780 275.771645 197.989899 205.060967\n66  63.639610   9.899495 104.651804  18.384776   4.242641  73.539105  66.468037\n67  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n68  25.455844  28.284271  66.468037  56.568542  42.426407  35.355339  28.284271\n69  22.627417  76.367532  18.384776 104.651804  90.509668  12.727922  19.798990\n70  15.556349  38.183766  56.568542  66.468037  52.325902  25.455844  18.384776\n71  80.610173  26.870058 121.622366   1.414214  12.727922  90.509668  83.438600\n72  63.639610 117.379726  22.627417 145.663997 131.521861  53.740115  60.811183\n73  55.154329   1.414214  96.166522  26.870058  12.727922  65.053824  57.982756\n74  73.539105  19.798990 114.551299   8.485281   5.656854  83.438600  76.367532\n75  84.852814  31.112698 125.865007   2.828427  16.970563  94.752309  87.681241\n76  62.225397   8.485281 103.237590  19.798990   5.656854  72.124892  65.053824\n77  74.953319  21.213203 115.965512   7.071068   7.071068  84.852814  77.781746\n78  65.053824  11.313708 106.066017  16.970563   2.828427  74.953319  67.882251\n79  70.710678  16.970563 111.722871  11.313708   2.828427  80.610173  73.539105\n80 214.960461 268.700577 173.948268 296.984848 282.842712 205.060967 212.132034\n           43         44         45         46         47         48         49\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44  16.970563                                                                  \n45 210.717821 193.747258                                                       \n46  24.041631   7.071068 186.676190                                            \n47 158.391919 141.421356  52.325902 134.350288                                 \n48   0.000000  16.970563 210.717821  24.041631 158.391919                      \n49 151.320851 134.350288  59.396970 127.279221   7.071068 151.320851           \n50 195.161472 178.190909  15.556349 171.119841  36.769553 195.161472  43.840620\n51  67.882251  50.911688 142.835570  43.840620  90.509668  67.882251  83.438600\n52  56.568542  39.597980 154.149278  32.526912 101.823376  56.568542  94.752309\n53   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n54   4.242641  12.727922 206.475180  19.798990 154.149278   4.242641 147.078210\n55   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n56  22.627417   5.656854 188.090404   1.414214 135.764502  22.627417 128.693434\n57   5.656854  11.313708 205.060967  18.384776 152.735065   5.656854 145.663997\n58 315.369624 298.399062 104.651804 291.327994 156.977705 315.369624 164.048773\n59   4.242641  12.727922 206.475180  19.798990 154.149278   4.242641 147.078210\n60  41.012193  24.041631 169.705627  16.970563 117.379726  41.012193 110.308658\n61   1.414214  15.556349 209.303607  22.627417 156.977705   1.414214 149.906638\n62  16.970563   0.000000 193.747258   7.071068 141.421356  16.970563 134.350288\n63  63.639610  46.669048 147.078210  39.597980  94.752309  63.639610  87.681241\n64 123.036580 106.066017  87.681241  98.994949  35.355339 123.036580  28.284271\n65 291.327994 274.357431  80.610173 267.286363 132.936075 291.327994 140.007143\n66  19.798990   2.828427 190.918831   4.242641 138.592929  19.798990 131.521861\n67   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n68  57.982756  41.012193 152.735065  33.941125 100.409163  57.982756  93.338095\n69 106.066017  89.095454 104.651804  82.024387  52.325902 106.066017  45.254834\n70  67.882251  50.911688 142.835570  43.840620  90.509668  67.882251  83.438600\n71   2.828427  14.142136 207.889394  21.213203 155.563492   2.828427 148.492424\n72 147.078210 130.107648  63.639610 123.036580  11.313708 147.078210   4.242641\n73  28.284271  11.313708 182.433550   4.242641 130.107648  28.284271 123.036580\n74   9.899495   7.071068 200.818326  14.142136 148.492424   9.899495 141.421356\n75   1.414214  18.384776 212.132034  25.455844 159.806133   1.414214 152.735065\n76  21.213203   4.242641 189.504617   2.828427 137.178716  21.213203 130.107648\n77   8.485281   8.485281 202.232539  15.556349 149.906638   8.485281 142.835570\n78  18.384776   1.414214 192.333044   5.656854 140.007143  18.384776 132.936075\n79  12.727922   4.242641 197.989899  11.313708 145.663997  12.727922 138.592929\n80 298.399062 281.428499  87.681241 274.357431 140.007143 298.399062 147.078210\n           50         51         52         53         54         55         56\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51 127.279221                                                                  \n52 138.592929  11.313708                                                       \n53 196.575685  69.296465  57.982756                                            \n54 190.918831  63.639610  52.325902   5.656854                                 \n55 196.575685  69.296465  57.982756   0.000000   5.656854                      \n56 172.534055  45.254834  33.941125  24.041631  18.384776  24.041631           \n57 189.504617  62.225397  50.911688   7.071068   1.414214   7.071068  16.970563\n58 120.208153 247.487373 258.801082 316.783838 311.126984 316.783838 292.742207\n59 190.918831  63.639610  52.325902   5.656854   0.000000   5.656854  18.384776\n60 154.149278  26.870058  15.556349  42.426407  36.769553  42.426407  18.384776\n61 193.747258  66.468037  55.154329   2.828427   2.828427   2.828427  21.213203\n62 178.190909  50.911688  39.597980  18.384776  12.727922  18.384776   5.656854\n63 131.521861   4.242641   7.071068  65.053824  59.396970  65.053824  41.012193\n64  72.124892  55.154329  66.468037 124.450793 118.793939 124.450793 100.409163\n65  96.166522 223.445743 234.759451 292.742207 287.085353 292.742207 268.700577\n66 175.362482  48.083261  36.769553  21.213203  15.556349  21.213203   2.828427\n67 196.575685  69.296465  57.982756   0.000000   5.656854   0.000000  24.041631\n68 137.178716   9.899495   1.414214  59.396970  53.740115  59.396970  35.355339\n69  89.095454  38.183766  49.497475 107.480231 101.823376 107.480231  83.438600\n70 127.279221   0.000000  11.313708  69.296465  63.639610  69.296465  45.254834\n71 192.333044  65.053824  53.740115   4.242641   1.414214   4.242641  19.798990\n72  48.083261  79.195959  90.509668 148.492424 142.835570 148.492424 124.450793\n73 166.877200  39.597980  28.284271  29.698485  24.041631  29.698485   5.656854\n74 185.261977  57.982756  46.669048  11.313708   5.656854  11.313708  12.727922\n75 196.575685  69.296465  57.982756   0.000000   5.656854   0.000000  24.041631\n76 173.948268  46.669048  35.355339  22.627417  16.970563  22.627417   1.414214\n77 186.676190  59.396970  48.083261   9.899495   4.242641   9.899495  14.142136\n78 176.776695  49.497475  38.183766  19.798990  14.142136  19.798990   4.242641\n79 182.433550  55.154329  43.840620  14.142136   8.485281  14.142136   9.899495\n80 103.237590 230.516811 241.830519 299.813275 294.156421 299.813275 275.771645\n           57         58         59         60         61         62         63\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51                                                                             \n52                                                                             \n53                                                                             \n54                                                                             \n55                                                                             \n56                                                                             \n57                                                                             \n58 309.712770                                                                  \n59   1.414214 311.126984                                                       \n60  35.355339 274.357431  36.769553                                            \n61   4.242641 313.955411   2.828427  39.597980                                 \n62  11.313708 298.399062  12.727922  24.041631  15.556349                      \n63  57.982756 251.730014  59.396970  22.627417  62.225397  46.669048           \n64 117.379726 192.333044 118.793939  82.024387 121.622366 106.066017  59.396970\n65 285.671140  24.041631 287.085353 250.315801 289.913780 274.357431 227.688384\n66  14.142136 295.570635  15.556349  21.213203  18.384776   2.828427  43.840620\n67   7.071068 316.783838   5.656854  42.426407   2.828427  18.384776  65.053824\n68  52.325902 257.386868  53.740115  16.970563  56.568542  41.012193   5.656854\n69 100.409163 209.303607 101.823376  65.053824 104.651804  89.095454  42.426407\n70  62.225397 247.487373  63.639610  26.870058  66.468037  50.911688   4.242641\n71   2.828427 312.541197   1.414214  38.183766   1.414214  14.142136  60.811183\n72 141.421356 168.291414 142.835570 106.066017 145.663997 130.107648  83.438600\n73  22.627417 287.085353  24.041631  12.727922  26.870058  11.313708  35.355339\n74   4.242641 305.470129   5.656854  31.112698   8.485281   7.071068  53.740115\n75   7.071068 316.783838   5.656854  42.426407   2.828427  18.384776  65.053824\n76  15.556349 294.156421  16.970563  19.798990  19.798990   4.242641  42.426407\n77   2.828427 306.884343   4.242641  32.526912   7.071068   8.485281  55.154329\n78  12.727922 296.984848  14.142136  22.627417  16.970563   1.414214  45.254834\n79   7.071068 302.641702   8.485281  28.284271  11.313708   4.242641  50.911688\n80 292.742207  16.970563 294.156421 257.386868 296.984848 281.428499 234.759451\n           64         65         66         67         68         69         70\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51                                                                             \n52                                                                             \n53                                                                             \n54                                                                             \n55                                                                             \n56                                                                             \n57                                                                             \n58                                                                             \n59                                                                             \n60                                                                             \n61                                                                             \n62                                                                             \n63                                                                             \n64                                                                             \n65 168.291414                                                                  \n66 103.237590 271.529004                                                       \n67 124.450793 292.742207  21.213203                                            \n68  65.053824 233.345238  38.183766  59.396970                                 \n69  16.970563 185.261977  86.267027 107.480231  48.083261                      \n70  55.154329 223.445743  48.083261  69.296465   9.899495  38.183766           \n71 120.208153 288.499567  16.970563   4.242641  55.154329 103.237590  65.053824\n72  24.041631 144.249783 127.279221 148.492424  89.095454  41.012193  79.195959\n73  94.752309 263.043723   8.485281  29.698485  29.698485  77.781746  39.597980\n74 113.137085 281.428499   9.899495  11.313708  48.083261  96.166522  57.982756\n75 124.450793 292.742207  21.213203   0.000000  59.396970 107.480231  69.296465\n76 101.823376 270.114790   1.414214  22.627417  36.769553  84.852814  46.669048\n77 114.551299 282.842712  11.313708   9.899495  49.497475  97.580736  59.396970\n78 104.651804 272.943218   1.414214  19.798990  39.597980  87.681241  49.497475\n79 110.308658 278.600072   7.071068  14.142136  45.254834  93.338095  55.154329\n80 175.362482   7.071068 278.600072 299.813275 240.416306 192.333044 230.516811\n           71         72         73         74         75         76         77\n2                                                                              \n3                                                                              \n4                                                                              \n5                                                                              \n6                                                                              \n7                                                                              \n8                                                                              \n9                                                                              \n10                                                                             \n11                                                                             \n12                                                                             \n13                                                                             \n14                                                                             \n15                                                                             \n16                                                                             \n17                                                                             \n18                                                                             \n19                                                                             \n20                                                                             \n21                                                                             \n22                                                                             \n23                                                                             \n24                                                                             \n25                                                                             \n26                                                                             \n27                                                                             \n28                                                                             \n29                                                                             \n30                                                                             \n31                                                                             \n32                                                                             \n33                                                                             \n34                                                                             \n35                                                                             \n36                                                                             \n37                                                                             \n38                                                                             \n39                                                                             \n40                                                                             \n41                                                                             \n42                                                                             \n43                                                                             \n44                                                                             \n45                                                                             \n46                                                                             \n47                                                                             \n48                                                                             \n49                                                                             \n50                                                                             \n51                                                                             \n52                                                                             \n53                                                                             \n54                                                                             \n55                                                                             \n56                                                                             \n57                                                                             \n58                                                                             \n59                                                                             \n60                                                                             \n61                                                                             \n62                                                                             \n63                                                                             \n64                                                                             \n65                                                                             \n66                                                                             \n67                                                                             \n68                                                                             \n69                                                                             \n70                                                                             \n71                                                                             \n72 144.249783                                                                  \n73  25.455844 118.793939                                                       \n74   7.071068 137.178716  18.384776                                            \n75   4.242641 148.492424  29.698485  11.313708                                 \n76  18.384776 125.865007   7.071068  11.313708  22.627417                      \n77   5.656854 138.592929  19.798990   1.414214   9.899495  12.727922           \n78  15.556349 128.693434   9.899495   8.485281  19.798990   2.828427   9.899495\n79   9.899495 134.350288  15.556349   2.828427  14.142136   8.485281   4.242641\n80 295.570635 151.320851 270.114790 288.499567 299.813275 277.185858 289.913780\n           78         79\n2                       \n3                       \n4                       \n5                       \n6                       \n7                       \n8                       \n9                       \n10                      \n11                      \n12                      \n13                      \n14                      \n15                      \n16                      \n17                      \n18                      \n19                      \n20                      \n21                      \n22                      \n23                      \n24                      \n25                      \n26                      \n27                      \n28                      \n29                      \n30                      \n31                      \n32                      \n33                      \n34                      \n35                      \n36                      \n37                      \n38                      \n39                      \n40                      \n41                      \n42                      \n43                      \n44                      \n45                      \n46                      \n47                      \n48                      \n49                      \n50                      \n51                      \n52                      \n53                      \n54                      \n55                      \n56                      \n57                      \n58                      \n59                      \n60                      \n61                      \n62                      \n63                      \n64                      \n65                      \n66                      \n67                      \n68                      \n69                      \n70                      \n71                      \n72                      \n73                      \n74                      \n75                      \n76                      \n77                      \n78                      \n79   5.656854           \n80 280.014285 285.671140\n\n\n\n\n\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn a nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\nclass(nongeo_cluster)\n\n[1] \"hclust\"\n\n\n\n\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nmyanmar_ngeo_cluster &lt;- cbind(mmr_shp_mimu_2, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(myanmar_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we can perform spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(mmr_shp_mimu_2, mmr_shp_mimu_2)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.6 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.6)\n\nNext, cutree() is used to derive the cluster object.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with mmr_shp_mimu polygon feature data frame by using the code chunk below.\n\nMMR_sf_Gcluster &lt;- cbind(mmr_shp_mimu_2, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(MMR_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "R-ex/R-Ex8/R_Ex8.html#visual-interpretation-of-clusters",
    "href": "R-ex/R-Ex8/R_Ex8.html#visual-interpretation-of-clusters",
    "title": "Geospatial Analysis3 - Spatially Constrained Clustering: ClustGeo method",
    "section": "",
    "text": "Code chunk below is used to reveal the distribution of a clustering variable (i.e Battles_2022_Incidents) by cluster.\n\n#ggplot(data = myanmar_ngeo_cluster,\n#       aes(x = CLUSTER, y = Battles_2022_Incidents)) +\n#  geom_boxplot()"
  },
  {
    "objectID": "R-ex/R-Ex8/R_Ex8.html#references",
    "href": "R-ex/R-Ex8/R_Ex8.html#references",
    "title": "Geospatial Analysis3 - Spatially Constrained Clustering: ClustGeo method",
    "section": "",
    "text": "Main reference: Kam, T.S. (2024). Geographical Segmentation with Spatially Constrained Clustering Techniques"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html",
    "title": "Assignment 1 - Show me the numbers",
    "section": "",
    "text": "City of Engagement is a small city located at Country of Nowhere. The local council of the city is in the process of preparing the Local Plan 2024. A sample survey of representative residents had been conducted to collect data related to their household demographic and spending patterns. The city aims to use this data to assist with their major community revitalization efforts, including how to allocate a large city renewal grant to improve the happiness of their local residents."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#converting-timestamp-to-datetime",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#converting-timestamp-to-datetime",
    "title": "Assignment 1 - Show me the numbers",
    "section": "3.1 Converting timestamp to datetime",
    "text": "3.1 Converting timestamp to datetime\nThe timestamp column in the financial journal dataset was set with a varchar format.\n\n\n\nFig 3: Timestamp format of transactions\n\n\n\n\n\nFig 4: Data type of variables in Participants dataset\n\n\nWe convert the timestamp variable from varchar format to an appropriate datetime format.\n\n\n\nFig 5: Conversion of datatype from varchar to datetime format"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#converting-categorical-variables-to-correct-data-type",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#converting-categorical-variables-to-correct-data-type",
    "title": "Assignment 1 - Show me the numbers",
    "section": "3.2 Converting categorical variables to correct data type",
    "text": "3.2 Converting categorical variables to correct data type\nThe ‘participantId’ and ‘householdSize’ variable was set with a numeric double format. We convert ‘participantId’ to a varchar format to ensure that this variable could be used as a distinct identifier when merging both datasets.\nSimilarly, we convert ‘householdSize’ to varchar to enable categorical analysis in the data analysis stage, since there are only 3 distinct values in household size.\n\n\n\nFig 6: ‘participantId’ and ‘householdSize’ are set as numeric double format\n\n\n\n\n\nFig 7: Converting datatypes using the transform column function in SAS Data Studio\n\n\nUpon conversion, we remove the old columns and rename the converted columns.\n\n\n\n\nFig 8: Removing columns with incorrect datatype and renaming the corrected columns"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#remove-duplicates-from-the-financial-journal-dataset",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#remove-duplicates-from-the-financial-journal-dataset",
    "title": "Assignment 1 - Show me the numbers",
    "section": "3.3 Remove Duplicates from the Financial Journal dataset",
    "text": "3.3 Remove Duplicates from the Financial Journal dataset\nThere are 1,113 duplicate entries for 2 categories, namely education and shelter. These are monthly entries and seem to occur only in March 2022.\n\n\n\nFig 9: Duplicates in the Education transaction entries\n\n\n\n\n\nFig 10: Duplicates in the Shelter transaction entries\n\n\nWe remove these duplicates in SAS studio, by using the remove duplicate function.\n\n\n\nFig 11: Utilizing the remove duplicates function in SAS Data Studio-Prepare Data\n\n\nUpon removal, our number of rows has decreased to 1,855,217 from the original 1,856,330.\n\n\n\nFig 12: 1,113 duplicated records were successfully removed"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#data-quality-issue-standardising-and-consolidating-financial-journal-entries",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#data-quality-issue-standardising-and-consolidating-financial-journal-entries",
    "title": "Assignment 1 - Show me the numbers",
    "section": "3.4 Data quality issue: Standardising and consolidating financial journal entries",
    "text": "3.4 Data quality issue: Standardising and consolidating financial journal entries\nFinancial journal entries are not standardised. Some transaction categories are recorded as single monthly entries while others are recorded as multiple daily entries.\n\n\n\nFig 13: Single monthly entries for Education category\n\n\n\n\n\nFig 14: Multiple daily entries for Food category\n\n\nWe standardize these timestamps and consolidate the daily entries to single monthly entries. First, we extract and separate the date, time, month, and year from our timestamp variable.\nWe use SAS studio to extract the month and year via SQL code.\n\n\n\nFig 15: Using SAS studio and sql code to extract the month and year from timestamp\n\n\nNext, we do a group by selection by ‘participantID’, ‘category’, ‘year’ and ‘month’; and sum up the total monthly amounts for each category.\nThese series of actions will consolidate all timestamp entries into single monthly amounts, and sort it by participantID, category, year, and month.\n\n\n\nFig 16: using SQL code to group to the transaction entries by year and month, and summing up to derive consolidated monthly amounts\n\n\n\n\n\nFig 17: Output of the group by action\n\n\nAs a result, we have reduced the number of rows from 1,855,217 rows to 55,498 rows by consolidating the daily transactions to monthly transaction entries."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#grouping-participants-by-age-groups-binning",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#grouping-participants-by-age-groups-binning",
    "title": "Assignment 1 - Show me the numbers",
    "section": "3.5 Grouping Participants by Age Groups (Binning)",
    "text": "3.5 Grouping Participants by Age Groups (Binning)\nThe 1011 participants in the survey range from an age of 18 to 60. In order to increase our flexibility in the data analysis stage, we can group participants into distinct age groups. Through binning, we convert a numerical variable into a categorical one, which enable us to do additional analyses like association testing. This is done by using the recode ranges task in SAS studio.\n\n\n\nFig 18: Utilizing the recode ranges task in SAS studio to bin ’Age”.\n\n\nNew age ranges are defined as per Fig 19. These represent 20th percentiles in sequence.\n\n\n\nFig 19: Define each Age group by percentiles. Each increasing range is a sequential 20 percentile.\n\n\nSubsequently, we create a new column ‘Age_Groups’ for our participants dataset.\n\n\n\nFig 20: Adding the new age bins to our Participants dataset"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#integrating-data-by-merging-participants-and-financial-journal-data-sets",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#integrating-data-by-merging-participants-and-financial-journal-data-sets",
    "title": "Assignment 1 - Show me the numbers",
    "section": "3.6 Integrating data by merging Participants and Financial Journal data sets",
    "text": "3.6 Integrating data by merging Participants and Financial Journal data sets\nPreviously, we had consolidated the Financial Journal dataset and reduced it to 55,498 rows. This lists all monthly transactions per category for each participant on a row by row basis.\n\n\n\nFig 21: The consolidated financial journal with 55,498 rows of monthly transaction amounts\n\n\nHowever, the participant dataset is set in the format of listing variables on a column by column basis.\n\n\n\nFig 22: Participants dataset lists variables on a column by column basis\n\n\nWe proceed to sort and transpose our Financial Journal dataset to a similar column by column format. This is done by SAS studio, using SQL code.\n\n\n\nFig 23: Creating new columns to be used for our grouping process\n\n\n\n\n\nFig 24: New columns, ‘month_str’ and ‘cat_month’ are created\n\n\nCreating the new column ‘cat_month’, allows us to use it as a criterion to group all amounts of the same type, month, and year together. We next sort by participant ID.\n\n\n\nFig 25: SQL code to sum up transaction amounts upon grouping them together\n\n\nThis enables us to get an ordered list of monthly category amounts by participant ID as can be seen in Fig 26\n\n\n\nFig 26: Consolidating all monthly transactions by category and sorting it by Participant ID\n\n\nNext, we use the below SQL code to transpose the rows into columns and standardize for any missing fields\n\n\n\nFig 27: SQL code to transpose the rows into columns and standardize for empty fields\n\n\nWe generate a new table with 1011 rows, which matches our participants dataset.\n\n\n\nFig 28: Transposed financial journal table which lists variables on a column by column basis per participant\n\n\nNext, we use a Join method in SAS studio to combine both datasets using the common identifier, ‘participantID’.\n\n\n\nFig 29: Using the Join function in SAS Data studio\n\n\nWe now have a combined dataset which we can use in our data analysis stage.\n\n\n\nFig 30: Combined Participants and Financial Journal dataset"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-1-the-distribution-for-joviality-and-age-amongst-participants-is-not-normal",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-1-the-distribution-for-joviality-and-age-amongst-participants-is-not-normal",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.1 Observation 1 – The distribution for Joviality and Age amongst participants is not normal",
    "text": "4.1 Observation 1 – The distribution for Joviality and Age amongst participants is not normal\nFrom our IDEA analysis, we noted that Joviality scores and Ages for participants are not evenly distributed. We examine this by conducting a Distribution Analysis test.\n\n\n\n\nFig 31: Histograms showing the distribution of joviality and age across participants\n\n\nNormality Distribution test Hypotheses:\nH0: The distribution of Joviality scores and Ages resembles normal distribution.\nH1: The distribution of Joviality scores and Ages failed to resemble normal distribution.\n\n\n\nFig 32: QQ plots and summary of Goodness of Fit tests for Joviality and Age\n\n\nFrom our QQ plot and Goodness of fit tests, p-values for both Joviality and Ages are &lt; 0.05.\nHence, we reject the null hypothesis and infer that the distribution of Joviality and Age does not resemble normal distribution."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-2-the-joviality-for-participants-with-children-is-the-same-as-participants-without.",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-2-the-joviality-for-participants-with-children-is-the-same-as-participants-without.",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.2 Observation 2 – The Joviality for participants with children is the same as participants without.",
    "text": "4.2 Observation 2 – The Joviality for participants with children is the same as participants without.\nFrom our IDEA analysis, we noted that mean joviality for participants with children is somewhat higher than participants without. We examine this by conducting a two-sample test.\n\n\n\nFig 33: Boxplot comparing Joviality Vs “haveKids”\n\n\nTwo-sample mean test hypotheses:\nH0: The mean joviality is the same for participants with and without children\nH1: The mean joviality is not the same for participants with and without children\n\n\n\nFig 34: Summary of the normal distribution test\n\n\nBoth the Shapiro-Wilk and Anderson-Darling tests derived p-values of &lt; 0.05; ie distribution is not normal. Therefore, two-sample non-parametric test was used to compare the mean joviality across participants with and without children.\n\n\n\nFig 35: Results from the Wilcoxon test\n\n\nThe two-sample Wilcoxon test yielded a p-value &gt; 0.05. Hence, we have failed to reject the null hypothesis. We can conclude that the mean joviality is the same.\nConclusion:\nThere is no strong evidence to conclude that participants with children happier on average."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-3-there-is-a-relationship-between-education-level-and-having-children",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-3-there-is-a-relationship-between-education-level-and-having-children",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.3 Observation 3 – There is a relationship between education level and having children",
    "text": "4.3 Observation 3 – There is a relationship between education level and having children\nFrom our IDEA analysis, we noted a relationship between education level and having children. Particularly, we noted that the percentage of not having children is comparatively higher for participants with Bachelors and Graduate. We examine this further by conducting a Chi-square test of independence.\n\n\n\nFig 36: A normalized stacked bar-chart to observe the frequency of participants with children across education levels.\n\n\nChi-square test hypotheses:\nH0: Education level is independent of whether one has children (There is no association between education level and having children)\nH1: Education level is not independent of whether one has children (There is an association between education level and having children)\n\n\n\n\nFig 37: A mosaic plot and chi-square results for our association analysis (education level Vs having children)\n\n\nFrom the test results, the p-value is &lt; 0.05. Hence, we reject the null hypothesis and conclude that there is a statistically significant association between education level and having children.\nConclusion:\nA participant’s education level may have some bearing on the decision to have children."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-4-the-mean-joviality-for-participants-is-the-same-across-all-education-levels",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-4-the-mean-joviality-for-participants-is-the-same-across-all-education-levels",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.4 Observation 4 – The mean Joviality for participants is the same across all education levels",
    "text": "4.4 Observation 4 – The mean Joviality for participants is the same across all education levels\nFrom our IDEA analysis, we noted that the mean joviality for participants with an education level of ‘bachelors’ and ‘graduate’ is slightly higher than other participants.\nWe examine this by conducting a One-way ANOVA test to test for differences in the means of the 4 groups of education level.\n\n\n\nFig 38: Box plot showing the difference in mean joviality across education levels\n\n\nFirst, we run normality tests to check the distribution.\n\n\n\nFig 39: Summary of normality distribution test\n\n\nBoth the Shapiro-Wilk and Anderson-Darling tests derived p-values &lt; 0.05; ie the distribution for all 4 classes is not normal. Hence, we will proceed with the non-parametric one-way ANOVA test.\nNon-parametric One-way ANOVA test hypotheses:\nH0: There is no difference in the mean joviality across different education levels.\nH1: There is a difference in the mean joviality across different education levels.\n\n\n\nFig 40: Summary of results from the Kruskal-Wallis test\n\n\nThe p-value from the Kruskal-Wallis test is 0.7018, which is &gt; 0.05. This means that we do not have sufficient evidence to reject the null hypothesis.\nConclusion:\nEducation level does not significantly impact one’s happiness level."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-5-there-is-a-relationship-between-education-level-and-mean-wages",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-5-there-is-a-relationship-between-education-level-and-mean-wages",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.5 Observation 5 – There is a relationship between Education Level and Mean Wages",
    "text": "4.5 Observation 5 – There is a relationship between Education Level and Mean Wages\nFrom our IDEA analysis, we noted a relationship in the mean wages for participants in different education levels, where participants with higher education levels have higher mean wages. We examine this by conducting a One-way ANOVA test to test for differences in the mean wages of the 4 different education levels.\n\n\n\nFig 41: Average wages seem higher the higher the education level\n\n\nFirst, we run normality tests to check the distribution.\n\n\n\nFig 42: Summary of normal distribution test for wage vs education level\n\n\nBoth the Shapiro-Wilk and Anderson-Darling tests derived p-values &lt; 0.05; ie the distribution for all 4 classes in education level is not normal. Therefore, we use the non-parametric one-way ANOVA test to compare the means.\nNon-parametric One-way ANOVA test hypotheses:\nH0: There is no difference in the mean wages across different education levels.\nH1: There is a difference in the mean wages for at least one pair of education levels.\n\n\n\nFig 43: Summary of the Kruskal-Wallis test for wages Vs Education level\n\n\nGiven that the p-value (&lt;.0001) is much smaller than the common significance threshold of 0.05, we have evidence to reject the null hypothesis.\nConclusion:\nA higher Education level is related to a higher mean wage."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-6---joviality-is-moderately-correlated-with-wages-and-recreation-spend.",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-6---joviality-is-moderately-correlated-with-wages-and-recreation-spend.",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.6 Observation 6 - Joviality is moderately correlated with Wages and Recreation spend.",
    "text": "4.6 Observation 6 - Joviality is moderately correlated with Wages and Recreation spend.\nFrom our IDEA analysis, we noted that Joviality is moderately correlated with Wages and Recreation spend. For this analysis, only financial data from March 2022 was considered, as Joviality scores were obtained at the start of the survey period. We examined the correlation between Joviality and March 2022 transaction categories (Food, Recreation, Education, Shelter, Rent Adjustment and Wages).\n\nFig 44: Scatter plots of Joviality Vs Education, Food, Recreation, Rent Adjustment, Shelter and Wages\nCorrelation test hypotheses:\nH0: There is no correlation between the variables\nH1: There is a correlation between the variables.\n\n\n\nFig 45: Pearson Coefficient scores for Joviality Vs transaction categories\n\n\nP-values for Food, Recreation, Wages and Rent Adjustment are &lt; 0.05. Hence, we can reject the null hypothesis and conclude that there is some correlation between Joviality and these transaction categories.\nSpecifically, there is a moderate positive linear relationship with Recreation spend (Pearson coefficient 0.345) and a moderate negative linear relationship with Wages (Pearson coefficient 0.318).\n\nConclusion:\nThe negative linear relationship with wages suggests that earning higher wages does not make one happier.\nThis observation links back to our observations in 4 & 5 , where earning higher wages by being more educated does not seem to have much impact on one’s happiness."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-7-there-is-no-difference-in-joviality-for-participants-in-different-age-groups",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-7-there-is-no-difference-in-joviality-for-participants-in-different-age-groups",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.7 Observation 7 – There is no difference in Joviality for participants in different Age groups",
    "text": "4.7 Observation 7 – There is no difference in Joviality for participants in different Age groups\nFrom our IDEA analysis, we noted a difference in the mean Joviality for different Age groups. We examined this by conducting a One-way ANOVA test to test for differences in the mean joviality for the 5 different age groups.\n\n\n\nFig 46: Box plot showing the mean joviality across Age groups\n\n\nFirst, we run normality tests to check the distribution.\n\n\n\nFig 47: Summary of normal distribution tests for joviality Vs Age groups\n\n\nBoth the Shapiro-Wilk and Anderson-Darling tests derived p-values &lt; 0.05; ie the distribution for all 4 classes in age groups is not normal. Therefore, we proceed with a non-parametric one-way ANOVA test to compare the means.\nNon-parametric One-way ANOVA test hypotheses:\nH0: There is no difference in the mean joviality across age groups\nH1: There is a difference in the mean joviality across age groups.\n\n\n\nFig 48: Summary of the Kruskal-Wallis test for Joviality Vs Age groups\n\n\nThe p-value from the Kruskal-Wallis test is 0.1852, which is &gt; 0.05. This means that we do not have sufficient evidence to reject the null hypothesis\nConclusion:\nThere is no strong evidence that different age groups are happier than others."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-8-there-is-no-difference-in-joviality-for-different-household-sizes.",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-8-there-is-no-difference-in-joviality-for-different-household-sizes.",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.8 Observation 8 – There is no difference in Joviality for different household sizes.",
    "text": "4.8 Observation 8 – There is no difference in Joviality for different household sizes.\nFrom our IDEA analysis, we noted that the mean Joviality for participants with a household size of 3 is somewhat higher than other participants. We examine this with a One-way ANOVA test to test for differences in the means of the 3 classes of household sizes.\n\n\n\nFig 49: Box plot showing the difference in mean joviality across household sizes\n\n\nFirst, we run normality tests to check the distribution.\n\n\n\nFig 50: Summary of Normal distribution test for joviality Vs Household sizes\n\n\nBoth the Shapiro-Wilk and Anderson-Darling tests derived p-values &lt; 0.05; ie the distribution for all 3 classes in Household size is not normal. Therefore, we use non-parametric one-way ANOVA test to compare the means.   \nNon-parametric One-way ANOVA test hypotheses:\nH0: There is no difference in the mean joviality across different household sizes\nH1: There is a difference in the mean joviality across different household sizes.\n\n\n\nFig 51: Summary of the Kruskal-Wallis test for Joviality Vs Household sizes\n\n\nThe p-value from the Kruskal-Wallis test is 0.6854, which is &gt; 0.05. This means that we do not have sufficient evidence to reject the null hypothesis\nConclusion:\nThere is no strong evidence that different household sizes are happier than others"
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-9-participants-financial-health-does-not-improve-over-time",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-9-participants-financial-health-does-not-improve-over-time",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.9 Observation 9 – Participants financial health does not improve over time",
    "text": "4.9 Observation 9 – Participants financial health does not improve over time\nFrom our IDEA analysis, we noted that monthly wages do not increase over time, and instead shows a decreasing trend.\n\n\n\nFig 52: Line chart showing the total monthly wages from March 2022 to May 2022\n\n\nAs can be seen from Fig 53, all participants in all household types, spend the most from their wages on shelter.\n\n\n\nFig 53: Breakdown of expenditure for participants across all household sizes\n\n\n\n\n\nFig 54: Comparison of total expenditure over different expense categories, over time period.\n\n\nFrom Fig 54, we can observe that Shelter occupies the top expenditure bracket and is constant throughout the time period.\n\n\n\nFig 55: Expenditure on recreation and food show a decreasing trend over time\n\n\nTherefore, from the constant expenditure on shelter and decreasing wages over time, we can infer that participants are compensating for this by spending lesser on recreation and food. Educational spend does not impact our conclusion due to its low relative cost and constant or fixed pricing. Furthermore, educational spend is exclusive to participants with children, who make up 1/3 of the sample size.\nConclusion:\nParticipants do not get wealthier over time and have to compensate by cutting back on recreation and food as they prioritize Shelter over other items."
  },
  {
    "objectID": "Write-ex/Write-Ex1/Write_Ex1.html#observation-10---wages-is-moderately-correlated-with-food-and-shelter-spend.",
    "href": "Write-ex/Write-Ex1/Write_Ex1.html#observation-10---wages-is-moderately-correlated-with-food-and-shelter-spend.",
    "title": "Assignment 1 - Show me the numbers",
    "section": "4.10 Observation 10 - Wages is moderately correlated with Food and Shelter spend.",
    "text": "4.10 Observation 10 - Wages is moderately correlated with Food and Shelter spend.\nFrom our IDEA analysis, we noted that wage is moderately correlated with Food and Shelter spend. We proceed to examine the correlation between Wages and Expense categories (Food, Recreation, Education, Shelter).\n\n\n\nFig 56: Scatter plots of Wage Vs Food, Education, Recreation and Shelter\n\n\nCorrelation test hypotheses:\nH0: There is no correlation between the variables\nH1: There is a correlation between the variables.\n\n\n\nFig 57: Pearson Coefficient scores for Wage Vs Expense categories. Correlations are indicated as negative as expense amounts in the dataset is recorded as negative amounts.\n\n\nThe p values for Education, Food, Recreation and Shelter are below &lt;0.05. Hence, we can reject the null hypothesis and conclude that there is some correlation between wages and these expense categories.\nSpecifically, we noted a moderate positive linear relationship with Food spend (Pearson coefficient 0.3609) and a moderate positive linear relationship with Shelter (Pearson coefficient 0.4176).\nConclusion:\nThe positive linear relationship with Food and Shelter suggests that participants prioritize these items ahead of other categories like recreation.\nThis observation links back to our observations in financial analysis , where recreation spend exhibits a decreasing trend in the time period."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "",
    "text": "Accurate predictions of loan defaults is crucial for financial institutions to mitigate risks and make informed lending decisions. This paper presents a comprehensive analysis of consumer loan data, utilizing predictive and machine learning models to forecast probabilities of loan defaults.\nThis study was conducted using data from SuperLender, a digital lending company. With limited granular customer data, feature extraction and selection played a significant role in identifying the most predictive variables for model training.\nVarious machine learning techniques, including logistic regression, decision trees, random forest, gradient boost, and neural networks, were employed to analyze these variables and their impact on loan repayments. Ensemble models were also explored to examine improvements over standalone models. We also explored different Data partition ratios to assess their impact on model evaluation, selection, and accuracy.\nDespite the data limitations, the ML models were able to achieve accuracies of over 80%, demonstrating their effectiveness in predicting consumer loan defaults. Key variables influencing loan defaults were identified, such as time taken to repay loans, delays in repaying loans and instances of past loan repayment failure.\nThis research contributes to financial risk management by providing a practical framework for implementing machine learning in loan prediction. It offers banks and financial institutions a more sophisticated toolset to manage credit risk effectively and make informed lending decisions."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#background",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#background",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "2.1 Background",
    "text": "2.1 Background\nBank loans play a fundamental role in the financial system, enabling individuals and businesses to pursue their financial goals and driving economic growth (International Monetary Fund,2017). However, loans also pose a significant credit risk for financial institutions, making accurate loan default prediction crucial for mitigating risks, and making informed lending decisions.\nTraditional loan approval processes rely heavily on manual underwriting, which is derived from credit scorecards and analysis on past credit history. However, this process can be subjective, prone to human error and impervious to future trends. This has led to a growing demand for more automated and accurate methods of loan prediction (Aslam et al., 2019). Machine learning (ML) has emerged as a promising approach to address this challenge, offering the ability to analyze large datasets and identify complex patterns that may not be readily apparent to human analysts (X. Zhu et al., 2023).\nThe emergence of digital lending platforms like SuperLender has exacerbated the need for more accurate modelling techniques, introducing new variables and credit history patterns into the risk assessment equation. This study is contextualized within this evolving landscape, proposing a shift towards more data-driven, machine learning-based approaches for predicting loan defaults."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#problem-statement",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#problem-statement",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "2.2 Problem Statement",
    "text": "2.2 Problem Statement\nDespite the proliferation of data in the lending sector, there is a conspicuous absence of effective predictive models that can accurately forecast loan defaults. This gap not only impedes the risk management capabilities of financial institutions but also limits their ability to extend credit to viable borrowers.\n\nWhile ML has shown considerable promise in loan prediction, there remains a gap between the potential of ML and its practical application in the financial industry. One of the key challenges is the limited availability of rich and granular customer data, which often restricts the effectiveness of ML models. Additionally, there is a need for more research on the optimal combination of ML techniques and the development of robust models that can generalize well to new types of data."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#project-objectives",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#project-objectives",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "2.3 Project Objectives",
    "text": "2.3 Project Objectives\nThis study aims to bridge the gap between traditional loan prediction methods and the potential of ML by developing and evaluating ML models for predicting consumer loan defaults.\nThe specific objectives of this study are:\n\nTo identify the key factors that could influence consumer loan defaults.\nTo develop and evaluate various ML models for predicting consumer loan defaults.\nTo compare the performance of different ML models and identify the most effective model for predicting loan defaults."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#predictive-modelling-process",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#predictive-modelling-process",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.1 Predictive Modelling Process",
    "text": "4.1 Predictive Modelling Process\nThe Predictive Modelling process in this study involved Data exploration and preparation, Variable creation and selection, Data Sampling, Model fitting and evaluation and Final model selection. All of these were performed using SAS Viya, alongside various SAS modules as depicted in Figure 1. Several iterations in terms of variable selection and data sampling and partitioning ratios were also explored, to observe any improvements in the performance of the models.\n\n\n\nFigure 1: Predictive Modelling Process"
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#data-set",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#data-set",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.2 Data Set",
    "text": "4.2 Data Set\nThe dataset used in this study was obtained from the Loan Default Prediction Challenge Early (Zindi, 2023).\nThere are three sets of data:\n\nDemographics: This contains description and details about loan applicants. Attributes such as birth dates, education level, employment type and account types can be found here.\nPerformance: This contains details of the latest loan applied by each applicant and a rating (good-bad-flag) which describes if an applicant is a good or bad debtor.\nPrevious Loans: This contains information on the previous loans disbursed, as well as the history of repayment.\n\n\n\n\nTable 1: Overview of Datasets\n\n\nWe observed that the variables provided in the Demographics dataset: ‘birthdate’, ‘bank account type’, ‘longitude’, ‘latitude’, ‘bank name’, ‘bank branch’, ‘employment status’ and ‘education’ were somewhat generic with sizeable missing values.\nOn the other hand, the previous loans dataset was extensively populated with previous loan applications as well as the history of payments. This lack of granularity in demographic variables led us to examine in more detail the relationships between past loan applications and payment histories to loan default probabilities."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#data-cleaning-preparation",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#data-cleaning-preparation",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.3 Data Cleaning & Preparation",
    "text": "4.3 Data Cleaning & Preparation\nThe Demographics and Performance datasets were grouped by Customer Identifiers while Previous loans were grouped by Loan Identifiers. Upon examining the datasets, we noted that there were 1099 customers which could not be found in the Demographics dataset.\nWe first merged the Performance and Demographics datasets to ensure that we only include unique customers found in both datasets. This enabled us to draw from existing variables present in both the Demographics and Loan performance datasets for our prediction models. This merged dataset derived 3269 unique customers.\nWe then merged this with the Previous Loans dataset, upon first completing a Group-by Customer Identifier operation. This operation enabled us to change the structure of the Previous loans dataset to be aligned with the other two datasets. After identifying and removing duplicates, our final merged dataset consisted of 3264 unique customers with variables drawn from all three original datasets. As highlighted in Figure 1, the process of deriving the final dataset went through several iterations, as we identified and experimented on various predictive variables."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#variable-creation-and-selection",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#variable-creation-and-selection",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.4 Variable creation and selection",
    "text": "4.4 Variable creation and selection\nAdditionally, we also created new predictor variables, in line with the influential factors that we identified and mentioned in our literature review.\nThe following summarizes the Variable creation and selection/filtering process:\n\nBank Branch was dropped as there were more than 80% missing values.\nLatitude and Longitude variables were dropped as only 25 customers were found to be residing outside of Africa. \nEducation level: 86.4% of this data is missing. However, since education was identified to be an influential factor in predicting loan defaults, we recoded this variable to ‘High Education’, ‘Low Education’ and ‘No Education’ classes.\nEmployment: Since there were 648 records missing, we recoded this variable into ‘Permanent’ and ‘Not Permanent’ classes.\nReferred by: We recoded this to ‘Yes’ and ‘No’ classes.\nCreated new ‘Recency’ variables to measure the relationship of time between loans, difference in time between the last loan in the previous loan’s dataset and the newest loan in the performance dataset.\nCreated new ‘Frequency’ variables to measure the number of loans, the minimum, maximum, average of number of loans.\nCreated new ‘Monetary’ variables to measure loan amounts, minimum, maximum, average and variations between previous loan amounts across the credit history period.\nCreated new ‘Comparison’ variables to compare a customer’s previous loan behaviour against the newest loan in performance dataset. Comparisons were also made in terms of differences in amount, term days, interest costs, interests and time or gap in between loans.\n\nIn total, 57 new predictive variables were created or recoded from data drawn from the original datasets."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#data-partitioning",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#data-partitioning",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.5 Data Partitioning",
    "text": "4.5 Data Partitioning\nOur consolidated final dataset was split into Training, Validation, and Test partitions in 4 combinations like below:\n\n50:30:20\n60:20:20\n60:30:10\n40:30:30\n\nThis is in line with the research mentioned in our literature review where Test partitions of 20% and 30% were observed to produce better results over other ratios. Furthermore, we wanted to observe if the choice of partition ratios could influence model selection."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#model-fitting-and-evaluation",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#model-fitting-and-evaluation",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.6 Model Fitting and Evaluation",
    "text": "4.6 Model Fitting and Evaluation\nThe following predictive models were applied to our training dataset upon selecting the Auto-tuning template in SAS Model Studio (Figure 2):\n\nNeural network\nStepwise Logistic regression\nForward Logistic regression\nGradient Boosting\nForest\nDecision Tree\nEnsemble\n\n\n\n\nFigure 2: Model Studio selects 7 models when an Auto-tune template was selected.\n\n\nAs part of our model building process, for models that are sensitive to highly correlated variables (Log regression and Neural networks), SAS Model Studio first analyses our predictive variables and rejects similar variables of high variance through the Variable Selection process. This represents an improvement over manual checks of collinearity as SAS Model Studio can analyze paired and grouped variances at the same time."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#analysis-and-results",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#analysis-and-results",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "4.7 Analysis and Results",
    "text": "4.7 Analysis and Results\nThe F-1 score metric is used to evaluate models’ accuracy in predicting loan defaults probability.\nIn SAS Model Studio, we ran four separate model projects based on an auto-tuning template with default settings on 4 different Train-Validate-Test partition ratios.\nOur champion model was the Ensemble model with a partition ratio of 60:30:10 (Table 2) and an F-1 score of 0.8972 on test data.\n\n\n\nTable 2: Summary of the F-1 scores for our 4 projects across 4 data sampling ratios on Test data\n\n\n\n\n\nFigure 3: Our Champion model was partitioned in a 60:30:10 ratio.\n\n\nWe further break down and compare the F1 and Accuracy scores of other models in the same project as our Champion model to examine the relative differences (Figure 4)\n\n\n\nFigure 4: Comparison of models for our Champion model’s project segment\n\n\nThe Ensemble model, which combines multiple algorithms, obtained the highest F1 score of 0.8972 and an Accuracy score of 82.21% (Figure 4). This superior performance underscores the ensemble’s robustness, likely due to its ability to integrate diverse predictive signals.\n\n\n\nFigure 5: Graphs depicting the F-1 and Accuracy scores for our Champion model, Ensemble model.\n\n\nF1 score combines the measures of precision and recall (or sensitivity), which are measures of classification based on the confusion matrix that are calculated at various cutoff values. It is calculated as 2*Precision*Recall / (Precision + Recall), which is the harmonic mean of Precision and Recall. A larger F1 score indicates a more accurate model.\nAccuracy is the proportion of observations that are correctly classified as either an event or non-event, calculated at various cutoff values. It is calculated as (true positives + true negatives) / (total observations).\n\n\n\nFigure 6: ROC Curve for our Champion Ensemble Model\n\n\nThe Receiver Operating Characteristic (ROC) curve is a plot of sensitivity (the true positive rate) against 1-specificity (the false positive rate), which are both measures of classification based on the confusion matrix. From Figure 6, we can observe that our Ensemble model is able to achieve a high Area under the Curve (AUC) for all our partitions Train (0.7501), Validate (0.6999) and Test (0.7423). (See Appendix for Fit statistics).\nThe diagonal line indicates a random model, where sensitivity = 1-specificity. An ROC curve that plots above this diagonal indicates a better-than-random ability to distinguish between the positive and negative classes."
  },
  {
    "objectID": "Write-ex/Write-Ex3/Write_Ex3.html#fit-statistics-for-the-champion-mode----ensemble",
    "href": "Write-ex/Write-Ex3/Write_Ex3.html#fit-statistics-for-the-champion-mode----ensemble",
    "title": "Comparison of Predictive and Machine Learning models for Loan Default prediction",
    "section": "Fit statistics for the Champion mode- - Ensemble",
    "text": "Fit statistics for the Champion mode- - Ensemble"
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "The Bitcoin blockchain can process only seven (7) transactions per second. Its low throughput impedes its adoption as a payment system. Numerous solutions were implemented to address Bitcoin’s scalability issue. In this question, you are required to study and evaluate two (2) Bitcoin scalability proposals.\n\n\nExamine one (1) layer 1 and one (1) layer 2 scalability solution for Bitcoin.\n\n\n\nYour company decides to use Bitcoin as a payment method. Choose either the adoption of a layer 1 or layer 2 solution. Justify your recommendation.\n\n\n\nLayer 1 and layer 2 scalability solutions are different in their role and focus on the blockchain.  Layer 1 solutions aim to change and improve the existing blockchain architecture, and include proposals like increasing the block size, reducing transaction size, changing the transaction format, changing the consensus algorithm and sharding.\nLayer 2 solutions instead, looks to increase the transaction throughput by alleviating the computational load, transferring some transactions offline and rely on secondary networks or technology that operate on top of the underlying blockchain. These include the use of rollups, sidechains, state channels and nested blockchains.\n\n\n\nSegregated Witness is a proposed change in the format or structure of transactions that allows for digital signatures to be removed to a separate data structure and not form part of transactions included in the block.\nA block is limited to 1Mb in size, with the majority of space taken up by transactions. The main components in a transaction are inputs (where coins are coming from) and outputs (where coins are going to). Most of the space in transactions is taken up by a digital signature located in the inputs. This accounts for around 65% of the space within a single transaction and acts as proof that the sender controls the wallet that has the required funds to make the payment (Academy, 2021).\nThe Segregated witness solution re-locates this signature from the input to its own structure towards the end of the transaction, known as the ‘witness’ section. This allows for an ‘increase’ in the block size limit, where more transactions can effectively be inserted into the blocks. As shown in Fig1, the removal of signatures free up block space and enables the insertion of more transactions into the block.\n\n\n\nFig 1: Comparing Non-SegWit and SegWit blocks , Source: Cointelegraph Analytics (Kuznetsov, 2019)\n\n\n\n\n\nThe lightning network is a layer 2 solution where most of the transactions between two counterparties are moved off-chain. Removing small and frequent payment transactions off chain can help achieve a dramatic increase in both transaction speed and throughput, making the Bitcoin network more scalable.\nIn the Bitcoin network, every single transaction has to be broadcast to every node. Broadcasted transactions also have to be included in blocks that are mined and recorded on the blockchain. This effective requires every transaction to be broadcasted twice to all nodes in the network. On average, each block is created every 10 minutes and has a size limit of 1 Mb. This places limits on the number of transactions that can be processed as the network becomes more heavily utilized.\nPayment channels in the Lightning network are examples of state channels, which represent an off-chain state alteration mechanism which is secured by eventual settlement on the blockchain. State channel solutions aim to bring ‘unimportant’ transactions offline or off-chain so as not to flood the main network. These off-chain transactions are not subject to transaction fees and can occur at high frequencies.\nPayment channels are formed on top of multi-signature addresses which are created from the public keys of the two transacting parties, thereby putting into effect a requirement for a signature quorum. For example, in a 2-of-2 multi-signature address, the bitcoin can only be unlocked and spent if both participants sign to spend the funds.\nWhen opening a new payment channel, two parties; Alice, or Bob or both commit initial funds (bitcoins) to this multi-signature address by sending a transaction, called a funding transaction to the Bitcoin network. The committed bitcoins represent the channel capacity, which is the maximum amount of bitcoin that can be transferred in the payment channel. The funding transaction includes a ‘refund’ transaction that protects the sender of the initial funds should the counterparty withhold his/her signature and deny the sender access to his/her funds. This ‘refund’ transaction represents the first commitment transaction in the payment lifecycle.\nAlice and Bob can transfer bitcoins to each other multiple times by creating new commitment transactions and exchanging signatures each time. Each new commitment transaction encodes the latest balance between Alice and Bob, altering the initial state but not broadcasting it to the Bitcoin network. This enables Alice and Bob to transfer an unlimited number of payments to each other without having to interact with the underlying Bitcoin network.\nA cheating counterparty that posts an older commitment transaction with a higher balance amount and closes the payment channel without consent will be encumbered by additional transactions that are created during force closure. The Delivery transaction, Revocable delivery transaction and Branch Remedy transactions function to enable cheated parties to claim their funds immediately, impose a time lock on cheating parties before they can use funds and punishes the cheater by confiscating his/her balances. These fail-safe measures and penalty mechanism disincentives cheating parties and reduces the need for trust amongst counterparties.\nIn terms of transaction fees, Alice and Bob only need to pay transactions fees twice (when the payment channel is opened and closed). No transaction fees are charged for transactions in between as these are off-chain and not broadcasted to the network.\nWhen Alice and Bob conclude their payments and decide close their payment channel, they simply sign a closing commitment transaction which will then be broadcast to the main network. In the lifetime of a payment channel, there are only 2 transactions that has to be broadcasted and verified in the Bitcoin network.\nUsers also do not need to open multiple individual payment channels with every party they wish to transact with. The lightning network enables for the routing of transactions by connecting multiple payment channels, creating a path from the payment source to payment destination, and allowing for transactions to be forwarded to any of the network participants. If Alice has an existing payment channel with Bob, and if Bob has another payment channel with Charlie; then Alice can route her payments to Charlie via Bob who acts as an intermediary.\nTransactions can be sent through the network of payment channels without the need to trust any of the intermediaries. A node constructs a path by connecting with channels with enough capacity using the possible shortest routes it can find. The communication between nodes is encrypted point to point. Intermediary nodes can verify and decrypt their own portion of the transaction message but cannot identify who the sender and receiver of the payment is, or their own position in the path (Mastering the Lightning Network – Aantonop, n.d.).\n\n\n\nA company using bitcoin as a payment method for its business activities, would have the following considerations and requirements:\n1) Fast and secure payments\n2) Finality of transactions\n3) Privacy of transaction data\n4) Privacy of company identity\n5) High transaction throughput\n6) Low transaction fees\n7) Efficient and frictionless cross border payments\n8) Secure from centralized points of failure and attack\n9) Available round the clock\nExisting centralized payment systems can accommodate a high transaction throughput but have inherent risks like counterparty risk, system failures, cyber-attacks and denial of service attacks and high transaction fees. Further, to participate in centralized payments systems you would need to have accounts with specific financial institutions or payment entities where terms of service are usually always in their favour. We need to trust that banks and payment companies act in good faith, in our best interests and do not change the rules to benefit and profit from rent-seeking motives.\nOn the other hand, the Bitcoin network, with its decentralised nature and cryptographic proof of work consensus, makes transactions more secure, transparent, and private but may not be able to support a large volume of transactions.\nThe Bitcoin network is slow compared to centralized payment systems because of consensus but is also more secure. Transactions are append-only and immutable, which makes it less vulnerable to manipulation and forgery.\nWhen considering between scalability, security, and decentralization; a business should lean towards security and decentralization. Security is important as transactions are more often high value and crucial to the business operations. With decentralization, there is less dependency on central parties and intermediaries which could potentially be points of failure. As much as a high capacity of transactions is useful for a business, it would not make sense if the payments were insecure or vulnerable to points of failure. To this point, the real question would be how to achieve a higher transaction throughput through layer 1 or layer 2 solutions.\nI would argue that layer 1, in its existing form, despite being ‘slow’ and ‘inefficient’ still functions as a secure environment for high value transfers.  \nLayer 1 solutions aim to change and improve the underlying blockchain architecture, and include solutions like increasing the block size, changing the consensus algorithm and Sharding.  However, each comes with its undesirable consequence.\nFor example, increasing the block size could help increase the transaction throughput by enabling more transaction to be included in each block. However, with larger block sizes, more work needs to be done by miners and validators during proof of work. This could technically increase the computational load required to perform validation and mining, enabling only larger entities and groups to be economically capable of performing the ‘work’. This can lead to centralization and collusion.\nChanging the consensus protocol from proof of work to proof of stake, may potentially make achieving consensus faster as miners no longer have to compete with each other and just be randomly selected to mine the blocks. However, this just simply means that participants with higher stakes gain a higher change and probability of being selected. Similarly, this can lead to centralization and incentivise more collusion.\nSharding refers to the splitting up of miners into separate and independent groups, where each group validate separate sets of transactions concurrently. However inter-shard communication can be costly and inefficient.\nFor these reasons, a layer 2 solutions like state channels would be appropriate to help businesses use the Bitcoin network more efficiently. Specifically, using the Lightning network can enable businesses to move frequent smaller value transactions off-chain, thereby reserving high value transactions only for the main chain.\nThe company can open and maintain payment channels with counterparties and transfer unlimited frequent payments to them as and when required. Security measures are in place with multi-sig addresses requiring both parties to agree and sign off on transactions before they are completed. Further, should one party cheat and abruptly posts an older transaction and close the payment channel, the affected party can 1) post Delivery transactions to recover their funds immediately and 2) post Branch remedy transactions to confiscate the cheater’s balances. This would sufficient disincentive any cheating attempt.\nThe company would not need to open multiple individual payment channels with every counterparty. The lightning network enables for the routing of transactions by connecting multiple payment channels, allowing for transactions to be forwarded to any network participants. The communication between nodes is encrypted point to point, where forwarding nodes can only decrypt their own portion of the route and cannot identify who the sender and receiver of the payment is, or their own position in the path. This enables the company to maintain privacy of identity and transaction information.\nIn terms of transaction fees, the company would only pay transactions fees twice ie when the payment channel is opened and closed. No transaction fees are charged for transactions in between as these are off-chain and not broadcasted to the network.\nBy utilizing solution like the Lightning network, the company have more transactions done faster and with less cost. Further, as these off-chain transactions do not need to be broadcasted to the main-chain, the full extent of the transaction history between counterparties do not have to be revealed to the public blockchain. Finally, with the fail-safe and penalty mechanisms in place, the company can safely transact with multiple counterparties without fear of cheating (Mastering the Lightning Network – Aantonop, n.d.).\n\n\n\nAcademy, B. (2021, April 29). A Beginner’s Guide to Segregated Witness (SegWit). Binance Academy. https://academy.binance.com/en/articles/a-beginners-guide-to-segretated-witness-segwit\nKuznetsov, N. (2019, September 28). SegWit, Explained. Cointelegraph. https://cointelegraph.com/explained/segwit-explained\nMastering the Lightning Network – aantonop. (n.d.). https://aantonop.com/books/mastering-the-lightning-network/\nAcademy, B. (2022, September 29). Blockchain Layer 1 vs. Layer 2 Scaling Solutions. Binance Academy. https://academy.binance.com/en/articles/blockchain-layer-1-vs-layer-2-scaling-solutions\nAcademy, B. (2022, September 29). Blockchain Layer 1 vs. Layer 2 Scaling Solutions. Binance Academy. https://academy.binance.com/en/articles/blockchain-layer-1-vs-layer-2-scaling-solutions"
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html#introduction---layer-1-layer-2-solution",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html#introduction---layer-1-layer-2-solution",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "Layer 1 and layer 2 scalability solutions are different in their role and focus on the blockchain.  Layer 1 solutions aim to change and improve the existing blockchain architecture, and include proposals like increasing the block size, reducing transaction size, changing the transaction format, changing the consensus algorithm and sharding.\nLayer 2 solutions instead, looks to increase the transaction throughput by alleviating the computational load, transferring some transactions offline and rely on secondary networks or technology that operate on top of the underlying blockchain. These include the use of rollups, sidechains, state channels and nested blockchains."
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html#layer-1-solution---segregated-witness-segwit",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html#layer-1-solution---segregated-witness-segwit",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "Segregated Witness is a proposed change in the format or structure of transactions that allows for digital signatures to be removed to a separate data structure and not form part of transactions included in the block.\nA block is limited to 1Mb in size, with the majority of space taken up by transactions. The main components in a transaction are inputs (where coins are coming from) and outputs (where coins are going to). Most of the space in transactions is taken up by a digital signature located in the inputs. This accounts for around 65% of the space within a single transaction and acts as proof that the sender controls the wallet that has the required funds to make the payment (Academy, 2021).\nThe Segregated witness solution re-locates this signature from the input to its own structure towards the end of the transaction, known as the ‘witness’ section. This allows for an ‘increase’ in the block size limit, where more transactions can effectively be inserted into the blocks. As shown in Fig1, the removal of signatures free up block space and enables the insertion of more transactions into the block.\n\n\n\nFig 1: Comparing Non-SegWit and SegWit blocks , Source: Cointelegraph Analytics (Kuznetsov, 2019)"
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html#layer-2-solution---the-lightning-network",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html#layer-2-solution---the-lightning-network",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "The lightning network is a layer 2 solution where most of the transactions between two counterparties are moved off-chain. Removing small and frequent payment transactions off chain can help achieve a dramatic increase in both transaction speed and throughput, making the Bitcoin network more scalable.\nIn the Bitcoin network, every single transaction has to be broadcast to every node. Broadcasted transactions also have to be included in blocks that are mined and recorded on the blockchain. This effective requires every transaction to be broadcasted twice to all nodes in the network. On average, each block is created every 10 minutes and has a size limit of 1 Mb. This places limits on the number of transactions that can be processed as the network becomes more heavily utilized.\nPayment channels in the Lightning network are examples of state channels, which represent an off-chain state alteration mechanism which is secured by eventual settlement on the blockchain. State channel solutions aim to bring ‘unimportant’ transactions offline or off-chain so as not to flood the main network. These off-chain transactions are not subject to transaction fees and can occur at high frequencies.\nPayment channels are formed on top of multi-signature addresses which are created from the public keys of the two transacting parties, thereby putting into effect a requirement for a signature quorum. For example, in a 2-of-2 multi-signature address, the bitcoin can only be unlocked and spent if both participants sign to spend the funds.\nWhen opening a new payment channel, two parties; Alice, or Bob or both commit initial funds (bitcoins) to this multi-signature address by sending a transaction, called a funding transaction to the Bitcoin network. The committed bitcoins represent the channel capacity, which is the maximum amount of bitcoin that can be transferred in the payment channel. The funding transaction includes a ‘refund’ transaction that protects the sender of the initial funds should the counterparty withhold his/her signature and deny the sender access to his/her funds. This ‘refund’ transaction represents the first commitment transaction in the payment lifecycle.\nAlice and Bob can transfer bitcoins to each other multiple times by creating new commitment transactions and exchanging signatures each time. Each new commitment transaction encodes the latest balance between Alice and Bob, altering the initial state but not broadcasting it to the Bitcoin network. This enables Alice and Bob to transfer an unlimited number of payments to each other without having to interact with the underlying Bitcoin network.\nA cheating counterparty that posts an older commitment transaction with a higher balance amount and closes the payment channel without consent will be encumbered by additional transactions that are created during force closure. The Delivery transaction, Revocable delivery transaction and Branch Remedy transactions function to enable cheated parties to claim their funds immediately, impose a time lock on cheating parties before they can use funds and punishes the cheater by confiscating his/her balances. These fail-safe measures and penalty mechanism disincentives cheating parties and reduces the need for trust amongst counterparties.\nIn terms of transaction fees, Alice and Bob only need to pay transactions fees twice (when the payment channel is opened and closed). No transaction fees are charged for transactions in between as these are off-chain and not broadcasted to the network.\nWhen Alice and Bob conclude their payments and decide close their payment channel, they simply sign a closing commitment transaction which will then be broadcast to the main network. In the lifetime of a payment channel, there are only 2 transactions that has to be broadcasted and verified in the Bitcoin network.\nUsers also do not need to open multiple individual payment channels with every party they wish to transact with. The lightning network enables for the routing of transactions by connecting multiple payment channels, creating a path from the payment source to payment destination, and allowing for transactions to be forwarded to any of the network participants. If Alice has an existing payment channel with Bob, and if Bob has another payment channel with Charlie; then Alice can route her payments to Charlie via Bob who acts as an intermediary.\nTransactions can be sent through the network of payment channels without the need to trust any of the intermediaries. A node constructs a path by connecting with channels with enough capacity using the possible shortest routes it can find. The communication between nodes is encrypted point to point. Intermediary nodes can verify and decrypt their own portion of the transaction message but cannot identify who the sender and receiver of the payment is, or their own position in the path (Mastering the Lightning Network – Aantonop, n.d.)."
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html#part-1---layer-1-layer-2-solution",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html#part-1---layer-1-layer-2-solution",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "Layer 1 and layer 2 scalability solutions are different in their role and focus on the blockchain.  Layer 1 solutions aim to change and improve the existing blockchain architecture, and include proposals like increasing the block size, reducing transaction size, changing the transaction format, changing the consensus algorithm and sharding.\nLayer 2 solutions instead, looks to increase the transaction throughput by alleviating the computational load, transferring some transactions offline and rely on secondary networks or technology that operate on top of the underlying blockchain. These include the use of rollups, sidechains, state channels and nested blockchains."
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html#part-2---adoption-for-a-payment-use-case",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html#part-2---adoption-for-a-payment-use-case",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "A company using bitcoin as a payment method for its business activities, would have the following considerations and requirements:\n1) Fast and secure payments\n2) Finality of transactions\n3) Privacy of transaction data\n4) Privacy of company identity\n5) High transaction throughput\n6) Low transaction fees\n7) Efficient and frictionless cross border payments\n8) Secure from centralized points of failure and attack\n9) Available round the clock\nExisting centralized payment systems can accommodate a high transaction throughput but have inherent risks like counterparty risk, system failures, cyber-attacks and denial of service attacks and high transaction fees. Further, to participate in centralized payments systems you would need to have accounts with specific financial institutions or payment entities where terms of service are usually always in their favour. We need to trust that banks and payment companies act in good faith, in our best interests and do not change the rules to benefit and profit from rent-seeking motives.\nOn the other hand, the Bitcoin network, with its decentralised nature and cryptographic proof of work consensus, makes transactions more secure, transparent, and private but may not be able to support a large volume of transactions.\nThe Bitcoin network is slow compared to centralized payment systems because of consensus but is also more secure. Transactions are append-only and immutable, which makes it less vulnerable to manipulation and forgery.\nWhen considering between scalability, security, and decentralization; a business should lean towards security and decentralization. Security is important as transactions are more often high value and crucial to the business operations. With decentralization, there is less dependency on central parties and intermediaries which could potentially be points of failure. As much as a high capacity of transactions is useful for a business, it would not make sense if the payments were insecure or vulnerable to points of failure. To this point, the real question would be how to achieve a higher transaction throughput through layer 1 or layer 2 solutions.\nI would argue that layer 1, in its existing form, despite being ‘slow’ and ‘inefficient’ still functions as a secure environment for high value transfers.  \nLayer 1 solutions aim to change and improve the underlying blockchain architecture, and include solutions like increasing the block size, changing the consensus algorithm and Sharding.  However, each comes with its undesirable consequence.\nFor example, increasing the block size could help increase the transaction throughput by enabling more transaction to be included in each block. However, with larger block sizes, more work needs to be done by miners and validators during proof of work. This could technically increase the computational load required to perform validation and mining, enabling only larger entities and groups to be economically capable of performing the ‘work’. This can lead to centralization and collusion.\nChanging the consensus protocol from proof of work to proof of stake, may potentially make achieving consensus faster as miners no longer have to compete with each other and just be randomly selected to mine the blocks. However, this just simply means that participants with higher stakes gain a higher change and probability of being selected. Similarly, this can lead to centralization and incentivise more collusion.\nSharding refers to the splitting up of miners into separate and independent groups, where each group validate separate sets of transactions concurrently. However inter-shard communication can be costly and inefficient.\nFor these reasons, a layer 2 solutions like state channels would be appropriate to help businesses use the Bitcoin network more efficiently. Specifically, using the Lightning network can enable businesses to move frequent smaller value transactions off-chain, thereby reserving high value transactions only for the main chain.\nThe company can open and maintain payment channels with counterparties and transfer unlimited frequent payments to them as and when required. Security measures are in place with multi-sig addresses requiring both parties to agree and sign off on transactions before they are completed. Further, should one party cheat and abruptly posts an older transaction and close the payment channel, the affected party can 1) post Delivery transactions to recover their funds immediately and 2) post Branch remedy transactions to confiscate the cheater’s balances. This would sufficient disincentive any cheating attempt.\nThe company would not need to open multiple individual payment channels with every counterparty. The lightning network enables for the routing of transactions by connecting multiple payment channels, allowing for transactions to be forwarded to any network participants. The communication between nodes is encrypted point to point, where forwarding nodes can only decrypt their own portion of the route and cannot identify who the sender and receiver of the payment is, or their own position in the path. This enables the company to maintain privacy of identity and transaction information.\nIn terms of transaction fees, the company would only pay transactions fees twice ie when the payment channel is opened and closed. No transaction fees are charged for transactions in between as these are off-chain and not broadcasted to the network.\nBy utilizing solution like the Lightning network, the company have more transactions done faster and with less cost. Further, as these off-chain transactions do not need to be broadcasted to the main-chain, the full extent of the transaction history between counterparties do not have to be revealed to the public blockchain. Finally, with the fail-safe and penalty mechanisms in place, the company can safely transact with multiple counterparties without fear of cheating (Mastering the Lightning Network – Aantonop, n.d.)."
  },
  {
    "objectID": "Article-ex/Article-Ex5/Article_Ex5.html#references",
    "href": "Article-ex/Article-Ex5/Article_Ex5.html#references",
    "title": "Bitcoin - Examining a Layer 1 & 2 solution and adoption for a payment use case",
    "section": "",
    "text": "Academy, B. (2021, April 29). A Beginner’s Guide to Segregated Witness (SegWit). Binance Academy. https://academy.binance.com/en/articles/a-beginners-guide-to-segretated-witness-segwit\nKuznetsov, N. (2019, September 28). SegWit, Explained. Cointelegraph. https://cointelegraph.com/explained/segwit-explained\nMastering the Lightning Network – aantonop. (n.d.). https://aantonop.com/books/mastering-the-lightning-network/\nAcademy, B. (2022, September 29). Blockchain Layer 1 vs. Layer 2 Scaling Solutions. Binance Academy. https://academy.binance.com/en/articles/blockchain-layer-1-vs-layer-2-scaling-solutions\nAcademy, B. (2022, September 29). Blockchain Layer 1 vs. Layer 2 Scaling Solutions. Binance Academy. https://academy.binance.com/en/articles/blockchain-layer-1-vs-layer-2-scaling-solutions"
  }
]